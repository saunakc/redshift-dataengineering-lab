{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import psycopg2\n",
    "import base64\n",
    "import json\n",
    "from random import shuffle\n",
    "import random\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import sagemaker\n",
    "import s3fs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import IntSlider, FloatSlider, Checkbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting, we can override the default values for the following:\n",
    "- The S3 bucket and prefix that you want to use for training and model data. This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "s3_bucket = sagemaker.Session().default_bucket()  # replace with an existing bucket if needed\n",
    "s3_prefix = 'redshift-deepar-nyctaxi-demo-notebook'    # prefix used for all data stored within the bucket\n",
    "\n",
    "role = sagemaker.get_execution_role() \n",
    "\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "s3_data_path = \"s3://{}/{}/data\".format(s3_bucket, s3_prefix)\n",
    "s3_output_path = \"s3://{}/{}/output\".format(s3_bucket, s3_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we configure the container image to be used for the region that we are running in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name = sagemaker.amazon.amazon_estimator.get_image_uri(region, \"forecasting-deepar\", \"latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uses session manager name to return connection and credential information\n",
    "def connection_info(db):\n",
    "\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager'\n",
    "    )\n",
    "\n",
    "    get_secret_value_response = client.get_secret_value(SecretId=db)\n",
    "\n",
    "    if 'SecretString' in get_secret_value_response:\n",
    "        secret = json.loads(get_secret_value_response['SecretString'])\n",
    "    else:\n",
    "        secret = json.loads(base64.b64decode(get_secret_value_response['SecretBinary']))\n",
    "        \n",
    "    return secret\n",
    "\n",
    "\n",
    "#creates a connection to the cluster\n",
    "def get_connection(db,db_creds):\n",
    "\n",
    "    con_params = connection_info(db_creds)\n",
    "    print(\"Connection info retrieved from Secrets manager\")\n",
    "    rs_conn=psycopg2.connect(dbname=db, host=con_params['host'], port=con_params['port'], user=con_params['username'], password=con_params['password'])\n",
    "    cur = rs_conn.cursor()\n",
    "    cur.execute(\"set statement_timeout = 1200000\")\n",
    "    return cur\n",
    "\n",
    "#Close the connection to the cluster\n",
    "def close_cursor(cursor):\n",
    "    cursor.close()\n",
    "\n",
    "#submits a query to the cluster\n",
    "def run_command(cursor, statement):\n",
    "    res = cursor.execute(statement)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connecting...\n",
      "Connection info retrieved from Secrets manager\n",
      "connected...running query...\n",
      "Query execution complete.\n"
     ]
    }
   ],
   "source": [
    "db ='nyctaxi'\n",
    "db_creds = 'nyctaxisecret'\n",
    "redshift_iam_role = 'arn:aws:iam::413094830157:role/deepar-demo-RedshiftRole-2R1VM0KJVXMX' #replace the IAM role with yours\n",
    "\n",
    "#get database connection\n",
    "print('connecting...')\n",
    "\n",
    "cursor = get_connection(db,db_creds)\n",
    "\n",
    "print(\"connected...running query...\")\n",
    "results = []\n",
    "query_str = \"unload('select coalesce(v1.pickup_timestamp_norm, v2.pickup_timestamp_norm) as pickup_timestamp_norm \\\n",
    ", coalesce(v1.vendor_1, 0) as vendor_1 \\\n",
    ", coalesce(v2.vendor_2, 0) as vendor_2 \\\n",
    "from \\\n",
    "(select case when extract(minute from lpep_dropoff_datetime) between 0 and 14 then dateadd(minute, 0, date_trunc(''hour'', lpep_dropoff_datetime)) \\\n",
    "when extract(minute from lpep_dropoff_datetime) between 15 and 29 then dateadd(minute, 15, date_trunc(''hour'', lpep_dropoff_datetime)) \\\n",
    "when extract(minute from lpep_dropoff_datetime) between 30 and 44 then dateadd(minute, 30, date_trunc(''hour'', lpep_dropoff_datetime)) \\\n",
    "when extract(minute from lpep_dropoff_datetime) between 45 and 59 then dateadd(minute, 45, date_trunc(''hour'', lpep_dropoff_datetime)) end as pickup_timestamp_norm \\\n",
    ", count(1) as vendor_1 \\\n",
    "from taxischema.nyc_greentaxi \\\n",
    "where vendorid = 1 group by 1) v1 \\\n",
    "full outer join \\\n",
    "(select case when extract(minute from lpep_dropoff_datetime) between 0 and 14 then dateadd(minute, 0, date_trunc(''hour'', lpep_dropoff_datetime)) \\\n",
    "when extract(minute from lpep_dropoff_datetime) between 15 and 29 then dateadd(minute, 15, date_trunc(''hour'', lpep_dropoff_datetime)) \\\n",
    "when extract(minute from lpep_dropoff_datetime) between 30 and 44 then dateadd(minute, 30, date_trunc(''hour'', lpep_dropoff_datetime)) \\\n",
    "when extract(minute from lpep_dropoff_datetime) between 45 and 59 then dateadd(minute, 45, date_trunc(''hour'', lpep_dropoff_datetime)) end as pickup_timestamp_norm \\\n",
    ", count(1)  as vendor_2 \\\n",
    "from taxischema.nyc_greentaxi \\\n",
    "where vendorid = 2 group by 1) v2 on v1.pickup_timestamp_norm = v2.pickup_timestamp_norm order by pickup_timestamp_norm ;') to '\" \\\n",
    "+ s3_output_path + \"/redshift-output/' iam_role '\" + redshift_iam_role + \"' format as CSV header ALLOWOVERWRITE GZIP\"\n",
    "\n",
    "result = run_command(cursor, query_str)\n",
    "\n",
    "print(\"Query execution complete.\")\n",
    "\n",
    "close_cursor(cursor) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File retrieved redshift-deepar-nyctaxi-demo-notebook/output/redshift-output/0001_part_00.gz\n",
      "File retrieved redshift-deepar-nyctaxi-demo-notebook/output/redshift-output/0002_part_00.gz\n",
      "File retrieved redshift-deepar-nyctaxi-demo-notebook/output/redshift-output/0003_part_00.gz\n"
     ]
    }
   ],
   "source": [
    "s3 = boto3.client('s3')\n",
    "#role = sagemaker.get_execution_role()\n",
    "\n",
    "def load_df_from_s3(s3_path):\n",
    "    assert s3_path.startswith('s3://')\n",
    "    split = s3_path.split('/')\n",
    "    bucket = split[2]\n",
    "    prefix = '/'.join(split[3:])\n",
    "    \n",
    " #   print(\"Bucket is %s\" %bucket)\n",
    " #   print(\"Prefix is %s\" %prefix)\n",
    "       \n",
    "    datafiles = s3.list_objects_v2(Bucket=bucket, Prefix = prefix)['Contents']\n",
    "    prefix_df = []\n",
    "    fs = s3fs.S3FileSystem()\n",
    "\n",
    "    for file in datafiles[1:]:\n",
    "        key = file['Key']\n",
    "        with fs.open('s3://'+ bucket + '/' + key) as f:\n",
    "            df = pd.read_csv(f, compression='gzip', index_col=0, parse_dates=True, decimal=',')\n",
    "        \n",
    "        prefix_df.append(df)\n",
    "        print(\"File retrieved %s\" %key)\n",
    "        \n",
    "    return pd.concat(prefix_df)\n",
    "\n",
    "pd_df = load_df_from_s3(s3_output_path + '/redshift-output/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_timeseries = pd_df.shape[1]\n",
    "data_trip = pd_df.resample('2H').sum() / 8\n",
    "timeseries = []\n",
    "for i in range(num_timeseries):\n",
    "    timeseries.append(np.trim_zeros(data_trip.iloc[:,i], trim='f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x144 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(20, 2), sharex=True)\n",
    "axx = axs.ravel()\n",
    "for i in range(0, 2):\n",
    "    timeseries[i].loc[\"2018-10-01\":\"2019-12-31\"].plot(ax=axx[i])\n",
    "    axx[i].set_xlabel(\"date\")    \n",
    "    axx[i].set_ylabel(\"Ride count\")   \n",
    "    axx[i].grid(which='minor', axis='x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test splits\n",
    "\n",
    "Often times one is interested in evaluating the model or tuning its hyperparameters by looking at error metrics on a hold-out test set. Here we split the available data into train and test sets for evaluating the trained model. For standard machine learning tasks such as classification and regression, one typically obtains this split by randomly separating examples into train and test sets. However, in forecasting it is important to do this train/test split based on time rather than by time series.\n",
    "\n",
    "In this example, we will reserve the last section of each of the time series for evalutation purpose and use only the first part as training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use 2 hour frequency for the time series\n",
    "freq = '2H'\n",
    "\n",
    "# we predict for 7 days\n",
    "prediction_length = 7 * 12\n",
    "\n",
    "# we also use 7 days as context length, this is the number of state updates accomplished before making predictions\n",
    "context_length = 7 * 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We specify here the portion of the data that is used for training: the model sees data from 2019-01-01 to 2019-04-01 for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_dataset = pd.Timestamp(\"2019-01-01 00:00:00\", freq=freq)\n",
    "end_training = pd.Timestamp(\"2019-04-01 00:00:00\", freq=freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DeepAR JSON input format represents each time series as a JSON object. In the simplest case each time series just consists of a start time stamp (``start``) and a list of values (``target``). For more complex cases, DeepAR also supports the fields ``dynamic_feat`` for time-series features and ``cat`` for categorical features, which we will use  later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [\n",
    "    {\n",
    "        \"start\": str(start_dataset),\n",
    "        \"target\": ts[start_dataset:end_training][:-1].tolist()  # We use -1, because pandas indexing includes the upper bound \n",
    "    }\n",
    "    for ts in timeseries\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As test data, we will consider time series extending beyond the training range: these will be used for computing test scores, by using the trained model to forecast their trailing 7 days, and comparing predictions with actual values.\n",
    "To evaluate our model performance on more than one week, we generate test data that extends to 1, 2, 3, 4 weeks beyond the training range. This way we perform *rolling evaluation* of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n"
     ]
    }
   ],
   "source": [
    "num_test_windows = 4\n",
    "\n",
    "idx =  []\n",
    "print(len(pd.date_range(start_dataset, end_training )))\n",
    "period_range = len(pd.date_range(start_dataset, end_training))\n",
    "      \n",
    "for i in range(1, num_test_windows + 1) :\n",
    "    idx.append(pd.date_range(start_dataset, periods = period_range + i * prediction_length, freq=freq))\n",
    "\n",
    "test_data = [\n",
    "    {   \n",
    "        \"start\": str(start_dataset),\n",
    "        \"target\": ts[idx[k]].tolist()\n",
    "    }\n",
    "    for k in range(0, num_test_windows)\n",
    "    for ts in timeseries\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now write the dictionary to the `jsonlines` file format that DeepAR understands (it also supports gzipped jsonlines and parquet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dicts_to_file(path, data):\n",
    "    with open(path, 'wb') as fp:\n",
    "        for d in data:\n",
    "            fp.write(json.dumps(d).encode(\"utf-8\"))\n",
    "            fp.write(\"\\n\".encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.92 ms, sys: 285 µs, total: 3.2 ms\n",
      "Wall time: 4.64 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "write_dicts_to_file(\"train.json\", training_data)\n",
    "write_dicts_to_file(\"test.json\", test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data files locally, let us copy them to S3 where DeepAR can access them. Depending on your connection, this may take a couple of minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "def copy_to_s3(local_file, s3_path, override=False):\n",
    "    assert s3_path.startswith('s3://')\n",
    "    split = s3_path.split('/')\n",
    "    bucket = split[2]\n",
    "    path = '/'.join(split[3:])\n",
    "    buk = s3.Bucket(bucket)\n",
    "    \n",
    "    if len(list(buk.objects.filter(Prefix=path))) > 0:\n",
    "        if not override:\n",
    "            print('File s3://{}/{} already exists.\\nSet override to upload anyway.\\n'.format(s3_bucket, s3_path))\n",
    "            return\n",
    "        else:\n",
    "            print('Overwriting existing file')\n",
    "    with open(local_file, 'rb') as data:\n",
    "        print('Uploading file to {}'.format(s3_path))\n",
    "        buk.put_object(Key=path, Body=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File s3://sagemaker-us-west-2-413094830157/s3://sagemaker-us-west-2-413094830157/redshift-deepar-nyctaxi-demo-notebook/data/train/train.json already exists.\n",
      "Set override to upload anyway.\n",
      "\n",
      "File s3://sagemaker-us-west-2-413094830157/s3://sagemaker-us-west-2-413094830157/redshift-deepar-nyctaxi-demo-notebook/data/test/test.json already exists.\n",
      "Set override to upload anyway.\n",
      "\n",
      "CPU times: user 24.8 ms, sys: 3.74 ms, total: 28.5 ms\n",
      "Wall time: 186 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "copy_to_s3(\"train.json\", s3_data_path + \"/train/train.json\")\n",
    "copy_to_s3(\"test.json\", s3_data_path + \"/test/test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look to what we just wrote to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"start\": \"2019-01-01 00:00:00\", \"target\": [42.625, 36.625, 17.5, 10.375, 6.875, 7.875, 15.5, 26.5, ...\n"
     ]
    }
   ],
   "source": [
    "s3filesystem = s3fs.S3FileSystem()\n",
    "with s3filesystem.open(s3_data_path + \"/train/train.json\", 'rb') as fp:\n",
    "    print(fp.readline().decode(\"utf-8\")[:100] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are all set with our dataset processing, we can now call DeepAR to train a model and generate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model\n",
    "\n",
    "Here we define the estimator that will launch the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    image_name=image_name,\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.c4.2xlarge',\n",
    "    base_job_name='redshift-deepar-nyctaxi-demo',\n",
    "    output_path=s3_output_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to set the hyperparameters for the training job. For example frequency of the time series used, number of data points the model will look at in the past, number of predicted data points. The other hyperparameters concern the model to train (number of layers, number of cells per layer, likelihood function) and the training options (number of epochs, batch size, learning rate...). We use default parameters for every optional parameter in this case (you can always use Sagemaker Automated Model Tuning to tune them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"time_freq\": freq,\n",
    "    \"epochs\": \"400\",\n",
    "    \"early_stopping_patience\": \"40\",\n",
    "    \"mini_batch_size\": \"64\",\n",
    "    \"learning_rate\": \"5E-4\",\n",
    "    \"context_length\": str(context_length),\n",
    "    \"prediction_length\": str(prediction_length)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to launch the training job. SageMaker will start an EC2 instance, download the data from S3, start training the model and save the trained model.\n",
    "\n",
    "If you provide the test data channel as we do in this example, DeepAR will also calculate accuracy metrics for the trained model on this test. This is done by predicting the last prediction_length points of each time-series in the test set and comparing this to the actual value of the time-series.\n",
    "\n",
    "Note: the next cell may take a few minutes to complete, depending on data size, model complexity, training options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-04 21:11:18 Starting - Starting the training job...\n",
      "2020-05-04 21:11:19 Starting - Launching requested ML instances......\n",
      "2020-05-04 21:12:20 Starting - Preparing the instances for training...\n",
      "2020-05-04 21:13:04 Downloading - Downloading input data\n",
      "2020-05-04 21:13:04 Training - Downloading the training image...\n",
      "2020-05-04 21:13:46 Training - Training image download completed. Training in progress..\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:48 INFO 140149161203520] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/resources/default-input.json: {u'num_dynamic_feat': u'auto', u'dropout_rate': u'0.10', u'mini_batch_size': u'128', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'num_eval_samples': u'100', u'learning_rate': u'0.001', u'num_cells': u'40', u'num_layers': u'2', u'embedding_dimension': u'10', u'_kvstore': u'auto', u'_num_kv_servers': u'auto', u'cardinality': u'auto', u'likelihood': u'student-t', u'early_stopping_patience': u''}\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:48 INFO 140149161203520] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'learning_rate': u'5E-4', u'prediction_length': u'84', u'epochs': u'400', u'time_freq': u'2H', u'context_length': u'84', u'mini_batch_size': u'64', u'early_stopping_patience': u'40'}\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:48 INFO 140149161203520] Final configuration: {u'dropout_rate': u'0.10', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'num_eval_samples': u'100', u'learning_rate': u'5E-4', u'num_layers': u'2', u'epochs': u'400', u'embedding_dimension': u'10', u'num_cells': u'40', u'_num_kv_servers': u'auto', u'mini_batch_size': u'64', u'likelihood': u'student-t', u'num_dynamic_feat': u'auto', u'cardinality': u'auto', u'_num_gpus': u'auto', u'prediction_length': u'84', u'time_freq': u'2H', u'context_length': u'84', u'_kvstore': u'auto', u'early_stopping_patience': u'40'}\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:48 INFO 140149161203520] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:48 INFO 140149161203520] Using early stopping with patience 40\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:48 INFO 140149161203520] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:48 INFO 140149161203520] [num_dynamic_feat=auto] `dynamic_feat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:48 INFO 140149161203520] Training set statistics:\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:48 INFO 140149161203520] Real time series\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:48 INFO 140149161203520] number of time series: 2\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:48 INFO 140149161203520] number of observations: 2160\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:48 INFO 140149161203520] mean target length: 1080\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:48 INFO 140149161203520] min/mean/max target: 1.25/78.727025463/352.125\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:48 INFO 140149161203520] mean abs(target): 78.727025463\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:48 INFO 140149161203520] contains missing values: no\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:48 INFO 140149161203520] Small number of time series. Doing 320 passes over dataset with prob 1.0 per epoch.\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:48 INFO 140149161203520] Test set statistics:\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:48 INFO 140149161203520] Real time series\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:48 INFO 140149161203520] number of time series: 20\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:48 INFO 140149161203520] number of observations: 30860\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:48 INFO 140149161203520] mean target length: 1543\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:48 INFO 140149161203520] min/mean/max target: 0.875/75.2997043098/352.125\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:48 INFO 140149161203520] mean abs(target): 75.2997043098\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:48 INFO 140149161203520] contains missing values: no\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:48 INFO 140149161203520] nvidia-smi took: 0.0252549648285 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:48 INFO 140149161203520] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:48 INFO 140149161203520] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 653.5251140594482, \"sum\": 653.5251140594482, \"min\": 653.5251140594482}}, \"EndTime\": 1588626829.364074, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626828.709755}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:49 INFO 140149161203520] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 1361.914873123169, \"sum\": 1361.914873123169, \"min\": 1361.914873123169}}, \"EndTime\": 1588626830.07178, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626829.364159}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:50 INFO 140149161203520] Epoch[0] Batch[0] avg_epoch_loss=5.177043\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:50 INFO 140149161203520] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=5.17704343796\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:51 INFO 140149161203520] Epoch[0] Batch[5] avg_epoch_loss=5.137715\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:51 INFO 140149161203520] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=5.1377146244\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:51 INFO 140149161203520] Epoch[0] Batch [5]#011Speed: 320.49 samples/sec#011loss=5.137715\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:52 INFO 140149161203520] Epoch[0] Batch[10] avg_epoch_loss=4.991520\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:52 INFO 140149161203520] #quality_metric: host=algo-1, epoch=0, batch=10 train loss <loss>=4.81608552933\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:52 INFO 140149161203520] Epoch[0] Batch [10]#011Speed: 298.95 samples/sec#011loss=4.816086\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:52 INFO 140149161203520] processed a total of 674 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 400, \"sum\": 400.0, \"min\": 400}, \"update.time\": {\"count\": 1, \"max\": 2700.7060050964355, \"sum\": 2700.7060050964355, \"min\": 2700.7060050964355}}, \"EndTime\": 1588626832.772679, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626830.071872}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:52 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=249.55080199 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:52 INFO 140149161203520] #progress_metric: host=algo-1, completed 0 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:52 INFO 140149161203520] #quality_metric: host=algo-1, epoch=0, train loss <loss>=4.99151958119\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:52 INFO 140149161203520] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:52 INFO 140149161203520] Saved checkpoint to \"/opt/ml/model/state_fbb98627-42db-48a2-b78a-a3a5edafeae6-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 101.9899845123291, \"sum\": 101.9899845123291, \"min\": 101.9899845123291}}, \"EndTime\": 1588626832.875338, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626832.772785}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:53 INFO 140149161203520] Epoch[1] Batch[0] avg_epoch_loss=4.970357\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:53 INFO 140149161203520] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=4.97035741806\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:54 INFO 140149161203520] Epoch[1] Batch[5] avg_epoch_loss=4.862825\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:54 INFO 140149161203520] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=4.86282467842\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:54 INFO 140149161203520] Epoch[1] Batch [5]#011Speed: 296.89 samples/sec#011loss=4.862825\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:55 INFO 140149161203520] Epoch[1] Batch[10] avg_epoch_loss=4.713749\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:55 INFO 140149161203520] #quality_metric: host=algo-1, epoch=1, batch=10 train loss <loss>=4.53485908508\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:55 INFO 140149161203520] Epoch[1] Batch [10]#011Speed: 303.55 samples/sec#011loss=4.534859\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:55 INFO 140149161203520] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2704.202890396118, \"sum\": 2704.202890396118, \"min\": 2704.202890396118}}, \"EndTime\": 1588626835.579697, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626832.87542}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:55 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=240.35431023 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:55 INFO 140149161203520] #progress_metric: host=algo-1, completed 0 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:55 INFO 140149161203520] #quality_metric: host=algo-1, epoch=1, train loss <loss>=4.71374940872\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:55 INFO 140149161203520] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:55 INFO 140149161203520] Saved checkpoint to \"/opt/ml/model/state_cf2f3a60-508b-4415-ac74-606232b79e63-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 67.8410530090332, \"sum\": 67.8410530090332, \"min\": 67.8410530090332}}, \"EndTime\": 1588626835.648166, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626835.579794}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:56 INFO 140149161203520] Epoch[2] Batch[0] avg_epoch_loss=4.795957\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:56 INFO 140149161203520] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=4.79595708847\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:57 INFO 140149161203520] Epoch[2] Batch[5] avg_epoch_loss=4.549928\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:57 INFO 140149161203520] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=4.54992802938\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:57 INFO 140149161203520] Epoch[2] Batch [5]#011Speed: 321.96 samples/sec#011loss=4.549928\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:58 INFO 140149161203520] Epoch[2] Batch[10] avg_epoch_loss=4.477165\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:58 INFO 140149161203520] #quality_metric: host=algo-1, epoch=2, batch=10 train loss <loss>=4.38984837532\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:58 INFO 140149161203520] Epoch[2] Batch [10]#011Speed: 314.70 samples/sec#011loss=4.389848\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:58 INFO 140149161203520] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2474.472999572754, \"sum\": 2474.472999572754, \"min\": 2474.472999572754}}, \"EndTime\": 1588626838.122797, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626835.648253}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:58 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=265.093544532 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:58 INFO 140149161203520] #progress_metric: host=algo-1, completed 0 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:58 INFO 140149161203520] #quality_metric: host=algo-1, epoch=2, train loss <loss>=4.47716455026\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:58 INFO 140149161203520] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:58 INFO 140149161203520] Saved checkpoint to \"/opt/ml/model/state_f0df2aa4-7426-47ba-a46a-4aefa32b4e1d-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 85.64400672912598, \"sum\": 85.64400672912598, \"min\": 85.64400672912598}}, \"EndTime\": 1588626838.209069, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626838.122882}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:58 INFO 140149161203520] Epoch[3] Batch[0] avg_epoch_loss=4.211368\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:58 INFO 140149161203520] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=4.21136808395\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:59 INFO 140149161203520] Epoch[3] Batch[5] avg_epoch_loss=4.358815\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:59 INFO 140149161203520] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=4.35881487528\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:13:59 INFO 140149161203520] Epoch[3] Batch [5]#011Speed: 324.13 samples/sec#011loss=4.358815\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:00 INFO 140149161203520] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2255.0101280212402, \"sum\": 2255.0101280212402, \"min\": 2255.0101280212402}}, \"EndTime\": 1588626840.464239, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626838.209153}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:00 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=283.346637072 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:00 INFO 140149161203520] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:00 INFO 140149161203520] #quality_metric: host=algo-1, epoch=3, train loss <loss>=4.36765480042\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:00 INFO 140149161203520] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:00 INFO 140149161203520] Saved checkpoint to \"/opt/ml/model/state_2c4f48b8-dbd4-45e6-83f2-226bf13ab8c6-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 65.64784049987793, \"sum\": 65.64784049987793, \"min\": 65.64784049987793}}, \"EndTime\": 1588626840.530555, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626840.464371}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:00 INFO 140149161203520] Epoch[4] Batch[0] avg_epoch_loss=4.347435\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:00 INFO 140149161203520] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=4.3474354744\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:02 INFO 140149161203520] Epoch[4] Batch[5] avg_epoch_loss=4.309273\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:02 INFO 140149161203520] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=4.30927268664\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:02 INFO 140149161203520] Epoch[4] Batch [5]#011Speed: 315.61 samples/sec#011loss=4.309273\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:02 INFO 140149161203520] Epoch[4] Batch[10] avg_epoch_loss=4.353863\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:02 INFO 140149161203520] #quality_metric: host=algo-1, epoch=4, batch=10 train loss <loss>=4.40737142563\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:02 INFO 140149161203520] Epoch[4] Batch [10]#011Speed: 325.72 samples/sec#011loss=4.407371\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:02 INFO 140149161203520] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2453.990936279297, \"sum\": 2453.990936279297, \"min\": 2453.990936279297}}, \"EndTime\": 1588626842.984682, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626840.530619}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:02 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=264.453357279 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:02 INFO 140149161203520] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:02 INFO 140149161203520] #quality_metric: host=algo-1, epoch=4, train loss <loss>=4.35386302254\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:02 INFO 140149161203520] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:03 INFO 140149161203520] Saved checkpoint to \"/opt/ml/model/state_5dc7b93e-4a9f-4efb-a4b3-d684ac3e542a-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 95.36409378051758, \"sum\": 95.36409378051758, \"min\": 95.36409378051758}}, \"EndTime\": 1588626843.080631, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626842.98477}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:03 INFO 140149161203520] Epoch[5] Batch[0] avg_epoch_loss=4.149581\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:03 INFO 140149161203520] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=4.14958143234\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:04 INFO 140149161203520] Epoch[5] Batch[5] avg_epoch_loss=4.137181\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:04 INFO 140149161203520] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=4.13718092442\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:04 INFO 140149161203520] Epoch[5] Batch [5]#011Speed: 312.10 samples/sec#011loss=4.137181\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:05 INFO 140149161203520] Epoch[5] Batch[10] avg_epoch_loss=4.098732\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:05 INFO 140149161203520] #quality_metric: host=algo-1, epoch=5, batch=10 train loss <loss>=4.0525929451\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:05 INFO 140149161203520] Epoch[5] Batch [10]#011Speed: 321.93 samples/sec#011loss=4.052593\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:05 INFO 140149161203520] processed a total of 673 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2528.446912765503, \"sum\": 2528.446912765503, \"min\": 2528.446912765503}}, \"EndTime\": 1588626845.609235, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626843.080717}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:05 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=266.157822104 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:05 INFO 140149161203520] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:05 INFO 140149161203520] #quality_metric: host=algo-1, epoch=5, train loss <loss>=4.09873184291\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:05 INFO 140149161203520] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:05 INFO 140149161203520] Saved checkpoint to \"/opt/ml/model/state_590dda8e-a634-499f-be6f-c7e3df57d2a5-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 59.374094009399414, \"sum\": 59.374094009399414, \"min\": 59.374094009399414}}, \"EndTime\": 1588626845.669188, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626845.609322}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:06 INFO 140149161203520] Epoch[6] Batch[0] avg_epoch_loss=4.331400\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:06 INFO 140149161203520] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=4.33140039444\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:07 INFO 140149161203520] Epoch[6] Batch[5] avg_epoch_loss=4.168215\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:07 INFO 140149161203520] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=4.16821479797\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:07 INFO 140149161203520] Epoch[6] Batch [5]#011Speed: 322.13 samples/sec#011loss=4.168215\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:07 INFO 140149161203520] processed a total of 610 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2232.990026473999, \"sum\": 2232.990026473999, \"min\": 2232.990026473999}}, \"EndTime\": 1588626847.902332, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626845.669271}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:07 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=273.160425624 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:07 INFO 140149161203520] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:07 INFO 140149161203520] #quality_metric: host=algo-1, epoch=6, train loss <loss>=4.15698962212\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:07 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:08 INFO 140149161203520] Epoch[7] Batch[0] avg_epoch_loss=4.236836\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:08 INFO 140149161203520] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=4.23683643341\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:09 INFO 140149161203520] Epoch[7] Batch[5] avg_epoch_loss=4.122758\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:09 INFO 140149161203520] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=4.12275846799\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:09 INFO 140149161203520] Epoch[7] Batch [5]#011Speed: 319.89 samples/sec#011loss=4.122758\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:10 INFO 140149161203520] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2272.110939025879, \"sum\": 2272.110939025879, \"min\": 2272.110939025879}}, \"EndTime\": 1588626850.175004, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626847.90242}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:10 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=280.77576014 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:10 INFO 140149161203520] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:10 INFO 140149161203520] #quality_metric: host=algo-1, epoch=7, train loss <loss>=4.1008477211\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:10 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:10 INFO 140149161203520] Epoch[8] Batch[0] avg_epoch_loss=4.101544\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:10 INFO 140149161203520] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=4.10154390335\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:11 INFO 140149161203520] Epoch[8] Batch[5] avg_epoch_loss=4.039683\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:11 INFO 140149161203520] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=4.03968250751\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:11 INFO 140149161203520] Epoch[8] Batch [5]#011Speed: 320.20 samples/sec#011loss=4.039683\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:12 INFO 140149161203520] Epoch[8] Batch[10] avg_epoch_loss=3.971218\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:12 INFO 140149161203520] #quality_metric: host=algo-1, epoch=8, batch=10 train loss <loss>=3.88906049728\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:12 INFO 140149161203520] Epoch[8] Batch [10]#011Speed: 310.75 samples/sec#011loss=3.889060\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:12 INFO 140149161203520] processed a total of 691 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2533.543109893799, \"sum\": 2533.543109893799, \"min\": 2533.543109893799}}, \"EndTime\": 1588626852.709175, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626850.175127}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:12 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=272.726919458 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:12 INFO 140149161203520] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:12 INFO 140149161203520] #quality_metric: host=algo-1, epoch=8, train loss <loss>=3.97121795741\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:12 INFO 140149161203520] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:12 INFO 140149161203520] Saved checkpoint to \"/opt/ml/model/state_16e8540c-7c8a-441e-8669-49a27224fc47-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 60.231924057006836, \"sum\": 60.231924057006836, \"min\": 60.231924057006836}}, \"EndTime\": 1588626852.769986, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626852.709262}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:13 INFO 140149161203520] Epoch[9] Batch[0] avg_epoch_loss=3.979204\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:13 INFO 140149161203520] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=3.97920417786\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:14 INFO 140149161203520] Epoch[9] Batch[5] avg_epoch_loss=3.985334\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:14 INFO 140149161203520] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=3.98533447584\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:14 INFO 140149161203520] Epoch[9] Batch [5]#011Speed: 320.63 samples/sec#011loss=3.985334\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:15 INFO 140149161203520] Epoch[9] Batch[10] avg_epoch_loss=3.958565\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:15 INFO 140149161203520] #quality_metric: host=algo-1, epoch=9, batch=10 train loss <loss>=3.92644119263\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:15 INFO 140149161203520] Epoch[9] Batch [10]#011Speed: 315.34 samples/sec#011loss=3.926441\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:15 INFO 140149161203520] processed a total of 672 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2463.6878967285156, \"sum\": 2463.6878967285156, \"min\": 2463.6878967285156}}, \"EndTime\": 1588626855.233808, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626852.770061}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:15 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=272.748413677 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:15 INFO 140149161203520] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:15 INFO 140149161203520] #quality_metric: host=algo-1, epoch=9, train loss <loss>=3.95856480165\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:15 INFO 140149161203520] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:15 INFO 140149161203520] Saved checkpoint to \"/opt/ml/model/state_2583664a-10a3-4159-a718-15d943714472-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 61.13100051879883, \"sum\": 61.13100051879883, \"min\": 61.13100051879883}}, \"EndTime\": 1588626855.295584, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626855.23389}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:15 INFO 140149161203520] Epoch[10] Batch[0] avg_epoch_loss=4.059045\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:15 INFO 140149161203520] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=4.05904531479\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:16 INFO 140149161203520] Epoch[10] Batch[5] avg_epoch_loss=3.976549\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:16 INFO 140149161203520] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=3.97654922803\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:16 INFO 140149161203520] Epoch[10] Batch [5]#011Speed: 319.78 samples/sec#011loss=3.976549\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:17 INFO 140149161203520] Epoch[10] Batch[10] avg_epoch_loss=3.883114\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:17 INFO 140149161203520] #quality_metric: host=algo-1, epoch=10, batch=10 train loss <loss>=3.77099146843\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:17 INFO 140149161203520] Epoch[10] Batch [10]#011Speed: 323.57 samples/sec#011loss=3.770991\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:17 INFO 140149161203520] processed a total of 671 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2437.8721714019775, \"sum\": 2437.8721714019775, \"min\": 2437.8721714019775}}, \"EndTime\": 1588626857.733591, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626855.295659}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:17 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=275.226164058 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:17 INFO 140149161203520] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:17 INFO 140149161203520] #quality_metric: host=algo-1, epoch=10, train loss <loss>=3.88311388276\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:17 INFO 140149161203520] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:17 INFO 140149161203520] Saved checkpoint to \"/opt/ml/model/state_a84ebd6b-e252-4a71-af7e-f2f019f4e282-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 67.02208518981934, \"sum\": 67.02208518981934, \"min\": 67.02208518981934}}, \"EndTime\": 1588626857.801178, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626857.733677}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:18 INFO 140149161203520] Epoch[11] Batch[0] avg_epoch_loss=3.971711\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:18 INFO 140149161203520] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=3.97171139717\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:19 INFO 140149161203520] Epoch[11] Batch[5] avg_epoch_loss=3.894064\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:19 INFO 140149161203520] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=3.89406414827\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:19 INFO 140149161203520] Epoch[11] Batch [5]#011Speed: 325.81 samples/sec#011loss=3.894064\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:20 INFO 140149161203520] processed a total of 614 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2227.663993835449, \"sum\": 2227.663993835449, \"min\": 2227.663993835449}}, \"EndTime\": 1588626860.028975, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626857.801246}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:20 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=275.607853226 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:20 INFO 140149161203520] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:20 INFO 140149161203520] #quality_metric: host=algo-1, epoch=11, train loss <loss>=3.87599177361\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:20 INFO 140149161203520] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:20 INFO 140149161203520] Saved checkpoint to \"/opt/ml/model/state_6cc8347e-7975-4c3f-8bb5-b8029c361fee-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 71.42305374145508, \"sum\": 71.42305374145508, \"min\": 71.42305374145508}}, \"EndTime\": 1588626860.101044, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626860.029071}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:20 INFO 140149161203520] Epoch[12] Batch[0] avg_epoch_loss=3.887520\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:20 INFO 140149161203520] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=3.88751959801\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:21 INFO 140149161203520] Epoch[12] Batch[5] avg_epoch_loss=3.916701\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:21 INFO 140149161203520] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=3.91670111815\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:21 INFO 140149161203520] Epoch[12] Batch [5]#011Speed: 325.00 samples/sec#011loss=3.916701\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:22 INFO 140149161203520] processed a total of 607 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2244.0831661224365, \"sum\": 2244.0831661224365, \"min\": 2244.0831661224365}}, \"EndTime\": 1588626862.34528, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626860.101118}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:22 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=270.471756311 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:22 INFO 140149161203520] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:22 INFO 140149161203520] #quality_metric: host=algo-1, epoch=12, train loss <loss>=3.85153927803\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:22 INFO 140149161203520] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:22 INFO 140149161203520] Saved checkpoint to \"/opt/ml/model/state_1895da98-b581-4bc9-8be6-7072836f7298-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 66.65802001953125, \"sum\": 66.65802001953125, \"min\": 66.65802001953125}}, \"EndTime\": 1588626862.412604, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626862.345379}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:22 INFO 140149161203520] Epoch[13] Batch[0] avg_epoch_loss=3.947944\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:22 INFO 140149161203520] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=3.94794392586\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:23 INFO 140149161203520] Epoch[13] Batch[5] avg_epoch_loss=3.900256\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:23 INFO 140149161203520] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=3.90025579929\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:23 INFO 140149161203520] Epoch[13] Batch [5]#011Speed: 324.84 samples/sec#011loss=3.900256\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:24 INFO 140149161203520] Epoch[13] Batch[10] avg_epoch_loss=3.944749\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:24 INFO 140149161203520] #quality_metric: host=algo-1, epoch=13, batch=10 train loss <loss>=3.99814147949\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:24 INFO 140149161203520] Epoch[13] Batch [10]#011Speed: 323.36 samples/sec#011loss=3.998141\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:24 INFO 140149161203520] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2431.598901748657, \"sum\": 2431.598901748657, \"min\": 2431.598901748657}}, \"EndTime\": 1588626864.844364, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626862.412691}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:24 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=266.065823605 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:24 INFO 140149161203520] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:24 INFO 140149161203520] #quality_metric: host=algo-1, epoch=13, train loss <loss>=3.94474929029\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:24 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:25 INFO 140149161203520] Epoch[14] Batch[0] avg_epoch_loss=3.834841\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:25 INFO 140149161203520] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=3.83484125137\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:26 INFO 140149161203520] Epoch[14] Batch[5] avg_epoch_loss=3.788784\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:26 INFO 140149161203520] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=3.78878418605\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:26 INFO 140149161203520] Epoch[14] Batch [5]#011Speed: 309.07 samples/sec#011loss=3.788784\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:27 INFO 140149161203520] Epoch[14] Batch[10] avg_epoch_loss=3.861933\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:27 INFO 140149161203520] #quality_metric: host=algo-1, epoch=14, batch=10 train loss <loss>=3.94971179962\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:27 INFO 140149161203520] Epoch[14] Batch [10]#011Speed: 309.37 samples/sec#011loss=3.949712\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:27 INFO 140149161203520] processed a total of 664 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2578.8841247558594, \"sum\": 2578.8841247558594, \"min\": 2578.8841247558594}}, \"EndTime\": 1588626867.423773, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626864.844453}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:27 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=257.462943693 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:27 INFO 140149161203520] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:27 INFO 140149161203520] #quality_metric: host=algo-1, epoch=14, train loss <loss>=3.86193310131\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:27 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:27 INFO 140149161203520] Epoch[15] Batch[0] avg_epoch_loss=3.947325\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:27 INFO 140149161203520] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=3.94732475281\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:28 INFO 140149161203520] Epoch[15] Batch[5] avg_epoch_loss=3.878317\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:28 INFO 140149161203520] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=3.87831660112\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:28 INFO 140149161203520] Epoch[15] Batch [5]#011Speed: 323.72 samples/sec#011loss=3.878317\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:29 INFO 140149161203520] Epoch[15] Batch[10] avg_epoch_loss=3.851317\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:29 INFO 140149161203520] #quality_metric: host=algo-1, epoch=15, batch=10 train loss <loss>=3.81891775131\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:29 INFO 140149161203520] Epoch[15] Batch [10]#011Speed: 325.90 samples/sec#011loss=3.818918\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:29 INFO 140149161203520] processed a total of 673 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2432.1351051330566, \"sum\": 2432.1351051330566, \"min\": 2432.1351051330566}}, \"EndTime\": 1588626869.856444, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626867.423861}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:29 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=276.697171417 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:29 INFO 140149161203520] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:29 INFO 140149161203520] #quality_metric: host=algo-1, epoch=15, train loss <loss>=3.85131712393\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:29 INFO 140149161203520] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:29 INFO 140149161203520] Saved checkpoint to \"/opt/ml/model/state_b1c81b51-c724-44c3-abc6-7e84debf5a8c-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 60.01901626586914, \"sum\": 60.01901626586914, \"min\": 60.01901626586914}}, \"EndTime\": 1588626869.917059, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626869.856529}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:30 INFO 140149161203520] Epoch[16] Batch[0] avg_epoch_loss=3.887479\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:30 INFO 140149161203520] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=3.88747882843\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:31 INFO 140149161203520] Epoch[16] Batch[5] avg_epoch_loss=3.812666\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:31 INFO 140149161203520] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=3.8126655817\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:31 INFO 140149161203520] Epoch[16] Batch [5]#011Speed: 326.71 samples/sec#011loss=3.812666\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:32 INFO 140149161203520] processed a total of 589 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2218.0678844451904, \"sum\": 2218.0678844451904, \"min\": 2218.0678844451904}}, \"EndTime\": 1588626872.135283, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626869.917141}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:32 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=265.529558219 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:32 INFO 140149161203520] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:32 INFO 140149161203520] #quality_metric: host=algo-1, epoch=16, train loss <loss>=3.88729784489\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:32 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:32 INFO 140149161203520] Epoch[17] Batch[0] avg_epoch_loss=3.840816\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:32 INFO 140149161203520] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=3.84081602097\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:33 INFO 140149161203520] Epoch[17] Batch[5] avg_epoch_loss=3.905067\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:33 INFO 140149161203520] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=3.90506720543\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:33 INFO 140149161203520] Epoch[17] Batch [5]#011Speed: 323.00 samples/sec#011loss=3.905067\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:34 INFO 140149161203520] processed a total of 614 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2339.538097381592, \"sum\": 2339.538097381592, \"min\": 2339.538097381592}}, \"EndTime\": 1588626874.475428, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626872.13538}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:34 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=262.42926895 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:34 INFO 140149161203520] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:34 INFO 140149161203520] #quality_metric: host=algo-1, epoch=17, train loss <loss>=3.82977104187\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:34 INFO 140149161203520] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:34 INFO 140149161203520] Saved checkpoint to \"/opt/ml/model/state_2fb45ea9-56ae-40d8-8e90-36b20a5b57d4-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 83.06407928466797, \"sum\": 83.06407928466797, \"min\": 83.06407928466797}}, \"EndTime\": 1588626874.559183, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626874.475524}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:35 INFO 140149161203520] Epoch[18] Batch[0] avg_epoch_loss=3.807682\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:35 INFO 140149161203520] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=3.80768227577\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:36 INFO 140149161203520] Epoch[18] Batch[5] avg_epoch_loss=3.784783\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:36 INFO 140149161203520] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=3.78478264809\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:36 INFO 140149161203520] Epoch[18] Batch [5]#011Speed: 321.56 samples/sec#011loss=3.784783\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:37 INFO 140149161203520] Epoch[18] Batch[10] avg_epoch_loss=3.767923\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:37 INFO 140149161203520] #quality_metric: host=algo-1, epoch=18, batch=10 train loss <loss>=3.74769172668\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:37 INFO 140149161203520] Epoch[18] Batch [10]#011Speed: 319.00 samples/sec#011loss=3.747692\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:37 INFO 140149161203520] processed a total of 693 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2471.1270332336426, \"sum\": 2471.1270332336426, \"min\": 2471.1270332336426}}, \"EndTime\": 1588626877.030452, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626874.559249}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:37 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=280.423391707 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:37 INFO 140149161203520] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:37 INFO 140149161203520] #quality_metric: host=algo-1, epoch=18, train loss <loss>=3.76792313836\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:37 INFO 140149161203520] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:37 INFO 140149161203520] Saved checkpoint to \"/opt/ml/model/state_3b8132a0-6294-4214-9a3c-439b1b71c0f0-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 67.72494316101074, \"sum\": 67.72494316101074, \"min\": 67.72494316101074}}, \"EndTime\": 1588626877.098818, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626877.030543}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:37 INFO 140149161203520] Epoch[19] Batch[0] avg_epoch_loss=3.857959\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:37 INFO 140149161203520] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=3.85795903206\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:38 INFO 140149161203520] Epoch[19] Batch[5] avg_epoch_loss=3.887241\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:38 INFO 140149161203520] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=3.88724076748\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:38 INFO 140149161203520] Epoch[19] Batch [5]#011Speed: 322.59 samples/sec#011loss=3.887241\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:39 INFO 140149161203520] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2244.1790103912354, \"sum\": 2244.1790103912354, \"min\": 2244.1790103912354}}, \"EndTime\": 1588626879.343151, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626877.098904}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:39 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=277.590514134 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:39 INFO 140149161203520] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:39 INFO 140149161203520] #quality_metric: host=algo-1, epoch=19, train loss <loss>=3.89821708202\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:39 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:39 INFO 140149161203520] Epoch[20] Batch[0] avg_epoch_loss=3.763657\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:39 INFO 140149161203520] #quality_metric: host=algo-1, epoch=20, batch=0 train loss <loss>=3.76365661621\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:40 INFO 140149161203520] Epoch[20] Batch[5] avg_epoch_loss=3.876493\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:40 INFO 140149161203520] #quality_metric: host=algo-1, epoch=20, batch=5 train loss <loss>=3.87649349372\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:40 INFO 140149161203520] Epoch[20] Batch [5]#011Speed: 321.94 samples/sec#011loss=3.876493\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:41 INFO 140149161203520] Epoch[20] Batch[10] avg_epoch_loss=3.821136\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:41 INFO 140149161203520] #quality_metric: host=algo-1, epoch=20, batch=10 train loss <loss>=3.75470752716\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:41 INFO 140149161203520] Epoch[20] Batch [10]#011Speed: 310.56 samples/sec#011loss=3.754708\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:41 INFO 140149161203520] processed a total of 693 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2493.3879375457764, \"sum\": 2493.3879375457764, \"min\": 2493.3879375457764}}, \"EndTime\": 1588626881.837098, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626879.343245}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:41 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=277.92060637 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:41 INFO 140149161203520] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:41 INFO 140149161203520] #quality_metric: host=algo-1, epoch=20, train loss <loss>=3.82113623619\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:41 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:42 INFO 140149161203520] Epoch[21] Batch[0] avg_epoch_loss=3.890120\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:42 INFO 140149161203520] #quality_metric: host=algo-1, epoch=21, batch=0 train loss <loss>=3.89011955261\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:43 INFO 140149161203520] Epoch[21] Batch[5] avg_epoch_loss=3.826273\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:43 INFO 140149161203520] #quality_metric: host=algo-1, epoch=21, batch=5 train loss <loss>=3.82627324263\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:43 INFO 140149161203520] Epoch[21] Batch [5]#011Speed: 320.07 samples/sec#011loss=3.826273\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:44 INFO 140149161203520] processed a total of 619 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2280.344009399414, \"sum\": 2280.344009399414, \"min\": 2280.344009399414}}, \"EndTime\": 1588626884.118023, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626881.837187}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:44 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=271.434917962 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:44 INFO 140149161203520] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:44 INFO 140149161203520] #quality_metric: host=algo-1, epoch=21, train loss <loss>=3.7796548605\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:44 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:44 INFO 140149161203520] Epoch[22] Batch[0] avg_epoch_loss=3.838287\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:44 INFO 140149161203520] #quality_metric: host=algo-1, epoch=22, batch=0 train loss <loss>=3.8382871151\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:45 INFO 140149161203520] Epoch[22] Batch[5] avg_epoch_loss=3.840261\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:45 INFO 140149161203520] #quality_metric: host=algo-1, epoch=22, batch=5 train loss <loss>=3.84026078383\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:45 INFO 140149161203520] Epoch[22] Batch [5]#011Speed: 314.19 samples/sec#011loss=3.840261\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:46 INFO 140149161203520] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2267.329216003418, \"sum\": 2267.329216003418, \"min\": 2267.329216003418}}, \"EndTime\": 1588626886.385958, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626884.118105}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:46 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=272.111117093 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:46 INFO 140149161203520] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:46 INFO 140149161203520] #quality_metric: host=algo-1, epoch=22, train loss <loss>=3.84851062298\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:46 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:46 INFO 140149161203520] Epoch[23] Batch[0] avg_epoch_loss=3.853056\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:46 INFO 140149161203520] #quality_metric: host=algo-1, epoch=23, batch=0 train loss <loss>=3.85305595398\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:47 INFO 140149161203520] Epoch[23] Batch[5] avg_epoch_loss=3.782525\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:47 INFO 140149161203520] #quality_metric: host=algo-1, epoch=23, batch=5 train loss <loss>=3.78252526124\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:47 INFO 140149161203520] Epoch[23] Batch [5]#011Speed: 317.77 samples/sec#011loss=3.782525\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:48 INFO 140149161203520] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2277.2421836853027, \"sum\": 2277.2421836853027, \"min\": 2277.2421836853027}}, \"EndTime\": 1588626888.663809, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626886.38604}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:48 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=276.633329271 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:48 INFO 140149161203520] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:48 INFO 140149161203520] #quality_metric: host=algo-1, epoch=23, train loss <loss>=3.79718387127\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:48 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:49 INFO 140149161203520] Epoch[24] Batch[0] avg_epoch_loss=3.674652\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:49 INFO 140149161203520] #quality_metric: host=algo-1, epoch=24, batch=0 train loss <loss>=3.67465209961\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:50 INFO 140149161203520] Epoch[24] Batch[5] avg_epoch_loss=3.835094\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:50 INFO 140149161203520] #quality_metric: host=algo-1, epoch=24, batch=5 train loss <loss>=3.83509421349\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:50 INFO 140149161203520] Epoch[24] Batch [5]#011Speed: 321.29 samples/sec#011loss=3.835094\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:51 INFO 140149161203520] Epoch[24] Batch[10] avg_epoch_loss=3.763672\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:51 INFO 140149161203520] #quality_metric: host=algo-1, epoch=24, batch=10 train loss <loss>=3.67796530724\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:51 INFO 140149161203520] Epoch[24] Batch [10]#011Speed: 314.06 samples/sec#011loss=3.677965\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:51 INFO 140149161203520] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2468.3330059051514, \"sum\": 2468.3330059051514, \"min\": 2468.3330059051514}}, \"EndTime\": 1588626891.132774, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626888.663906}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:51 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=262.106527274 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:51 INFO 140149161203520] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:51 INFO 140149161203520] #quality_metric: host=algo-1, epoch=24, train loss <loss>=3.76367198337\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:51 INFO 140149161203520] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:51 INFO 140149161203520] Saved checkpoint to \"/opt/ml/model/state_6b0763c6-b3de-4c71-9d3e-2bb3f722089c-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 87.44096755981445, \"sum\": 87.44096755981445, \"min\": 87.44096755981445}}, \"EndTime\": 1588626891.220815, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626891.132863}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:51 INFO 140149161203520] Epoch[25] Batch[0] avg_epoch_loss=3.922602\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:51 INFO 140149161203520] #quality_metric: host=algo-1, epoch=25, batch=0 train loss <loss>=3.92260193825\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:52 INFO 140149161203520] Epoch[25] Batch[5] avg_epoch_loss=3.822298\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:52 INFO 140149161203520] #quality_metric: host=algo-1, epoch=25, batch=5 train loss <loss>=3.8222976923\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:52 INFO 140149161203520] Epoch[25] Batch [5]#011Speed: 315.57 samples/sec#011loss=3.822298\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:53 INFO 140149161203520] processed a total of 621 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2281.651020050049, \"sum\": 2281.651020050049, \"min\": 2281.651020050049}}, \"EndTime\": 1588626893.502598, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626891.220874}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:53 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=272.153928876 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:53 INFO 140149161203520] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:53 INFO 140149161203520] #quality_metric: host=algo-1, epoch=25, train loss <loss>=3.87451972961\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:53 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:53 INFO 140149161203520] Epoch[26] Batch[0] avg_epoch_loss=3.880462\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:53 INFO 140149161203520] #quality_metric: host=algo-1, epoch=26, batch=0 train loss <loss>=3.88046216965\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:54 INFO 140149161203520] Epoch[26] Batch[5] avg_epoch_loss=3.773812\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:54 INFO 140149161203520] #quality_metric: host=algo-1, epoch=26, batch=5 train loss <loss>=3.7738121748\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:54 INFO 140149161203520] Epoch[26] Batch [5]#011Speed: 322.01 samples/sec#011loss=3.773812\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:55 INFO 140149161203520] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2247.358798980713, \"sum\": 2247.358798980713, \"min\": 2247.358798980713}}, \"EndTime\": 1588626895.750612, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626893.502693}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:55 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=277.19712773 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:55 INFO 140149161203520] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:55 INFO 140149161203520] #quality_metric: host=algo-1, epoch=26, train loss <loss>=3.81451842785\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:55 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:56 INFO 140149161203520] Epoch[27] Batch[0] avg_epoch_loss=3.720328\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:56 INFO 140149161203520] #quality_metric: host=algo-1, epoch=27, batch=0 train loss <loss>=3.72032833099\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:57 INFO 140149161203520] Epoch[27] Batch[5] avg_epoch_loss=3.801495\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:57 INFO 140149161203520] #quality_metric: host=algo-1, epoch=27, batch=5 train loss <loss>=3.80149451892\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:57 INFO 140149161203520] Epoch[27] Batch [5]#011Speed: 317.09 samples/sec#011loss=3.801495\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:58 INFO 140149161203520] Epoch[27] Batch[10] avg_epoch_loss=3.833173\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:58 INFO 140149161203520] #quality_metric: host=algo-1, epoch=27, batch=10 train loss <loss>=3.87118821144\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:58 INFO 140149161203520] Epoch[27] Batch [10]#011Speed: 318.08 samples/sec#011loss=3.871188\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:58 INFO 140149161203520] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2467.1831130981445, \"sum\": 2467.1831130981445, \"min\": 2467.1831130981445}}, \"EndTime\": 1588626898.218458, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626895.750709}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:58 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=262.228783591 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:58 INFO 140149161203520] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:58 INFO 140149161203520] #quality_metric: host=algo-1, epoch=27, train loss <loss>=3.83317347006\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:58 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:58 INFO 140149161203520] Epoch[28] Batch[0] avg_epoch_loss=3.733549\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:58 INFO 140149161203520] #quality_metric: host=algo-1, epoch=28, batch=0 train loss <loss>=3.7335486412\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:59 INFO 140149161203520] Epoch[28] Batch[5] avg_epoch_loss=3.757554\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:59 INFO 140149161203520] #quality_metric: host=algo-1, epoch=28, batch=5 train loss <loss>=3.75755449136\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:14:59 INFO 140149161203520] Epoch[28] Batch [5]#011Speed: 318.87 samples/sec#011loss=3.757554\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:00 INFO 140149161203520] Epoch[28] Batch[10] avg_epoch_loss=3.706771\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:00 INFO 140149161203520] #quality_metric: host=algo-1, epoch=28, batch=10 train loss <loss>=3.64583182335\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:00 INFO 140149161203520] Epoch[28] Batch [10]#011Speed: 316.89 samples/sec#011loss=3.645832\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:00 INFO 140149161203520] processed a total of 662 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2513.5579109191895, \"sum\": 2513.5579109191895, \"min\": 2513.5579109191895}}, \"EndTime\": 1588626900.732544, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626898.218546}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:00 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=263.356928725 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:00 INFO 140149161203520] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:00 INFO 140149161203520] #quality_metric: host=algo-1, epoch=28, train loss <loss>=3.70677146045\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:00 INFO 140149161203520] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:00 INFO 140149161203520] Saved checkpoint to \"/opt/ml/model/state_ab4e37b6-7e77-4784-b054-f8f59de6e8d7-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 60.59598922729492, \"sum\": 60.59598922729492, \"min\": 60.59598922729492}}, \"EndTime\": 1588626900.793773, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626900.73264}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:01 INFO 140149161203520] Epoch[29] Batch[0] avg_epoch_loss=3.848319\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:01 INFO 140149161203520] #quality_metric: host=algo-1, epoch=29, batch=0 train loss <loss>=3.84831881523\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:02 INFO 140149161203520] Epoch[29] Batch[5] avg_epoch_loss=3.713708\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:02 INFO 140149161203520] #quality_metric: host=algo-1, epoch=29, batch=5 train loss <loss>=3.71370772521\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:02 INFO 140149161203520] Epoch[29] Batch [5]#011Speed: 303.21 samples/sec#011loss=3.713708\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:03 INFO 140149161203520] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2326.0059356689453, \"sum\": 2326.0059356689453, \"min\": 2326.0059356689453}}, \"EndTime\": 1588626903.119919, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626900.79385}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:03 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=272.554540965 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:03 INFO 140149161203520] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:03 INFO 140149161203520] #quality_metric: host=algo-1, epoch=29, train loss <loss>=3.7455725193\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:03 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:03 INFO 140149161203520] Epoch[30] Batch[0] avg_epoch_loss=3.891387\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:03 INFO 140149161203520] #quality_metric: host=algo-1, epoch=30, batch=0 train loss <loss>=3.89138650894\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:04 INFO 140149161203520] Epoch[30] Batch[5] avg_epoch_loss=3.796505\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:04 INFO 140149161203520] #quality_metric: host=algo-1, epoch=30, batch=5 train loss <loss>=3.79650489489\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:04 INFO 140149161203520] Epoch[30] Batch [5]#011Speed: 308.64 samples/sec#011loss=3.796505\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:05 INFO 140149161203520] Epoch[30] Batch[10] avg_epoch_loss=3.839573\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:05 INFO 140149161203520] #quality_metric: host=algo-1, epoch=30, batch=10 train loss <loss>=3.89125542641\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:05 INFO 140149161203520] Epoch[30] Batch [10]#011Speed: 324.77 samples/sec#011loss=3.891255\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:05 INFO 140149161203520] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2489.8011684417725, \"sum\": 2489.8011684417725, \"min\": 2489.8011684417725}}, \"EndTime\": 1588626905.610255, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626903.120014}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:05 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=260.649781951 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:05 INFO 140149161203520] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:05 INFO 140149161203520] #quality_metric: host=algo-1, epoch=30, train loss <loss>=3.83957331831\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:05 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:06 INFO 140149161203520] Epoch[31] Batch[0] avg_epoch_loss=3.673983\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:06 INFO 140149161203520] #quality_metric: host=algo-1, epoch=31, batch=0 train loss <loss>=3.67398309708\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:07 INFO 140149161203520] Epoch[31] Batch[5] avg_epoch_loss=3.811371\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:07 INFO 140149161203520] #quality_metric: host=algo-1, epoch=31, batch=5 train loss <loss>=3.81137132645\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:07 INFO 140149161203520] Epoch[31] Batch [5]#011Speed: 323.03 samples/sec#011loss=3.811371\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:08 INFO 140149161203520] Epoch[31] Batch[10] avg_epoch_loss=3.869594\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:08 INFO 140149161203520] #quality_metric: host=algo-1, epoch=31, batch=10 train loss <loss>=3.93946037292\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:08 INFO 140149161203520] Epoch[31] Batch [10]#011Speed: 314.89 samples/sec#011loss=3.939460\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:08 INFO 140149161203520] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2481.556177139282, \"sum\": 2481.556177139282, \"min\": 2481.556177139282}}, \"EndTime\": 1588626908.092345, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626905.610344}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:08 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=264.739794843 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:08 INFO 140149161203520] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:08 INFO 140149161203520] #quality_metric: host=algo-1, epoch=31, train loss <loss>=3.8695936203\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:08 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:08 INFO 140149161203520] Epoch[32] Batch[0] avg_epoch_loss=3.870207\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:08 INFO 140149161203520] #quality_metric: host=algo-1, epoch=32, batch=0 train loss <loss>=3.8702070713\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:09 INFO 140149161203520] Epoch[32] Batch[5] avg_epoch_loss=3.779840\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:09 INFO 140149161203520] #quality_metric: host=algo-1, epoch=32, batch=5 train loss <loss>=3.77984023094\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:09 INFO 140149161203520] Epoch[32] Batch [5]#011Speed: 321.53 samples/sec#011loss=3.779840\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:10 INFO 140149161203520] Epoch[32] Batch[10] avg_epoch_loss=3.680970\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:10 INFO 140149161203520] #quality_metric: host=algo-1, epoch=32, batch=10 train loss <loss>=3.56232514381\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:10 INFO 140149161203520] Epoch[32] Batch [10]#011Speed: 318.17 samples/sec#011loss=3.562325\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:10 INFO 140149161203520] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2481.498956680298, \"sum\": 2481.498956680298, \"min\": 2481.498956680298}}, \"EndTime\": 1588626910.574363, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626908.092432}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:10 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=258.701124242 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:10 INFO 140149161203520] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:10 INFO 140149161203520] #quality_metric: host=algo-1, epoch=32, train loss <loss>=3.68096973679\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:10 INFO 140149161203520] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:10 INFO 140149161203520] Saved checkpoint to \"/opt/ml/model/state_cfbc53cb-2cef-4102-9946-3dd914a24d37-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 65.95897674560547, \"sum\": 65.95897674560547, \"min\": 65.95897674560547}}, \"EndTime\": 1588626910.64094, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626910.574452}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:11 INFO 140149161203520] Epoch[33] Batch[0] avg_epoch_loss=3.845461\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:11 INFO 140149161203520] #quality_metric: host=algo-1, epoch=33, batch=0 train loss <loss>=3.84546065331\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:12 INFO 140149161203520] Epoch[33] Batch[5] avg_epoch_loss=3.753320\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:12 INFO 140149161203520] #quality_metric: host=algo-1, epoch=33, batch=5 train loss <loss>=3.75331954161\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:12 INFO 140149161203520] Epoch[33] Batch [5]#011Speed: 319.33 samples/sec#011loss=3.753320\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:12 INFO 140149161203520] processed a total of 618 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2255.753993988037, \"sum\": 2255.753993988037, \"min\": 2255.753993988037}}, \"EndTime\": 1588626912.896828, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626910.641009}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:12 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=273.949540726 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:12 INFO 140149161203520] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:12 INFO 140149161203520] #quality_metric: host=algo-1, epoch=33, train loss <loss>=3.77779266834\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:12 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:13 INFO 140149161203520] Epoch[34] Batch[0] avg_epoch_loss=3.710939\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:13 INFO 140149161203520] #quality_metric: host=algo-1, epoch=34, batch=0 train loss <loss>=3.71093916893\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:14 INFO 140149161203520] Epoch[34] Batch[5] avg_epoch_loss=3.738818\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:14 INFO 140149161203520] #quality_metric: host=algo-1, epoch=34, batch=5 train loss <loss>=3.73881840706\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:14 INFO 140149161203520] Epoch[34] Batch [5]#011Speed: 313.08 samples/sec#011loss=3.738818\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:15 INFO 140149161203520] Epoch[34] Batch[10] avg_epoch_loss=3.641838\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:15 INFO 140149161203520] #quality_metric: host=algo-1, epoch=34, batch=10 train loss <loss>=3.52546191216\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:15 INFO 140149161203520] Epoch[34] Batch [10]#011Speed: 323.00 samples/sec#011loss=3.525462\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:15 INFO 140149161203520] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2476.544141769409, \"sum\": 2476.544141769409, \"min\": 2476.544141769409}}, \"EndTime\": 1588626915.373946, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626912.896924}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:15 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=260.026139583 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:15 INFO 140149161203520] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:15 INFO 140149161203520] #quality_metric: host=algo-1, epoch=34, train loss <loss>=3.6418381821\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:15 INFO 140149161203520] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:15 INFO 140149161203520] Saved checkpoint to \"/opt/ml/model/state_9be8b5e5-95fc-49bc-b981-f14dcdda8dcf-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 65.64903259277344, \"sum\": 65.64903259277344, \"min\": 65.64903259277344}}, \"EndTime\": 1588626915.440203, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626915.374036}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:15 INFO 140149161203520] Epoch[35] Batch[0] avg_epoch_loss=3.553849\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:15 INFO 140149161203520] #quality_metric: host=algo-1, epoch=35, batch=0 train loss <loss>=3.55384850502\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:16 INFO 140149161203520] Epoch[35] Batch[5] avg_epoch_loss=3.660824\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:16 INFO 140149161203520] #quality_metric: host=algo-1, epoch=35, batch=5 train loss <loss>=3.66082386176\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:16 INFO 140149161203520] Epoch[35] Batch [5]#011Speed: 321.23 samples/sec#011loss=3.660824\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:17 INFO 140149161203520] Epoch[35] Batch[10] avg_epoch_loss=3.761460\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:17 INFO 140149161203520] #quality_metric: host=algo-1, epoch=35, batch=10 train loss <loss>=3.88222403526\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:17 INFO 140149161203520] Epoch[35] Batch [10]#011Speed: 314.66 samples/sec#011loss=3.882224\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:17 INFO 140149161203520] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2478.835105895996, \"sum\": 2478.835105895996, \"min\": 2478.835105895996}}, \"EndTime\": 1588626917.919184, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626915.440285}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:17 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=258.979244387 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:17 INFO 140149161203520] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:17 INFO 140149161203520] #quality_metric: host=algo-1, epoch=35, train loss <loss>=3.76146030426\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:17 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:18 INFO 140149161203520] Epoch[36] Batch[0] avg_epoch_loss=3.703848\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:18 INFO 140149161203520] #quality_metric: host=algo-1, epoch=36, batch=0 train loss <loss>=3.70384836197\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:19 INFO 140149161203520] Epoch[36] Batch[5] avg_epoch_loss=3.791855\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:19 INFO 140149161203520] #quality_metric: host=algo-1, epoch=36, batch=5 train loss <loss>=3.79185545444\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:19 INFO 140149161203520] Epoch[36] Batch [5]#011Speed: 319.22 samples/sec#011loss=3.791855\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:20 INFO 140149161203520] Epoch[36] Batch[10] avg_epoch_loss=3.866155\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:20 INFO 140149161203520] #quality_metric: host=algo-1, epoch=36, batch=10 train loss <loss>=3.95531539917\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:20 INFO 140149161203520] Epoch[36] Batch [10]#011Speed: 324.15 samples/sec#011loss=3.955315\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:20 INFO 140149161203520] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2459.5839977264404, \"sum\": 2459.5839977264404, \"min\": 2459.5839977264404}}, \"EndTime\": 1588626920.379324, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626917.919273}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:20 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=262.632166523 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:20 INFO 140149161203520] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:20 INFO 140149161203520] #quality_metric: host=algo-1, epoch=36, train loss <loss>=3.86615542932\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:20 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:20 INFO 140149161203520] Epoch[37] Batch[0] avg_epoch_loss=3.962897\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:20 INFO 140149161203520] #quality_metric: host=algo-1, epoch=37, batch=0 train loss <loss>=3.9628970623\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:21 INFO 140149161203520] Epoch[37] Batch[5] avg_epoch_loss=3.837144\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:21 INFO 140149161203520] #quality_metric: host=algo-1, epoch=37, batch=5 train loss <loss>=3.83714397748\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:21 INFO 140149161203520] Epoch[37] Batch [5]#011Speed: 319.12 samples/sec#011loss=3.837144\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:22 INFO 140149161203520] processed a total of 615 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2319.897174835205, \"sum\": 2319.897174835205, \"min\": 2319.897174835205}}, \"EndTime\": 1588626922.69975, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626920.379414}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:22 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=265.083926215 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:22 INFO 140149161203520] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:22 INFO 140149161203520] #quality_metric: host=algo-1, epoch=37, train loss <loss>=3.78685154915\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:22 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:23 INFO 140149161203520] Epoch[38] Batch[0] avg_epoch_loss=3.754203\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:23 INFO 140149161203520] #quality_metric: host=algo-1, epoch=38, batch=0 train loss <loss>=3.75420284271\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:24 INFO 140149161203520] Epoch[38] Batch[5] avg_epoch_loss=3.743546\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:24 INFO 140149161203520] #quality_metric: host=algo-1, epoch=38, batch=5 train loss <loss>=3.74354628722\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:24 INFO 140149161203520] Epoch[38] Batch [5]#011Speed: 310.73 samples/sec#011loss=3.743546\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:25 INFO 140149161203520] Epoch[38] Batch[10] avg_epoch_loss=3.670322\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:25 INFO 140149161203520] #quality_metric: host=algo-1, epoch=38, batch=10 train loss <loss>=3.5824529171\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:25 INFO 140149161203520] Epoch[38] Batch [10]#011Speed: 316.94 samples/sec#011loss=3.582453\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:25 INFO 140149161203520] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2492.72084236145, \"sum\": 2492.72084236145, \"min\": 2492.72084236145}}, \"EndTime\": 1588626925.19307, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626922.699836}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:25 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=258.742489004 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:25 INFO 140149161203520] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:25 INFO 140149161203520] #quality_metric: host=algo-1, epoch=38, train loss <loss>=3.67032202807\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:25 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:25 INFO 140149161203520] Epoch[39] Batch[0] avg_epoch_loss=3.895843\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:25 INFO 140149161203520] #quality_metric: host=algo-1, epoch=39, batch=0 train loss <loss>=3.89584255219\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:26 INFO 140149161203520] Epoch[39] Batch[5] avg_epoch_loss=3.741309\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:26 INFO 140149161203520] #quality_metric: host=algo-1, epoch=39, batch=5 train loss <loss>=3.74130856991\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:26 INFO 140149161203520] Epoch[39] Batch [5]#011Speed: 323.05 samples/sec#011loss=3.741309\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:27 INFO 140149161203520] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2246.3371753692627, \"sum\": 2246.3371753692627, \"min\": 2246.3371753692627}}, \"EndTime\": 1588626927.440003, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626925.193137}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:27 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=281.774274454 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:27 INFO 140149161203520] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:27 INFO 140149161203520] #quality_metric: host=algo-1, epoch=39, train loss <loss>=3.73932971954\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:27 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:27 INFO 140149161203520] Epoch[40] Batch[0] avg_epoch_loss=3.735651\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:27 INFO 140149161203520] #quality_metric: host=algo-1, epoch=40, batch=0 train loss <loss>=3.73565101624\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:28 INFO 140149161203520] Epoch[40] Batch[5] avg_epoch_loss=3.788528\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:28 INFO 140149161203520] #quality_metric: host=algo-1, epoch=40, batch=5 train loss <loss>=3.78852776686\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:28 INFO 140149161203520] Epoch[40] Batch [5]#011Speed: 319.67 samples/sec#011loss=3.788528\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:29 INFO 140149161203520] processed a total of 616 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2308.238983154297, \"sum\": 2308.238983154297, \"min\": 2308.238983154297}}, \"EndTime\": 1588626929.748831, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626927.440102}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:29 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=266.855312124 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:29 INFO 140149161203520] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:29 INFO 140149161203520] #quality_metric: host=algo-1, epoch=40, train loss <loss>=3.71963701248\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:29 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:30 INFO 140149161203520] Epoch[41] Batch[0] avg_epoch_loss=3.913870\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:30 INFO 140149161203520] #quality_metric: host=algo-1, epoch=41, batch=0 train loss <loss>=3.91387033463\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:31 INFO 140149161203520] Epoch[41] Batch[5] avg_epoch_loss=3.768397\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:31 INFO 140149161203520] #quality_metric: host=algo-1, epoch=41, batch=5 train loss <loss>=3.76839749018\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:31 INFO 140149161203520] Epoch[41] Batch [5]#011Speed: 315.79 samples/sec#011loss=3.768397\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:32 INFO 140149161203520] Epoch[41] Batch[10] avg_epoch_loss=3.733106\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:32 INFO 140149161203520] #quality_metric: host=algo-1, epoch=41, batch=10 train loss <loss>=3.69075727463\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:32 INFO 140149161203520] Epoch[41] Batch [10]#011Speed: 316.99 samples/sec#011loss=3.690757\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:32 INFO 140149161203520] processed a total of 692 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2483.464002609253, \"sum\": 2483.464002609253, \"min\": 2483.464002609253}}, \"EndTime\": 1588626932.232918, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626929.748915}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:32 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=278.63016324 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:32 INFO 140149161203520] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:32 INFO 140149161203520] #quality_metric: host=algo-1, epoch=41, train loss <loss>=3.73310648311\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:32 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:32 INFO 140149161203520] Epoch[42] Batch[0] avg_epoch_loss=3.730074\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:32 INFO 140149161203520] #quality_metric: host=algo-1, epoch=42, batch=0 train loss <loss>=3.73007392883\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:33 INFO 140149161203520] Epoch[42] Batch[5] avg_epoch_loss=3.698816\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:33 INFO 140149161203520] #quality_metric: host=algo-1, epoch=42, batch=5 train loss <loss>=3.69881554445\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:33 INFO 140149161203520] Epoch[42] Batch [5]#011Speed: 316.63 samples/sec#011loss=3.698816\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:34 INFO 140149161203520] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2321.7339515686035, \"sum\": 2321.7339515686035, \"min\": 2321.7339515686035}}, \"EndTime\": 1588626934.555187, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626932.232997}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:34 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=270.471079921 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:34 INFO 140149161203520] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:34 INFO 140149161203520] #quality_metric: host=algo-1, epoch=42, train loss <loss>=3.72034904957\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:34 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:35 INFO 140149161203520] Epoch[43] Batch[0] avg_epoch_loss=3.825273\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:35 INFO 140149161203520] #quality_metric: host=algo-1, epoch=43, batch=0 train loss <loss>=3.82527303696\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:36 INFO 140149161203520] Epoch[43] Batch[5] avg_epoch_loss=3.707082\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:36 INFO 140149161203520] #quality_metric: host=algo-1, epoch=43, batch=5 train loss <loss>=3.707081755\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:36 INFO 140149161203520] Epoch[43] Batch [5]#011Speed: 311.86 samples/sec#011loss=3.707082\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:37 INFO 140149161203520] Epoch[43] Batch[10] avg_epoch_loss=3.585294\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:37 INFO 140149161203520] #quality_metric: host=algo-1, epoch=43, batch=10 train loss <loss>=3.43914871216\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:37 INFO 140149161203520] Epoch[43] Batch [10]#011Speed: 322.37 samples/sec#011loss=3.439149\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:37 INFO 140149161203520] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2488.558053970337, \"sum\": 2488.558053970337, \"min\": 2488.558053970337}}, \"EndTime\": 1588626937.044337, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626934.555285}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:37 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=259.978010484 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:37 INFO 140149161203520] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:37 INFO 140149161203520] #quality_metric: host=algo-1, epoch=43, train loss <loss>=3.58529400826\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:37 INFO 140149161203520] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:37 INFO 140149161203520] Saved checkpoint to \"/opt/ml/model/state_0de85bf0-f888-4bfc-aed4-cb40f60cb86f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 58.959007263183594, \"sum\": 58.959007263183594, \"min\": 58.959007263183594}}, \"EndTime\": 1588626937.103919, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626937.044408}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:37 INFO 140149161203520] Epoch[44] Batch[0] avg_epoch_loss=3.616749\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:37 INFO 140149161203520] #quality_metric: host=algo-1, epoch=44, batch=0 train loss <loss>=3.61674880981\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:38 INFO 140149161203520] Epoch[44] Batch[5] avg_epoch_loss=3.664316\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:38 INFO 140149161203520] #quality_metric: host=algo-1, epoch=44, batch=5 train loss <loss>=3.66431641579\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:38 INFO 140149161203520] Epoch[44] Batch [5]#011Speed: 321.52 samples/sec#011loss=3.664316\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:39 INFO 140149161203520] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2241.0030364990234, \"sum\": 2241.0030364990234, \"min\": 2241.0030364990234}}, \"EndTime\": 1588626939.345083, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626937.104006}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:39 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=282.44465371 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:39 INFO 140149161203520] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:39 INFO 140149161203520] #quality_metric: host=algo-1, epoch=44, train loss <loss>=3.68729653358\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:39 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:39 INFO 140149161203520] Epoch[45] Batch[0] avg_epoch_loss=3.895697\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:39 INFO 140149161203520] #quality_metric: host=algo-1, epoch=45, batch=0 train loss <loss>=3.89569735527\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:40 INFO 140149161203520] Epoch[45] Batch[5] avg_epoch_loss=3.708525\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:40 INFO 140149161203520] #quality_metric: host=algo-1, epoch=45, batch=5 train loss <loss>=3.70852522055\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:40 INFO 140149161203520] Epoch[45] Batch [5]#011Speed: 318.50 samples/sec#011loss=3.708525\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:41 INFO 140149161203520] Epoch[45] Batch[10] avg_epoch_loss=3.621738\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:41 INFO 140149161203520] #quality_metric: host=algo-1, epoch=45, batch=10 train loss <loss>=3.51759314537\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:41 INFO 140149161203520] Epoch[45] Batch [10]#011Speed: 323.25 samples/sec#011loss=3.517593\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:41 INFO 140149161203520] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2441.2128925323486, \"sum\": 2441.2128925323486, \"min\": 2441.2128925323486}}, \"EndTime\": 1588626941.786895, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626939.345182}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:41 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=263.379538812 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:41 INFO 140149161203520] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:41 INFO 140149161203520] #quality_metric: host=algo-1, epoch=45, train loss <loss>=3.62173791365\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:41 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:42 INFO 140149161203520] Epoch[46] Batch[0] avg_epoch_loss=3.519441\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:42 INFO 140149161203520] #quality_metric: host=algo-1, epoch=46, batch=0 train loss <loss>=3.51944065094\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:43 INFO 140149161203520] Epoch[46] Batch[5] avg_epoch_loss=3.708484\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:43 INFO 140149161203520] #quality_metric: host=algo-1, epoch=46, batch=5 train loss <loss>=3.70848421256\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:43 INFO 140149161203520] Epoch[46] Batch [5]#011Speed: 316.14 samples/sec#011loss=3.708484\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:44 INFO 140149161203520] processed a total of 618 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2303.093194961548, \"sum\": 2303.093194961548, \"min\": 2303.093194961548}}, \"EndTime\": 1588626944.090553, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626941.786984}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:44 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=268.31825042 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:44 INFO 140149161203520] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:44 INFO 140149161203520] #quality_metric: host=algo-1, epoch=46, train loss <loss>=3.73579039574\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:44 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:44 INFO 140149161203520] Epoch[47] Batch[0] avg_epoch_loss=3.567680\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:44 INFO 140149161203520] #quality_metric: host=algo-1, epoch=47, batch=0 train loss <loss>=3.56767988205\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:45 INFO 140149161203520] Epoch[47] Batch[5] avg_epoch_loss=3.599106\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:45 INFO 140149161203520] #quality_metric: host=algo-1, epoch=47, batch=5 train loss <loss>=3.59910551707\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:45 INFO 140149161203520] Epoch[47] Batch [5]#011Speed: 314.42 samples/sec#011loss=3.599106\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:46 INFO 140149161203520] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2294.2779064178467, \"sum\": 2294.2779064178467, \"min\": 2294.2779064178467}}, \"EndTime\": 1588626946.385451, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626944.09065}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:46 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=275.887318031 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:46 INFO 140149161203520] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:46 INFO 140149161203520] #quality_metric: host=algo-1, epoch=47, train loss <loss>=3.67025182247\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:46 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:46 INFO 140149161203520] Epoch[48] Batch[0] avg_epoch_loss=3.764888\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:46 INFO 140149161203520] #quality_metric: host=algo-1, epoch=48, batch=0 train loss <loss>=3.76488804817\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:47 INFO 140149161203520] Epoch[48] Batch[5] avg_epoch_loss=3.713163\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:47 INFO 140149161203520] #quality_metric: host=algo-1, epoch=48, batch=5 train loss <loss>=3.71316262086\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:47 INFO 140149161203520] Epoch[48] Batch [5]#011Speed: 320.51 samples/sec#011loss=3.713163\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:48 INFO 140149161203520] Epoch[48] Batch[10] avg_epoch_loss=3.685813\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:48 INFO 140149161203520] #quality_metric: host=algo-1, epoch=48, batch=10 train loss <loss>=3.65299263\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:48 INFO 140149161203520] Epoch[48] Batch [10]#011Speed: 321.64 samples/sec#011loss=3.652993\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:48 INFO 140149161203520] processed a total of 681 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2450.4239559173584, \"sum\": 2450.4239559173584, \"min\": 2450.4239559173584}}, \"EndTime\": 1588626948.836454, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626946.385547}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:48 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=277.896357087 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:48 INFO 140149161203520] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:48 INFO 140149161203520] #quality_metric: host=algo-1, epoch=48, train loss <loss>=3.68581262502\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:48 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:49 INFO 140149161203520] Epoch[49] Batch[0] avg_epoch_loss=3.585194\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:49 INFO 140149161203520] #quality_metric: host=algo-1, epoch=49, batch=0 train loss <loss>=3.58519387245\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:50 INFO 140149161203520] Epoch[49] Batch[5] avg_epoch_loss=3.687721\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:50 INFO 140149161203520] #quality_metric: host=algo-1, epoch=49, batch=5 train loss <loss>=3.68772085508\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:50 INFO 140149161203520] Epoch[49] Batch [5]#011Speed: 319.97 samples/sec#011loss=3.687721\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:51 INFO 140149161203520] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2290.60697555542, \"sum\": 2290.60697555542, \"min\": 2290.60697555542}}, \"EndTime\": 1588626951.127608, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626948.836543}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:51 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=277.641474551 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:51 INFO 140149161203520] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:51 INFO 140149161203520] #quality_metric: host=algo-1, epoch=49, train loss <loss>=3.67201628685\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:51 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:51 INFO 140149161203520] Epoch[50] Batch[0] avg_epoch_loss=3.627627\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:51 INFO 140149161203520] #quality_metric: host=algo-1, epoch=50, batch=0 train loss <loss>=3.62762713432\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:52 INFO 140149161203520] Epoch[50] Batch[5] avg_epoch_loss=3.691850\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:52 INFO 140149161203520] #quality_metric: host=algo-1, epoch=50, batch=5 train loss <loss>=3.69185014566\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:52 INFO 140149161203520] Epoch[50] Batch [5]#011Speed: 323.01 samples/sec#011loss=3.691850\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:53 INFO 140149161203520] Epoch[50] Batch[10] avg_epoch_loss=3.621164\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:53 INFO 140149161203520] #quality_metric: host=algo-1, epoch=50, batch=10 train loss <loss>=3.53634057045\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:53 INFO 140149161203520] Epoch[50] Batch [10]#011Speed: 323.13 samples/sec#011loss=3.536341\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:53 INFO 140149161203520] processed a total of 666 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2438.70210647583, \"sum\": 2438.70210647583, \"min\": 2438.70210647583}}, \"EndTime\": 1588626953.566901, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626951.127689}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:53 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=273.081749248 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:53 INFO 140149161203520] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:53 INFO 140149161203520] #quality_metric: host=algo-1, epoch=50, train loss <loss>=3.62116397511\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:53 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:54 INFO 140149161203520] Epoch[51] Batch[0] avg_epoch_loss=3.779361\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:54 INFO 140149161203520] #quality_metric: host=algo-1, epoch=51, batch=0 train loss <loss>=3.77936148643\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:55 INFO 140149161203520] Epoch[51] Batch[5] avg_epoch_loss=3.703921\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:55 INFO 140149161203520] #quality_metric: host=algo-1, epoch=51, batch=5 train loss <loss>=3.70392088095\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:55 INFO 140149161203520] Epoch[51] Batch [5]#011Speed: 325.74 samples/sec#011loss=3.703921\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:55 INFO 140149161203520] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2250.293970108032, \"sum\": 2250.293970108032, \"min\": 2250.293970108032}}, \"EndTime\": 1588626955.817781, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626953.56699}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:55 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=277.279946893 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:55 INFO 140149161203520] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:55 INFO 140149161203520] #quality_metric: host=algo-1, epoch=51, train loss <loss>=3.69996843338\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:55 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:56 INFO 140149161203520] Epoch[52] Batch[0] avg_epoch_loss=3.883396\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:56 INFO 140149161203520] #quality_metric: host=algo-1, epoch=52, batch=0 train loss <loss>=3.88339591026\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:57 INFO 140149161203520] Epoch[52] Batch[5] avg_epoch_loss=3.749800\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:57 INFO 140149161203520] #quality_metric: host=algo-1, epoch=52, batch=5 train loss <loss>=3.74980048339\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:57 INFO 140149161203520] Epoch[52] Batch [5]#011Speed: 324.68 samples/sec#011loss=3.749800\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:58 INFO 140149161203520] processed a total of 614 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2252.474069595337, \"sum\": 2252.474069595337, \"min\": 2252.474069595337}}, \"EndTime\": 1588626958.070863, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626955.817879}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:58 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=272.57233336 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:58 INFO 140149161203520] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:58 INFO 140149161203520] #quality_metric: host=algo-1, epoch=52, train loss <loss>=3.79773406982\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:58 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:58 INFO 140149161203520] Epoch[53] Batch[0] avg_epoch_loss=3.601796\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:58 INFO 140149161203520] #quality_metric: host=algo-1, epoch=53, batch=0 train loss <loss>=3.60179615021\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:59 INFO 140149161203520] Epoch[53] Batch[5] avg_epoch_loss=3.634019\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:59 INFO 140149161203520] #quality_metric: host=algo-1, epoch=53, batch=5 train loss <loss>=3.63401869933\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:15:59 INFO 140149161203520] Epoch[53] Batch [5]#011Speed: 324.94 samples/sec#011loss=3.634019\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:00 INFO 140149161203520] processed a total of 593 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2278.158187866211, \"sum\": 2278.158187866211, \"min\": 2278.158187866211}}, \"EndTime\": 1588626960.349666, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626958.07096}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:00 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=260.282246737 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:00 INFO 140149161203520] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:00 INFO 140149161203520] #quality_metric: host=algo-1, epoch=53, train loss <loss>=3.71303529739\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:00 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:00 INFO 140149161203520] Epoch[54] Batch[0] avg_epoch_loss=3.836178\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:00 INFO 140149161203520] #quality_metric: host=algo-1, epoch=54, batch=0 train loss <loss>=3.83617758751\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:01 INFO 140149161203520] Epoch[54] Batch[5] avg_epoch_loss=3.763993\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:01 INFO 140149161203520] #quality_metric: host=algo-1, epoch=54, batch=5 train loss <loss>=3.76399294535\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:01 INFO 140149161203520] Epoch[54] Batch [5]#011Speed: 317.71 samples/sec#011loss=3.763993\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:02 INFO 140149161203520] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2276.238203048706, \"sum\": 2276.238203048706, \"min\": 2276.238203048706}}, \"EndTime\": 1588626962.626499, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626960.349762}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:02 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=273.680621101 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:02 INFO 140149161203520] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:02 INFO 140149161203520] #quality_metric: host=algo-1, epoch=54, train loss <loss>=3.69593734741\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:02 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:03 INFO 140149161203520] Epoch[55] Batch[0] avg_epoch_loss=3.641074\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:03 INFO 140149161203520] #quality_metric: host=algo-1, epoch=55, batch=0 train loss <loss>=3.64107370377\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:04 INFO 140149161203520] Epoch[55] Batch[5] avg_epoch_loss=3.687380\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:04 INFO 140149161203520] #quality_metric: host=algo-1, epoch=55, batch=5 train loss <loss>=3.68738027414\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:04 INFO 140149161203520] Epoch[55] Batch [5]#011Speed: 323.25 samples/sec#011loss=3.687380\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:05 INFO 140149161203520] Epoch[55] Batch[10] avg_epoch_loss=3.683356\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:05 INFO 140149161203520] #quality_metric: host=algo-1, epoch=55, batch=10 train loss <loss>=3.67852735519\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:05 INFO 140149161203520] Epoch[55] Batch [10]#011Speed: 311.69 samples/sec#011loss=3.678527\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:05 INFO 140149161203520] processed a total of 696 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2492.5880432128906, \"sum\": 2492.5880432128906, \"min\": 2492.5880432128906}}, \"EndTime\": 1588626965.11971, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626962.626596}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:05 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=279.213401391 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:05 INFO 140149161203520] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:05 INFO 140149161203520] #quality_metric: host=algo-1, epoch=55, train loss <loss>=3.68335622007\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:05 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:05 INFO 140149161203520] Epoch[56] Batch[0] avg_epoch_loss=3.624180\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:05 INFO 140149161203520] #quality_metric: host=algo-1, epoch=56, batch=0 train loss <loss>=3.62417960167\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:06 INFO 140149161203520] Epoch[56] Batch[5] avg_epoch_loss=3.632457\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:06 INFO 140149161203520] #quality_metric: host=algo-1, epoch=56, batch=5 train loss <loss>=3.6324565808\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:06 INFO 140149161203520] Epoch[56] Batch [5]#011Speed: 318.97 samples/sec#011loss=3.632457\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:07 INFO 140149161203520] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2266.378879547119, \"sum\": 2266.378879547119, \"min\": 2266.378879547119}}, \"EndTime\": 1588626967.386611, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626965.119798}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:07 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=277.522128779 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:07 INFO 140149161203520] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:07 INFO 140149161203520] #quality_metric: host=algo-1, epoch=56, train loss <loss>=3.64868671894\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:07 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:07 INFO 140149161203520] Epoch[57] Batch[0] avg_epoch_loss=3.750489\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:07 INFO 140149161203520] #quality_metric: host=algo-1, epoch=57, batch=0 train loss <loss>=3.75048899651\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:08 INFO 140149161203520] Epoch[57] Batch[5] avg_epoch_loss=3.633134\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:08 INFO 140149161203520] #quality_metric: host=algo-1, epoch=57, batch=5 train loss <loss>=3.63313408693\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:08 INFO 140149161203520] Epoch[57] Batch [5]#011Speed: 319.27 samples/sec#011loss=3.633134\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:09 INFO 140149161203520] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2251.708984375, \"sum\": 2251.708984375, \"min\": 2251.708984375}}, \"EndTime\": 1588626969.638916, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626967.386683}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:09 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=278.877097688 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:09 INFO 140149161203520] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:09 INFO 140149161203520] #quality_metric: host=algo-1, epoch=57, train loss <loss>=3.60396199226\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:09 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:10 INFO 140149161203520] Epoch[58] Batch[0] avg_epoch_loss=3.590420\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:10 INFO 140149161203520] #quality_metric: host=algo-1, epoch=58, batch=0 train loss <loss>=3.59041953087\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:11 INFO 140149161203520] Epoch[58] Batch[5] avg_epoch_loss=3.750328\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:11 INFO 140149161203520] #quality_metric: host=algo-1, epoch=58, batch=5 train loss <loss>=3.75032774607\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:11 INFO 140149161203520] Epoch[58] Batch [5]#011Speed: 320.85 samples/sec#011loss=3.750328\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:12 INFO 140149161203520] Epoch[58] Batch[10] avg_epoch_loss=3.783911\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:12 INFO 140149161203520] #quality_metric: host=algo-1, epoch=58, batch=10 train loss <loss>=3.82421021461\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:12 INFO 140149161203520] Epoch[58] Batch [10]#011Speed: 318.95 samples/sec#011loss=3.824210\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:12 INFO 140149161203520] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2457.4339389801025, \"sum\": 2457.4339389801025, \"min\": 2457.4339389801025}}, \"EndTime\": 1588626972.09698, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626969.639051}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:12 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=262.861707066 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:12 INFO 140149161203520] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:12 INFO 140149161203520] #quality_metric: host=algo-1, epoch=58, train loss <loss>=3.78391068632\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:12 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:12 INFO 140149161203520] Epoch[59] Batch[0] avg_epoch_loss=3.592513\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:12 INFO 140149161203520] #quality_metric: host=algo-1, epoch=59, batch=0 train loss <loss>=3.59251332283\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:13 INFO 140149161203520] Epoch[59] Batch[5] avg_epoch_loss=3.612059\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:13 INFO 140149161203520] #quality_metric: host=algo-1, epoch=59, batch=5 train loss <loss>=3.61205923557\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:13 INFO 140149161203520] Epoch[59] Batch [5]#011Speed: 317.33 samples/sec#011loss=3.612059\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:14 INFO 140149161203520] Epoch[59] Batch[10] avg_epoch_loss=3.690477\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:14 INFO 140149161203520] #quality_metric: host=algo-1, epoch=59, batch=10 train loss <loss>=3.78457808495\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:14 INFO 140149161203520] Epoch[59] Batch [10]#011Speed: 318.65 samples/sec#011loss=3.784578\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:14 INFO 140149161203520] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2469.8939323425293, \"sum\": 2469.8939323425293, \"min\": 2469.8939323425293}}, \"EndTime\": 1588626974.567432, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626972.097071}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:14 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=267.608956467 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:14 INFO 140149161203520] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:14 INFO 140149161203520] #quality_metric: host=algo-1, epoch=59, train loss <loss>=3.69047689438\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:14 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:15 INFO 140149161203520] Epoch[60] Batch[0] avg_epoch_loss=3.807926\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:15 INFO 140149161203520] #quality_metric: host=algo-1, epoch=60, batch=0 train loss <loss>=3.80792617798\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:16 INFO 140149161203520] Epoch[60] Batch[5] avg_epoch_loss=3.677966\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:16 INFO 140149161203520] #quality_metric: host=algo-1, epoch=60, batch=5 train loss <loss>=3.67796623707\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:16 INFO 140149161203520] Epoch[60] Batch [5]#011Speed: 325.92 samples/sec#011loss=3.677966\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:17 INFO 140149161203520] Epoch[60] Batch[10] avg_epoch_loss=3.650319\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:17 INFO 140149161203520] #quality_metric: host=algo-1, epoch=60, batch=10 train loss <loss>=3.61714186668\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:17 INFO 140149161203520] Epoch[60] Batch [10]#011Speed: 324.92 samples/sec#011loss=3.617142\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:17 INFO 140149161203520] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2464.8690223693848, \"sum\": 2464.8690223693848, \"min\": 2464.8690223693848}}, \"EndTime\": 1588626977.032832, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626974.567521}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:17 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=263.691896789 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:17 INFO 140149161203520] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:17 INFO 140149161203520] #quality_metric: host=algo-1, epoch=60, train loss <loss>=3.65031879598\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:17 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:17 INFO 140149161203520] Epoch[61] Batch[0] avg_epoch_loss=3.635991\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:17 INFO 140149161203520] #quality_metric: host=algo-1, epoch=61, batch=0 train loss <loss>=3.63599133492\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:18 INFO 140149161203520] Epoch[61] Batch[5] avg_epoch_loss=3.640431\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:18 INFO 140149161203520] #quality_metric: host=algo-1, epoch=61, batch=5 train loss <loss>=3.64043124517\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:18 INFO 140149161203520] Epoch[61] Batch [5]#011Speed: 314.69 samples/sec#011loss=3.640431\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:19 INFO 140149161203520] processed a total of 627 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2271.9929218292236, \"sum\": 2271.9929218292236, \"min\": 2271.9929218292236}}, \"EndTime\": 1588626979.305357, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626977.032923}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:19 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=275.954574707 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:19 INFO 140149161203520] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:19 INFO 140149161203520] #quality_metric: host=algo-1, epoch=61, train loss <loss>=3.66438937187\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:19 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:19 INFO 140149161203520] Epoch[62] Batch[0] avg_epoch_loss=3.799590\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:19 INFO 140149161203520] #quality_metric: host=algo-1, epoch=62, batch=0 train loss <loss>=3.79958987236\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:20 INFO 140149161203520] Epoch[62] Batch[5] avg_epoch_loss=3.637605\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:20 INFO 140149161203520] #quality_metric: host=algo-1, epoch=62, batch=5 train loss <loss>=3.63760475318\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:20 INFO 140149161203520] Epoch[62] Batch [5]#011Speed: 321.11 samples/sec#011loss=3.637605\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:21 INFO 140149161203520] Epoch[62] Batch[10] avg_epoch_loss=3.599768\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:21 INFO 140149161203520] #quality_metric: host=algo-1, epoch=62, batch=10 train loss <loss>=3.55436468124\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:21 INFO 140149161203520] Epoch[62] Batch [10]#011Speed: 319.16 samples/sec#011loss=3.554365\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:21 INFO 140149161203520] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2460.91890335083, \"sum\": 2460.91890335083, \"min\": 2460.91890335083}}, \"EndTime\": 1588626981.766898, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626979.305438}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:21 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=266.553768432 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:21 INFO 140149161203520] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:21 INFO 140149161203520] #quality_metric: host=algo-1, epoch=62, train loss <loss>=3.59976835684\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:21 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:22 INFO 140149161203520] Epoch[63] Batch[0] avg_epoch_loss=3.437382\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:22 INFO 140149161203520] #quality_metric: host=algo-1, epoch=63, batch=0 train loss <loss>=3.4373819828\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:23 INFO 140149161203520] Epoch[63] Batch[5] avg_epoch_loss=3.592498\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:23 INFO 140149161203520] #quality_metric: host=algo-1, epoch=63, batch=5 train loss <loss>=3.5924975872\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:23 INFO 140149161203520] Epoch[63] Batch [5]#011Speed: 316.28 samples/sec#011loss=3.592498\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:24 INFO 140149161203520] Epoch[63] Batch[10] avg_epoch_loss=3.721977\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:24 INFO 140149161203520] #quality_metric: host=algo-1, epoch=63, batch=10 train loss <loss>=3.87735228539\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:24 INFO 140149161203520] Epoch[63] Batch [10]#011Speed: 321.44 samples/sec#011loss=3.877352\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:24 INFO 140149161203520] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2461.6990089416504, \"sum\": 2461.6990089416504, \"min\": 2461.6990089416504}}, \"EndTime\": 1588626984.2292, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626981.766979}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:24 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=268.09336237 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:24 INFO 140149161203520] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:24 INFO 140149161203520] #quality_metric: host=algo-1, epoch=63, train loss <loss>=3.72197699547\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:24 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:24 INFO 140149161203520] Epoch[64] Batch[0] avg_epoch_loss=3.700585\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:24 INFO 140149161203520] #quality_metric: host=algo-1, epoch=64, batch=0 train loss <loss>=3.70058512688\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:25 INFO 140149161203520] Epoch[64] Batch[5] avg_epoch_loss=3.725562\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:25 INFO 140149161203520] #quality_metric: host=algo-1, epoch=64, batch=5 train loss <loss>=3.72556217511\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:25 INFO 140149161203520] Epoch[64] Batch [5]#011Speed: 319.02 samples/sec#011loss=3.725562\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:26 INFO 140149161203520] processed a total of 607 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2270.880937576294, \"sum\": 2270.880937576294, \"min\": 2270.880937576294}}, \"EndTime\": 1588626986.500647, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626984.229289}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:26 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=267.280199644 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:26 INFO 140149161203520] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:26 INFO 140149161203520] #quality_metric: host=algo-1, epoch=64, train loss <loss>=3.65158293247\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:26 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:26 INFO 140149161203520] Epoch[65] Batch[0] avg_epoch_loss=3.676682\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:26 INFO 140149161203520] #quality_metric: host=algo-1, epoch=65, batch=0 train loss <loss>=3.67668247223\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:27 INFO 140149161203520] Epoch[65] Batch[5] avg_epoch_loss=3.637745\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:27 INFO 140149161203520] #quality_metric: host=algo-1, epoch=65, batch=5 train loss <loss>=3.63774474462\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:27 INFO 140149161203520] Epoch[65] Batch [5]#011Speed: 318.23 samples/sec#011loss=3.637745\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:28 INFO 140149161203520] processed a total of 612 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2261.3091468811035, \"sum\": 2261.3091468811035, \"min\": 2261.3091468811035}}, \"EndTime\": 1588626988.762771, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626986.500753}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:28 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=270.623052479 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:28 INFO 140149161203520] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:28 INFO 140149161203520] #quality_metric: host=algo-1, epoch=65, train loss <loss>=3.59885687828\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:28 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:29 INFO 140149161203520] Epoch[66] Batch[0] avg_epoch_loss=3.713264\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:29 INFO 140149161203520] #quality_metric: host=algo-1, epoch=66, batch=0 train loss <loss>=3.71326351166\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:30 INFO 140149161203520] Epoch[66] Batch[5] avg_epoch_loss=3.620783\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:30 INFO 140149161203520] #quality_metric: host=algo-1, epoch=66, batch=5 train loss <loss>=3.62078344822\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:30 INFO 140149161203520] Epoch[66] Batch [5]#011Speed: 326.70 samples/sec#011loss=3.620783\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:31 INFO 140149161203520] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2240.21315574646, \"sum\": 2240.21315574646, \"min\": 2240.21315574646}}, \"EndTime\": 1588626991.00364, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626988.762868}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:31 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=278.085159218 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:31 INFO 140149161203520] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:31 INFO 140149161203520] #quality_metric: host=algo-1, epoch=66, train loss <loss>=3.59432523251\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:31 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:31 INFO 140149161203520] Epoch[67] Batch[0] avg_epoch_loss=3.606860\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:31 INFO 140149161203520] #quality_metric: host=algo-1, epoch=67, batch=0 train loss <loss>=3.60686039925\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:32 INFO 140149161203520] Epoch[67] Batch[5] avg_epoch_loss=3.603404\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:32 INFO 140149161203520] #quality_metric: host=algo-1, epoch=67, batch=5 train loss <loss>=3.6034040451\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:32 INFO 140149161203520] Epoch[67] Batch [5]#011Speed: 321.63 samples/sec#011loss=3.603404\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:33 INFO 140149161203520] Epoch[67] Batch[10] avg_epoch_loss=3.569664\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:33 INFO 140149161203520] #quality_metric: host=algo-1, epoch=67, batch=10 train loss <loss>=3.52917556763\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:33 INFO 140149161203520] Epoch[67] Batch [10]#011Speed: 316.90 samples/sec#011loss=3.529176\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:33 INFO 140149161203520] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2450.6499767303467, \"sum\": 2450.6499767303467, \"min\": 2450.6499767303467}}, \"EndTime\": 1588626993.454866, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626991.003712}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:33 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=266.039054539 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:33 INFO 140149161203520] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:33 INFO 140149161203520] #quality_metric: host=algo-1, epoch=67, train loss <loss>=3.56966382807\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:33 INFO 140149161203520] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:33 INFO 140149161203520] Saved checkpoint to \"/opt/ml/model/state_8e8c2ae1-45c7-4369-9ddd-f949ef828045-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 59.84306335449219, \"sum\": 59.84306335449219, \"min\": 59.84306335449219}}, \"EndTime\": 1588626993.515334, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626993.454945}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:33 INFO 140149161203520] Epoch[68] Batch[0] avg_epoch_loss=3.628981\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:33 INFO 140149161203520] #quality_metric: host=algo-1, epoch=68, batch=0 train loss <loss>=3.6289806366\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:34 INFO 140149161203520] Epoch[68] Batch[5] avg_epoch_loss=3.632361\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:34 INFO 140149161203520] #quality_metric: host=algo-1, epoch=68, batch=5 train loss <loss>=3.63236061732\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:34 INFO 140149161203520] Epoch[68] Batch [5]#011Speed: 309.81 samples/sec#011loss=3.632361\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:35 INFO 140149161203520] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2278.9039611816406, \"sum\": 2278.9039611816406, \"min\": 2278.9039611816406}}, \"EndTime\": 1588626995.794394, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626993.515413}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:35 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=278.186829317 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:35 INFO 140149161203520] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:35 INFO 140149161203520] #quality_metric: host=algo-1, epoch=68, train loss <loss>=3.62590179443\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:35 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:36 INFO 140149161203520] Epoch[69] Batch[0] avg_epoch_loss=3.513249\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:36 INFO 140149161203520] #quality_metric: host=algo-1, epoch=69, batch=0 train loss <loss>=3.51324892044\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:37 INFO 140149161203520] Epoch[69] Batch[5] avg_epoch_loss=3.575849\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:37 INFO 140149161203520] #quality_metric: host=algo-1, epoch=69, batch=5 train loss <loss>=3.57584861914\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:37 INFO 140149161203520] Epoch[69] Batch [5]#011Speed: 323.82 samples/sec#011loss=3.575849\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:38 INFO 140149161203520] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2235.430955886841, \"sum\": 2235.430955886841, \"min\": 2235.430955886841}}, \"EndTime\": 1588626998.030462, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626995.794491}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:38 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=279.572135528 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:38 INFO 140149161203520] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:38 INFO 140149161203520] #quality_metric: host=algo-1, epoch=69, train loss <loss>=3.602339077\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:38 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:38 INFO 140149161203520] Epoch[70] Batch[0] avg_epoch_loss=3.750513\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:38 INFO 140149161203520] #quality_metric: host=algo-1, epoch=70, batch=0 train loss <loss>=3.7505133152\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:39 INFO 140149161203520] Epoch[70] Batch[5] avg_epoch_loss=3.610966\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:39 INFO 140149161203520] #quality_metric: host=algo-1, epoch=70, batch=5 train loss <loss>=3.61096560955\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:39 INFO 140149161203520] Epoch[70] Batch [5]#011Speed: 322.33 samples/sec#011loss=3.610966\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:40 INFO 140149161203520] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2229.8381328582764, \"sum\": 2229.8381328582764, \"min\": 2229.8381328582764}}, \"EndTime\": 1588627000.260994, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588626998.030544}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:40 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=276.687942501 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:40 INFO 140149161203520] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:40 INFO 140149161203520] #quality_metric: host=algo-1, epoch=70, train loss <loss>=3.57952752113\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:40 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:40 INFO 140149161203520] Epoch[71] Batch[0] avg_epoch_loss=3.560721\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:40 INFO 140149161203520] #quality_metric: host=algo-1, epoch=71, batch=0 train loss <loss>=3.5607213974\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:41 INFO 140149161203520] Epoch[71] Batch[5] avg_epoch_loss=3.622644\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:41 INFO 140149161203520] #quality_metric: host=algo-1, epoch=71, batch=5 train loss <loss>=3.62264434497\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:41 INFO 140149161203520] Epoch[71] Batch [5]#011Speed: 317.29 samples/sec#011loss=3.622644\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:42 INFO 140149161203520] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2248.3041286468506, \"sum\": 2248.3041286468506, \"min\": 2248.3041286468506}}, \"EndTime\": 1588627002.50988, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627000.261067}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:42 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=281.083144322 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:42 INFO 140149161203520] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:42 INFO 140149161203520] #quality_metric: host=algo-1, epoch=71, train loss <loss>=3.66106648445\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:42 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:42 INFO 140149161203520] Epoch[72] Batch[0] avg_epoch_loss=3.647162\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:42 INFO 140149161203520] #quality_metric: host=algo-1, epoch=72, batch=0 train loss <loss>=3.64716219902\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:44 INFO 140149161203520] Epoch[72] Batch[5] avg_epoch_loss=3.685410\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:44 INFO 140149161203520] #quality_metric: host=algo-1, epoch=72, batch=5 train loss <loss>=3.68540974458\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:44 INFO 140149161203520] Epoch[72] Batch [5]#011Speed: 312.75 samples/sec#011loss=3.685410\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:45 INFO 140149161203520] Epoch[72] Batch[10] avg_epoch_loss=3.732283\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:45 INFO 140149161203520] #quality_metric: host=algo-1, epoch=72, batch=10 train loss <loss>=3.78853096962\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:45 INFO 140149161203520] Epoch[72] Batch [10]#011Speed: 317.09 samples/sec#011loss=3.788531\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:45 INFO 140149161203520] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2513.659954071045, \"sum\": 2513.659954071045, \"min\": 2513.659954071045}}, \"EndTime\": 1588627005.024124, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627002.509978}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:45 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=261.358779967 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:45 INFO 140149161203520] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:45 INFO 140149161203520] #quality_metric: host=algo-1, epoch=72, train loss <loss>=3.73228302869\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:45 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:45 INFO 140149161203520] Epoch[73] Batch[0] avg_epoch_loss=3.629212\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:45 INFO 140149161203520] #quality_metric: host=algo-1, epoch=73, batch=0 train loss <loss>=3.62921190262\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:46 INFO 140149161203520] Epoch[73] Batch[5] avg_epoch_loss=3.596716\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:46 INFO 140149161203520] #quality_metric: host=algo-1, epoch=73, batch=5 train loss <loss>=3.59671588739\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:46 INFO 140149161203520] Epoch[73] Batch [5]#011Speed: 322.91 samples/sec#011loss=3.596716\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:47 INFO 140149161203520] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2239.2520904541016, \"sum\": 2239.2520904541016, \"min\": 2239.2520904541016}}, \"EndTime\": 1588627007.263901, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627005.024211}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:47 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=279.094378433 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:47 INFO 140149161203520] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:47 INFO 140149161203520] #quality_metric: host=algo-1, epoch=73, train loss <loss>=3.60742468834\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:47 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:47 INFO 140149161203520] Epoch[74] Batch[0] avg_epoch_loss=3.720263\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:47 INFO 140149161203520] #quality_metric: host=algo-1, epoch=74, batch=0 train loss <loss>=3.72026324272\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:48 INFO 140149161203520] Epoch[74] Batch[5] avg_epoch_loss=3.574967\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:48 INFO 140149161203520] #quality_metric: host=algo-1, epoch=74, batch=5 train loss <loss>=3.57496738434\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:48 INFO 140149161203520] Epoch[74] Batch [5]#011Speed: 316.91 samples/sec#011loss=3.574967\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:49 INFO 140149161203520] Epoch[74] Batch[10] avg_epoch_loss=3.541093\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:49 INFO 140149161203520] #quality_metric: host=algo-1, epoch=74, batch=10 train loss <loss>=3.50044350624\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:49 INFO 140149161203520] Epoch[74] Batch [10]#011Speed: 316.87 samples/sec#011loss=3.500444\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:49 INFO 140149161203520] processed a total of 662 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2516.7291164398193, \"sum\": 2516.7291164398193, \"min\": 2516.7291164398193}}, \"EndTime\": 1588627009.781192, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627007.263994}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:49 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=263.025727199 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:49 INFO 140149161203520] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:49 INFO 140149161203520] #quality_metric: host=algo-1, epoch=74, train loss <loss>=3.54109289429\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:49 INFO 140149161203520] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:49 INFO 140149161203520] Saved checkpoint to \"/opt/ml/model/state_04d7e45a-c2d6-4122-b768-f609d57bcefa-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 60.43720245361328, \"sum\": 60.43720245361328, \"min\": 60.43720245361328}}, \"EndTime\": 1588627009.842311, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627009.781285}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:50 INFO 140149161203520] Epoch[75] Batch[0] avg_epoch_loss=3.762105\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:50 INFO 140149161203520] #quality_metric: host=algo-1, epoch=75, batch=0 train loss <loss>=3.76210546494\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:51 INFO 140149161203520] Epoch[75] Batch[5] avg_epoch_loss=3.556436\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:51 INFO 140149161203520] #quality_metric: host=algo-1, epoch=75, batch=5 train loss <loss>=3.55643649896\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:51 INFO 140149161203520] Epoch[75] Batch [5]#011Speed: 318.05 samples/sec#011loss=3.556436\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:52 INFO 140149161203520] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2264.6470069885254, \"sum\": 2264.6470069885254, \"min\": 2264.6470069885254}}, \"EndTime\": 1588627012.107119, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627009.842388}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:52 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=278.169634102 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:52 INFO 140149161203520] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:52 INFO 140149161203520] #quality_metric: host=algo-1, epoch=75, train loss <loss>=3.54487798214\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:52 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:52 INFO 140149161203520] Epoch[76] Batch[0] avg_epoch_loss=3.639057\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:52 INFO 140149161203520] #quality_metric: host=algo-1, epoch=76, batch=0 train loss <loss>=3.63905668259\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:53 INFO 140149161203520] Epoch[76] Batch[5] avg_epoch_loss=3.619609\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:53 INFO 140149161203520] #quality_metric: host=algo-1, epoch=76, batch=5 train loss <loss>=3.61960911751\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:53 INFO 140149161203520] Epoch[76] Batch [5]#011Speed: 316.77 samples/sec#011loss=3.619609\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:54 INFO 140149161203520] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2263.6990547180176, \"sum\": 2263.6990547180176, \"min\": 2263.6990547180176}}, \"EndTime\": 1588627014.371471, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627012.107221}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:54 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=280.055250484 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:54 INFO 140149161203520] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:54 INFO 140149161203520] #quality_metric: host=algo-1, epoch=76, train loss <loss>=3.57708921432\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:54 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:54 INFO 140149161203520] Epoch[77] Batch[0] avg_epoch_loss=3.453230\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:54 INFO 140149161203520] #quality_metric: host=algo-1, epoch=77, batch=0 train loss <loss>=3.45322990417\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:55 INFO 140149161203520] Epoch[77] Batch[5] avg_epoch_loss=3.542647\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:55 INFO 140149161203520] #quality_metric: host=algo-1, epoch=77, batch=5 train loss <loss>=3.54264748096\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:55 INFO 140149161203520] Epoch[77] Batch [5]#011Speed: 315.12 samples/sec#011loss=3.542647\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:56 INFO 140149161203520] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2308.6330890655518, \"sum\": 2308.6330890655518, \"min\": 2308.6330890655518}}, \"EndTime\": 1588627016.68069, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627014.371568}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:56 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=274.171827097 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:56 INFO 140149161203520] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:56 INFO 140149161203520] #quality_metric: host=algo-1, epoch=77, train loss <loss>=3.53898165226\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:56 INFO 140149161203520] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:56 INFO 140149161203520] Saved checkpoint to \"/opt/ml/model/state_b7210918-2089-49dc-bc8f-af79d7046079-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 59.3569278717041, \"sum\": 59.3569278717041, \"min\": 59.3569278717041}}, \"EndTime\": 1588627016.740701, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627016.680784}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:57 INFO 140149161203520] Epoch[78] Batch[0] avg_epoch_loss=3.617787\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:57 INFO 140149161203520] #quality_metric: host=algo-1, epoch=78, batch=0 train loss <loss>=3.61778688431\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:58 INFO 140149161203520] Epoch[78] Batch[5] avg_epoch_loss=3.639280\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:58 INFO 140149161203520] #quality_metric: host=algo-1, epoch=78, batch=5 train loss <loss>=3.63927968343\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:58 INFO 140149161203520] Epoch[78] Batch [5]#011Speed: 320.45 samples/sec#011loss=3.639280\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:59 INFO 140149161203520] Epoch[78] Batch[10] avg_epoch_loss=3.531311\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:59 INFO 140149161203520] #quality_metric: host=algo-1, epoch=78, batch=10 train loss <loss>=3.40174818039\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:59 INFO 140149161203520] Epoch[78] Batch [10]#011Speed: 316.14 samples/sec#011loss=3.401748\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:59 INFO 140149161203520] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2469.1309928894043, \"sum\": 2469.1309928894043, \"min\": 2469.1309928894043}}, \"EndTime\": 1588627019.210027, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627016.740774}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:59 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=259.991979163 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:59 INFO 140149161203520] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:59 INFO 140149161203520] #quality_metric: host=algo-1, epoch=78, train loss <loss>=3.53131081841\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:59 INFO 140149161203520] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:59 INFO 140149161203520] Saved checkpoint to \"/opt/ml/model/state_190e8c6b-7dc0-4010-ad6d-df6251e75eec-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 65.97614288330078, \"sum\": 65.97614288330078, \"min\": 65.97614288330078}}, \"EndTime\": 1588627019.2766, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627019.210118}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:59 INFO 140149161203520] Epoch[79] Batch[0] avg_epoch_loss=3.596972\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:16:59 INFO 140149161203520] #quality_metric: host=algo-1, epoch=79, batch=0 train loss <loss>=3.59697175026\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:00 INFO 140149161203520] Epoch[79] Batch[5] avg_epoch_loss=3.568032\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:00 INFO 140149161203520] #quality_metric: host=algo-1, epoch=79, batch=5 train loss <loss>=3.56803174814\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:00 INFO 140149161203520] Epoch[79] Batch [5]#011Speed: 315.54 samples/sec#011loss=3.568032\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:01 INFO 140149161203520] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2290.621042251587, \"sum\": 2290.621042251587, \"min\": 2290.621042251587}}, \"EndTime\": 1588627021.567355, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627019.276665}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:01 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=276.763692846 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:01 INFO 140149161203520] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:01 INFO 140149161203520] #quality_metric: host=algo-1, epoch=79, train loss <loss>=3.50806105137\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:01 INFO 140149161203520] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:01 INFO 140149161203520] Saved checkpoint to \"/opt/ml/model/state_364eb125-83c3-4357-8ed1-a5f1bc98caf0-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 74.5840072631836, \"sum\": 74.5840072631836, \"min\": 74.5840072631836}}, \"EndTime\": 1588627021.642588, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627021.567455}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:02 INFO 140149161203520] Epoch[80] Batch[0] avg_epoch_loss=3.457848\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:02 INFO 140149161203520] #quality_metric: host=algo-1, epoch=80, batch=0 train loss <loss>=3.45784759521\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:03 INFO 140149161203520] Epoch[80] Batch[5] avg_epoch_loss=3.590639\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:03 INFO 140149161203520] #quality_metric: host=algo-1, epoch=80, batch=5 train loss <loss>=3.59063887596\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:03 INFO 140149161203520] Epoch[80] Batch [5]#011Speed: 319.01 samples/sec#011loss=3.590639\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:04 INFO 140149161203520] Epoch[80] Batch[10] avg_epoch_loss=3.659387\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:04 INFO 140149161203520] #quality_metric: host=algo-1, epoch=80, batch=10 train loss <loss>=3.74188365936\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:04 INFO 140149161203520] Epoch[80] Batch [10]#011Speed: 312.58 samples/sec#011loss=3.741884\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:04 INFO 140149161203520] processed a total of 663 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2496.2379932403564, \"sum\": 2496.2379932403564, \"min\": 2496.2379932403564}}, \"EndTime\": 1588627024.138983, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627021.642667}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:04 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=265.582020341 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:04 INFO 140149161203520] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:04 INFO 140149161203520] #quality_metric: host=algo-1, epoch=80, train loss <loss>=3.65938650478\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:04 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:04 INFO 140149161203520] Epoch[81] Batch[0] avg_epoch_loss=3.526186\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:04 INFO 140149161203520] #quality_metric: host=algo-1, epoch=81, batch=0 train loss <loss>=3.52618646622\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:05 INFO 140149161203520] Epoch[81] Batch[5] avg_epoch_loss=3.520754\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:05 INFO 140149161203520] #quality_metric: host=algo-1, epoch=81, batch=5 train loss <loss>=3.52075362206\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:05 INFO 140149161203520] Epoch[81] Batch [5]#011Speed: 307.93 samples/sec#011loss=3.520754\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:06 INFO 140149161203520] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2314.1210079193115, \"sum\": 2314.1210079193115, \"min\": 2314.1210079193115}}, \"EndTime\": 1588627026.453665, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627024.139107}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:06 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=276.54819076 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:06 INFO 140149161203520] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:06 INFO 140149161203520] #quality_metric: host=algo-1, epoch=81, train loss <loss>=3.59460780621\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:06 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:06 INFO 140149161203520] Epoch[82] Batch[0] avg_epoch_loss=3.680879\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:06 INFO 140149161203520] #quality_metric: host=algo-1, epoch=82, batch=0 train loss <loss>=3.68087863922\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:07 INFO 140149161203520] Epoch[82] Batch[5] avg_epoch_loss=3.568310\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:07 INFO 140149161203520] #quality_metric: host=algo-1, epoch=82, batch=5 train loss <loss>=3.56830982367\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:07 INFO 140149161203520] Epoch[82] Batch [5]#011Speed: 321.06 samples/sec#011loss=3.568310\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:08 INFO 140149161203520] Epoch[82] Batch[10] avg_epoch_loss=3.529553\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:08 INFO 140149161203520] #quality_metric: host=algo-1, epoch=82, batch=10 train loss <loss>=3.4830447197\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:08 INFO 140149161203520] Epoch[82] Batch [10]#011Speed: 315.26 samples/sec#011loss=3.483045\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:08 INFO 140149161203520] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2505.642890930176, \"sum\": 2505.642890930176, \"min\": 2505.642890930176}}, \"EndTime\": 1588627028.959905, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627026.453754}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:08 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=262.994792143 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:08 INFO 140149161203520] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:08 INFO 140149161203520] #quality_metric: host=algo-1, epoch=82, train loss <loss>=3.52955295823\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:08 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:09 INFO 140149161203520] Epoch[83] Batch[0] avg_epoch_loss=3.579091\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:09 INFO 140149161203520] #quality_metric: host=algo-1, epoch=83, batch=0 train loss <loss>=3.57909083366\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:10 INFO 140149161203520] Epoch[83] Batch[5] avg_epoch_loss=3.543619\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:10 INFO 140149161203520] #quality_metric: host=algo-1, epoch=83, batch=5 train loss <loss>=3.54361891747\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:10 INFO 140149161203520] Epoch[83] Batch [5]#011Speed: 321.40 samples/sec#011loss=3.543619\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:11 INFO 140149161203520] Epoch[83] Batch[10] avg_epoch_loss=3.638729\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:11 INFO 140149161203520] #quality_metric: host=algo-1, epoch=83, batch=10 train loss <loss>=3.7528617382\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:11 INFO 140149161203520] Epoch[83] Batch [10]#011Speed: 321.70 samples/sec#011loss=3.752862\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:11 INFO 140149161203520] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2442.4710273742676, \"sum\": 2442.4710273742676, \"min\": 2442.4710273742676}}, \"EndTime\": 1588627031.402921, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627028.959974}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:11 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=266.516784067 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:11 INFO 140149161203520] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:11 INFO 140149161203520] #quality_metric: host=algo-1, epoch=83, train loss <loss>=3.63872929053\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:11 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:11 INFO 140149161203520] Epoch[84] Batch[0] avg_epoch_loss=3.443824\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:11 INFO 140149161203520] #quality_metric: host=algo-1, epoch=84, batch=0 train loss <loss>=3.44382429123\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:12 INFO 140149161203520] Epoch[84] Batch[5] avg_epoch_loss=3.520599\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:12 INFO 140149161203520] #quality_metric: host=algo-1, epoch=84, batch=5 train loss <loss>=3.52059908708\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:12 INFO 140149161203520] Epoch[84] Batch [5]#011Speed: 324.21 samples/sec#011loss=3.520599\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:13 INFO 140149161203520] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2252.8440952301025, \"sum\": 2252.8440952301025, \"min\": 2252.8440952301025}}, \"EndTime\": 1588627033.656344, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627031.403031}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:13 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=283.623995907 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:13 INFO 140149161203520] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:13 INFO 140149161203520] #quality_metric: host=algo-1, epoch=84, train loss <loss>=3.54320764542\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:13 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:14 INFO 140149161203520] Epoch[85] Batch[0] avg_epoch_loss=3.587258\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:14 INFO 140149161203520] #quality_metric: host=algo-1, epoch=85, batch=0 train loss <loss>=3.58725786209\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:15 INFO 140149161203520] Epoch[85] Batch[5] avg_epoch_loss=3.559055\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:15 INFO 140149161203520] #quality_metric: host=algo-1, epoch=85, batch=5 train loss <loss>=3.55905512969\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:15 INFO 140149161203520] Epoch[85] Batch [5]#011Speed: 322.31 samples/sec#011loss=3.559055\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:16 INFO 140149161203520] Epoch[85] Batch[10] avg_epoch_loss=3.527535\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:16 INFO 140149161203520] #quality_metric: host=algo-1, epoch=85, batch=10 train loss <loss>=3.48971061707\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:16 INFO 140149161203520] Epoch[85] Batch [10]#011Speed: 320.18 samples/sec#011loss=3.489711\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:16 INFO 140149161203520] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2464.1940593719482, \"sum\": 2464.1940593719482, \"min\": 2464.1940593719482}}, \"EndTime\": 1588627036.12114, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627033.656442}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:16 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=260.517918773 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:16 INFO 140149161203520] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:16 INFO 140149161203520] #quality_metric: host=algo-1, epoch=85, train loss <loss>=3.52753489668\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:16 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:16 INFO 140149161203520] Epoch[86] Batch[0] avg_epoch_loss=3.663485\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:16 INFO 140149161203520] #quality_metric: host=algo-1, epoch=86, batch=0 train loss <loss>=3.66348481178\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:17 INFO 140149161203520] Epoch[86] Batch[5] avg_epoch_loss=3.657103\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:17 INFO 140149161203520] #quality_metric: host=algo-1, epoch=86, batch=5 train loss <loss>=3.65710326036\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:17 INFO 140149161203520] Epoch[86] Batch [5]#011Speed: 324.41 samples/sec#011loss=3.657103\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:18 INFO 140149161203520] Epoch[86] Batch[10] avg_epoch_loss=3.563112\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:18 INFO 140149161203520] #quality_metric: host=algo-1, epoch=86, batch=10 train loss <loss>=3.45032248497\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:18 INFO 140149161203520] Epoch[86] Batch [10]#011Speed: 319.71 samples/sec#011loss=3.450322\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:18 INFO 140149161203520] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2453.071117401123, \"sum\": 2453.071117401123, \"min\": 2453.071117401123}}, \"EndTime\": 1588627038.574765, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627036.121228}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:18 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=268.221322445 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:18 INFO 140149161203520] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:18 INFO 140149161203520] #quality_metric: host=algo-1, epoch=86, train loss <loss>=3.56311199882\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:18 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:19 INFO 140149161203520] Epoch[87] Batch[0] avg_epoch_loss=3.709108\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:19 INFO 140149161203520] #quality_metric: host=algo-1, epoch=87, batch=0 train loss <loss>=3.70910763741\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:20 INFO 140149161203520] Epoch[87] Batch[5] avg_epoch_loss=3.581569\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:20 INFO 140149161203520] #quality_metric: host=algo-1, epoch=87, batch=5 train loss <loss>=3.58156911532\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:20 INFO 140149161203520] Epoch[87] Batch [5]#011Speed: 324.91 samples/sec#011loss=3.581569\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:21 INFO 140149161203520] Epoch[87] Batch[10] avg_epoch_loss=3.566017\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:21 INFO 140149161203520] #quality_metric: host=algo-1, epoch=87, batch=10 train loss <loss>=3.54735426903\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:21 INFO 140149161203520] Epoch[87] Batch [10]#011Speed: 319.61 samples/sec#011loss=3.547354\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:21 INFO 140149161203520] processed a total of 686 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2476.576089859009, \"sum\": 2476.576089859009, \"min\": 2476.576089859009}}, \"EndTime\": 1588627041.0519, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627038.574853}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:21 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=276.980899791 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:21 INFO 140149161203520] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:21 INFO 140149161203520] #quality_metric: host=algo-1, epoch=87, train loss <loss>=3.56601691246\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:21 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:21 INFO 140149161203520] Epoch[88] Batch[0] avg_epoch_loss=3.886612\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:21 INFO 140149161203520] #quality_metric: host=algo-1, epoch=88, batch=0 train loss <loss>=3.88661170006\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:22 INFO 140149161203520] Epoch[88] Batch[5] avg_epoch_loss=3.625232\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:22 INFO 140149161203520] #quality_metric: host=algo-1, epoch=88, batch=5 train loss <loss>=3.62523225943\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:22 INFO 140149161203520] Epoch[88] Batch [5]#011Speed: 323.40 samples/sec#011loss=3.625232\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:23 INFO 140149161203520] processed a total of 618 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2254.246950149536, \"sum\": 2254.246950149536, \"min\": 2254.246950149536}}, \"EndTime\": 1588627043.306705, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627041.051989}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:23 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=274.132906579 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:23 INFO 140149161203520] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:23 INFO 140149161203520] #quality_metric: host=algo-1, epoch=88, train loss <loss>=3.64270040989\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:23 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:23 INFO 140149161203520] Epoch[89] Batch[0] avg_epoch_loss=3.647164\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:23 INFO 140149161203520] #quality_metric: host=algo-1, epoch=89, batch=0 train loss <loss>=3.64716434479\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:24 INFO 140149161203520] Epoch[89] Batch[5] avg_epoch_loss=3.494888\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:24 INFO 140149161203520] #quality_metric: host=algo-1, epoch=89, batch=5 train loss <loss>=3.49488763014\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:24 INFO 140149161203520] Epoch[89] Batch [5]#011Speed: 316.96 samples/sec#011loss=3.494888\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:25 INFO 140149161203520] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2251.0478496551514, \"sum\": 2251.0478496551514, \"min\": 2251.0478496551514}}, \"EndTime\": 1588627045.558428, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627043.306801}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:25 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=279.852351627 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:25 INFO 140149161203520] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:25 INFO 140149161203520] #quality_metric: host=algo-1, epoch=89, train loss <loss>=3.54178733826\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:25 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:26 INFO 140149161203520] Epoch[90] Batch[0] avg_epoch_loss=3.569957\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:26 INFO 140149161203520] #quality_metric: host=algo-1, epoch=90, batch=0 train loss <loss>=3.56995749474\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:27 INFO 140149161203520] Epoch[90] Batch[5] avg_epoch_loss=3.641404\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:27 INFO 140149161203520] #quality_metric: host=algo-1, epoch=90, batch=5 train loss <loss>=3.6414043506\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:27 INFO 140149161203520] Epoch[90] Batch [5]#011Speed: 317.49 samples/sec#011loss=3.641404\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:27 INFO 140149161203520] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2276.29017829895, \"sum\": 2276.29017829895, \"min\": 2276.29017829895}}, \"EndTime\": 1588627047.835297, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627045.558524}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:27 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=278.945905788 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:27 INFO 140149161203520] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:27 INFO 140149161203520] #quality_metric: host=algo-1, epoch=90, train loss <loss>=3.58597955704\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:27 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:28 INFO 140149161203520] Epoch[91] Batch[0] avg_epoch_loss=3.335153\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:28 INFO 140149161203520] #quality_metric: host=algo-1, epoch=91, batch=0 train loss <loss>=3.33515310287\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:29 INFO 140149161203520] Epoch[91] Batch[5] avg_epoch_loss=3.471183\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:29 INFO 140149161203520] #quality_metric: host=algo-1, epoch=91, batch=5 train loss <loss>=3.47118266424\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:29 INFO 140149161203520] Epoch[91] Batch [5]#011Speed: 322.88 samples/sec#011loss=3.471183\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:30 INFO 140149161203520] Epoch[91] Batch[10] avg_epoch_loss=3.578125\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:30 INFO 140149161203520] #quality_metric: host=algo-1, epoch=91, batch=10 train loss <loss>=3.70645480156\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:30 INFO 140149161203520] Epoch[91] Batch [10]#011Speed: 320.76 samples/sec#011loss=3.706455\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:30 INFO 140149161203520] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2464.3020629882812, \"sum\": 2464.3020629882812, \"min\": 2464.3020629882812}}, \"EndTime\": 1588627050.300183, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627047.835392}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:30 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=260.913107923 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:30 INFO 140149161203520] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:30 INFO 140149161203520] #quality_metric: host=algo-1, epoch=91, train loss <loss>=3.57812454484\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:30 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:30 INFO 140149161203520] Epoch[92] Batch[0] avg_epoch_loss=3.501791\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:30 INFO 140149161203520] #quality_metric: host=algo-1, epoch=92, batch=0 train loss <loss>=3.50179100037\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:31 INFO 140149161203520] Epoch[92] Batch[5] avg_epoch_loss=3.525400\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:31 INFO 140149161203520] #quality_metric: host=algo-1, epoch=92, batch=5 train loss <loss>=3.52540020148\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:31 INFO 140149161203520] Epoch[92] Batch [5]#011Speed: 320.91 samples/sec#011loss=3.525400\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:32 INFO 140149161203520] processed a total of 610 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2238.633871078491, \"sum\": 2238.633871078491, \"min\": 2238.633871078491}}, \"EndTime\": 1588627052.539356, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627050.300262}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:32 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=272.470782232 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:32 INFO 140149161203520] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:32 INFO 140149161203520] #quality_metric: host=algo-1, epoch=92, train loss <loss>=3.59984099865\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:32 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:32 INFO 140149161203520] Epoch[93] Batch[0] avg_epoch_loss=3.453299\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:32 INFO 140149161203520] #quality_metric: host=algo-1, epoch=93, batch=0 train loss <loss>=3.45329904556\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:34 INFO 140149161203520] Epoch[93] Batch[5] avg_epoch_loss=3.576940\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:34 INFO 140149161203520] #quality_metric: host=algo-1, epoch=93, batch=5 train loss <loss>=3.57693958282\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:34 INFO 140149161203520] Epoch[93] Batch [5]#011Speed: 301.93 samples/sec#011loss=3.576940\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:34 INFO 140149161203520] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2309.2689514160156, \"sum\": 2309.2689514160156, \"min\": 2309.2689514160156}}, \"EndTime\": 1588627054.849222, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627052.539451}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:34 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=273.665219122 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:34 INFO 140149161203520] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:34 INFO 140149161203520] #quality_metric: host=algo-1, epoch=93, train loss <loss>=3.560632658\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:34 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:35 INFO 140149161203520] Epoch[94] Batch[0] avg_epoch_loss=3.749697\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:35 INFO 140149161203520] #quality_metric: host=algo-1, epoch=94, batch=0 train loss <loss>=3.74969696999\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:36 INFO 140149161203520] Epoch[94] Batch[5] avg_epoch_loss=3.584229\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:36 INFO 140149161203520] #quality_metric: host=algo-1, epoch=94, batch=5 train loss <loss>=3.58422915141\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:36 INFO 140149161203520] Epoch[94] Batch [5]#011Speed: 316.66 samples/sec#011loss=3.584229\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:37 INFO 140149161203520] processed a total of 602 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2268.941879272461, \"sum\": 2268.941879272461, \"min\": 2268.941879272461}}, \"EndTime\": 1588627057.118755, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627054.849301}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:37 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=265.305632235 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:37 INFO 140149161203520] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:37 INFO 140149161203520] #quality_metric: host=algo-1, epoch=94, train loss <loss>=3.63916172981\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:37 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:37 INFO 140149161203520] Epoch[95] Batch[0] avg_epoch_loss=3.322293\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:37 INFO 140149161203520] #quality_metric: host=algo-1, epoch=95, batch=0 train loss <loss>=3.32229328156\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:38 INFO 140149161203520] Epoch[95] Batch[5] avg_epoch_loss=3.421312\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:38 INFO 140149161203520] #quality_metric: host=algo-1, epoch=95, batch=5 train loss <loss>=3.42131233215\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:38 INFO 140149161203520] Epoch[95] Batch [5]#011Speed: 315.33 samples/sec#011loss=3.421312\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:39 INFO 140149161203520] Epoch[95] Batch[10] avg_epoch_loss=3.399887\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:39 INFO 140149161203520] #quality_metric: host=algo-1, epoch=95, batch=10 train loss <loss>=3.37417564392\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:39 INFO 140149161203520] Epoch[95] Batch [10]#011Speed: 323.27 samples/sec#011loss=3.374176\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:39 INFO 140149161203520] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2455.834150314331, \"sum\": 2455.834150314331, \"min\": 2455.834150314331}}, \"EndTime\": 1588627059.575232, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627057.118851}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:39 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=265.069254001 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:39 INFO 140149161203520] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:39 INFO 140149161203520] #quality_metric: host=algo-1, epoch=95, train loss <loss>=3.39988656477\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:39 INFO 140149161203520] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:39 INFO 140149161203520] Saved checkpoint to \"/opt/ml/model/state_d5899110-d242-462c-a54f-9fa51e04bed6-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 85.12020111083984, \"sum\": 85.12020111083984, \"min\": 85.12020111083984}}, \"EndTime\": 1588627059.660948, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627059.575321}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:40 INFO 140149161203520] Epoch[96] Batch[0] avg_epoch_loss=3.534515\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:40 INFO 140149161203520] #quality_metric: host=algo-1, epoch=96, batch=0 train loss <loss>=3.53451514244\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:41 INFO 140149161203520] Epoch[96] Batch[5] avg_epoch_loss=3.621499\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:41 INFO 140149161203520] #quality_metric: host=algo-1, epoch=96, batch=5 train loss <loss>=3.62149910132\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:41 INFO 140149161203520] Epoch[96] Batch [5]#011Speed: 320.59 samples/sec#011loss=3.621499\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:41 INFO 140149161203520] processed a total of 631 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2253.9100646972656, \"sum\": 2253.9100646972656, \"min\": 2253.9100646972656}}, \"EndTime\": 1588627061.91506, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627059.661035}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:41 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=279.935219668 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:41 INFO 140149161203520] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:41 INFO 140149161203520] #quality_metric: host=algo-1, epoch=96, train loss <loss>=3.56295244694\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:41 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:42 INFO 140149161203520] Epoch[97] Batch[0] avg_epoch_loss=3.598913\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:42 INFO 140149161203520] #quality_metric: host=algo-1, epoch=97, batch=0 train loss <loss>=3.59891343117\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:43 INFO 140149161203520] Epoch[97] Batch[5] avg_epoch_loss=3.609430\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:43 INFO 140149161203520] #quality_metric: host=algo-1, epoch=97, batch=5 train loss <loss>=3.60943039258\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:43 INFO 140149161203520] Epoch[97] Batch [5]#011Speed: 316.97 samples/sec#011loss=3.609430\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:44 INFO 140149161203520] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2310.4021549224854, \"sum\": 2310.4021549224854, \"min\": 2310.4021549224854}}, \"EndTime\": 1588627064.226064, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627061.915159}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:44 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=275.693132683 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:44 INFO 140149161203520] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:44 INFO 140149161203520] #quality_metric: host=algo-1, epoch=97, train loss <loss>=3.58459384441\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:44 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:44 INFO 140149161203520] Epoch[98] Batch[0] avg_epoch_loss=3.574360\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:44 INFO 140149161203520] #quality_metric: host=algo-1, epoch=98, batch=0 train loss <loss>=3.57435965538\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:45 INFO 140149161203520] Epoch[98] Batch[5] avg_epoch_loss=3.499801\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:45 INFO 140149161203520] #quality_metric: host=algo-1, epoch=98, batch=5 train loss <loss>=3.4998006026\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:45 INFO 140149161203520] Epoch[98] Batch [5]#011Speed: 325.71 samples/sec#011loss=3.499801\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:46 INFO 140149161203520] processed a total of 621 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2242.202043533325, \"sum\": 2242.202043533325, \"min\": 2242.202043533325}}, \"EndTime\": 1588627066.468847, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627064.22616}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:46 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=276.945181359 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:46 INFO 140149161203520] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:46 INFO 140149161203520] #quality_metric: host=algo-1, epoch=98, train loss <loss>=3.47330648899\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:46 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:46 INFO 140149161203520] Epoch[99] Batch[0] avg_epoch_loss=3.582060\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:46 INFO 140149161203520] #quality_metric: host=algo-1, epoch=99, batch=0 train loss <loss>=3.58205962181\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:47 INFO 140149161203520] Epoch[99] Batch[5] avg_epoch_loss=3.538967\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:47 INFO 140149161203520] #quality_metric: host=algo-1, epoch=99, batch=5 train loss <loss>=3.53896725178\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:47 INFO 140149161203520] Epoch[99] Batch [5]#011Speed: 320.98 samples/sec#011loss=3.538967\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:48 INFO 140149161203520] processed a total of 592 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2238.1742000579834, \"sum\": 2238.1742000579834, \"min\": 2238.1742000579834}}, \"EndTime\": 1588627068.70762, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627066.468928}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:48 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=264.484543007 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:48 INFO 140149161203520] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:48 INFO 140149161203520] #quality_metric: host=algo-1, epoch=99, train loss <loss>=3.4723626852\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:48 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:49 INFO 140149161203520] Epoch[100] Batch[0] avg_epoch_loss=3.563473\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:49 INFO 140149161203520] #quality_metric: host=algo-1, epoch=100, batch=0 train loss <loss>=3.56347298622\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:50 INFO 140149161203520] Epoch[100] Batch[5] avg_epoch_loss=3.560377\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:50 INFO 140149161203520] #quality_metric: host=algo-1, epoch=100, batch=5 train loss <loss>=3.56037692229\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:50 INFO 140149161203520] Epoch[100] Batch [5]#011Speed: 323.79 samples/sec#011loss=3.560377\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:51 INFO 140149161203520] Epoch[100] Batch[10] avg_epoch_loss=3.600356\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:51 INFO 140149161203520] #quality_metric: host=algo-1, epoch=100, batch=10 train loss <loss>=3.64833168983\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:51 INFO 140149161203520] Epoch[100] Batch [10]#011Speed: 323.43 samples/sec#011loss=3.648332\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:51 INFO 140149161203520] processed a total of 669 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2485.093116760254, \"sum\": 2485.093116760254, \"min\": 2485.093116760254}}, \"EndTime\": 1588627071.193291, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627068.707718}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:51 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=269.191232314 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:51 INFO 140149161203520] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:51 INFO 140149161203520] #quality_metric: host=algo-1, epoch=100, train loss <loss>=3.60035636208\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:51 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:51 INFO 140149161203520] Epoch[101] Batch[0] avg_epoch_loss=3.566499\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:51 INFO 140149161203520] #quality_metric: host=algo-1, epoch=101, batch=0 train loss <loss>=3.56649899483\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:52 INFO 140149161203520] Epoch[101] Batch[5] avg_epoch_loss=3.547086\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:52 INFO 140149161203520] #quality_metric: host=algo-1, epoch=101, batch=5 train loss <loss>=3.54708604018\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:52 INFO 140149161203520] Epoch[101] Batch [5]#011Speed: 325.34 samples/sec#011loss=3.547086\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:53 INFO 140149161203520] processed a total of 621 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2265.4640674591064, \"sum\": 2265.4640674591064, \"min\": 2265.4640674591064}}, \"EndTime\": 1588627073.459282, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627071.193381}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:53 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=274.099320777 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:53 INFO 140149161203520] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:53 INFO 140149161203520] #quality_metric: host=algo-1, epoch=101, train loss <loss>=3.53599867821\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:53 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:53 INFO 140149161203520] Epoch[102] Batch[0] avg_epoch_loss=3.453310\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:53 INFO 140149161203520] #quality_metric: host=algo-1, epoch=102, batch=0 train loss <loss>=3.45331001282\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:54 INFO 140149161203520] Epoch[102] Batch[5] avg_epoch_loss=3.482664\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:54 INFO 140149161203520] #quality_metric: host=algo-1, epoch=102, batch=5 train loss <loss>=3.48266363144\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:54 INFO 140149161203520] Epoch[102] Batch [5]#011Speed: 326.42 samples/sec#011loss=3.482664\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:55 INFO 140149161203520] processed a total of 631 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2234.4729900360107, \"sum\": 2234.4729900360107, \"min\": 2234.4729900360107}}, \"EndTime\": 1588627075.694346, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627073.459379}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:55 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=282.375644349 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:55 INFO 140149161203520] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:55 INFO 140149161203520] #quality_metric: host=algo-1, epoch=102, train loss <loss>=3.51188688278\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:55 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:56 INFO 140149161203520] Epoch[103] Batch[0] avg_epoch_loss=3.571669\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:56 INFO 140149161203520] #quality_metric: host=algo-1, epoch=103, batch=0 train loss <loss>=3.57166862488\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:57 INFO 140149161203520] Epoch[103] Batch[5] avg_epoch_loss=3.588093\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:57 INFO 140149161203520] #quality_metric: host=algo-1, epoch=103, batch=5 train loss <loss>=3.58809336027\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:57 INFO 140149161203520] Epoch[103] Batch [5]#011Speed: 326.91 samples/sec#011loss=3.588093\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:58 INFO 140149161203520] Epoch[103] Batch[10] avg_epoch_loss=3.647849\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:58 INFO 140149161203520] #quality_metric: host=algo-1, epoch=103, batch=10 train loss <loss>=3.71955537796\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:58 INFO 140149161203520] Epoch[103] Batch [10]#011Speed: 320.90 samples/sec#011loss=3.719555\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:58 INFO 140149161203520] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2463.901996612549, \"sum\": 2463.901996612549, \"min\": 2463.901996612549}}, \"EndTime\": 1588627078.158829, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627075.694443}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:58 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=262.17212973 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:58 INFO 140149161203520] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:58 INFO 140149161203520] #quality_metric: host=algo-1, epoch=103, train loss <loss>=3.64784882285\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:58 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:58 INFO 140149161203520] Epoch[104] Batch[0] avg_epoch_loss=3.634344\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:58 INFO 140149161203520] #quality_metric: host=algo-1, epoch=104, batch=0 train loss <loss>=3.63434362411\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:59 INFO 140149161203520] Epoch[104] Batch[5] avg_epoch_loss=3.594932\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:59 INFO 140149161203520] #quality_metric: host=algo-1, epoch=104, batch=5 train loss <loss>=3.59493168195\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:17:59 INFO 140149161203520] Epoch[104] Batch [5]#011Speed: 326.44 samples/sec#011loss=3.594932\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:00 INFO 140149161203520] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2246.1440563201904, \"sum\": 2246.1440563201904, \"min\": 2246.1440563201904}}, \"EndTime\": 1588627080.405541, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627078.158917}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:00 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=279.573819424 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:00 INFO 140149161203520] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:00 INFO 140149161203520] #quality_metric: host=algo-1, epoch=104, train loss <loss>=3.58729374409\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:00 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:00 INFO 140149161203520] Epoch[105] Batch[0] avg_epoch_loss=3.369328\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:00 INFO 140149161203520] #quality_metric: host=algo-1, epoch=105, batch=0 train loss <loss>=3.369328022\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:01 INFO 140149161203520] Epoch[105] Batch[5] avg_epoch_loss=3.494994\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:01 INFO 140149161203520] #quality_metric: host=algo-1, epoch=105, batch=5 train loss <loss>=3.49499352773\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:01 INFO 140149161203520] Epoch[105] Batch [5]#011Speed: 319.91 samples/sec#011loss=3.494994\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:02 INFO 140149161203520] Epoch[105] Batch[10] avg_epoch_loss=3.590447\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:02 INFO 140149161203520] #quality_metric: host=algo-1, epoch=105, batch=10 train loss <loss>=3.70499038696\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:02 INFO 140149161203520] Epoch[105] Batch [10]#011Speed: 320.97 samples/sec#011loss=3.704990\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:02 INFO 140149161203520] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2477.276086807251, \"sum\": 2477.276086807251, \"min\": 2477.276086807251}}, \"EndTime\": 1588627082.883408, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627080.405631}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:02 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=260.352637622 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:02 INFO 140149161203520] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:02 INFO 140149161203520] #quality_metric: host=algo-1, epoch=105, train loss <loss>=3.59044664556\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:02 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:03 INFO 140149161203520] Epoch[106] Batch[0] avg_epoch_loss=3.618420\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:03 INFO 140149161203520] #quality_metric: host=algo-1, epoch=106, batch=0 train loss <loss>=3.61842012405\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:04 INFO 140149161203520] Epoch[106] Batch[5] avg_epoch_loss=3.572928\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:04 INFO 140149161203520] #quality_metric: host=algo-1, epoch=106, batch=5 train loss <loss>=3.57292834918\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:04 INFO 140149161203520] Epoch[106] Batch [5]#011Speed: 322.78 samples/sec#011loss=3.572928\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:05 INFO 140149161203520] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2234.5330715179443, \"sum\": 2234.5330715179443, \"min\": 2234.5330715179443}}, \"EndTime\": 1588627085.118515, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627082.8835}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:05 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=279.686735152 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:05 INFO 140149161203520] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:05 INFO 140149161203520] #quality_metric: host=algo-1, epoch=106, train loss <loss>=3.54036204815\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:05 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:05 INFO 140149161203520] Epoch[107] Batch[0] avg_epoch_loss=3.651612\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:05 INFO 140149161203520] #quality_metric: host=algo-1, epoch=107, batch=0 train loss <loss>=3.65161156654\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:06 INFO 140149161203520] Epoch[107] Batch[5] avg_epoch_loss=3.601395\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:06 INFO 140149161203520] #quality_metric: host=algo-1, epoch=107, batch=5 train loss <loss>=3.60139497121\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:06 INFO 140149161203520] Epoch[107] Batch [5]#011Speed: 319.56 samples/sec#011loss=3.601395\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:07 INFO 140149161203520] processed a total of 610 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2265.566110610962, \"sum\": 2265.566110610962, \"min\": 2265.566110610962}}, \"EndTime\": 1588627087.384667, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627085.118587}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:07 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=269.234607873 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:07 INFO 140149161203520] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:07 INFO 140149161203520] #quality_metric: host=algo-1, epoch=107, train loss <loss>=3.59366793633\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:07 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:07 INFO 140149161203520] Epoch[108] Batch[0] avg_epoch_loss=3.366512\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:07 INFO 140149161203520] #quality_metric: host=algo-1, epoch=108, batch=0 train loss <loss>=3.36651206017\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:08 INFO 140149161203520] Epoch[108] Batch[5] avg_epoch_loss=3.407836\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:08 INFO 140149161203520] #quality_metric: host=algo-1, epoch=108, batch=5 train loss <loss>=3.40783596039\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:08 INFO 140149161203520] Epoch[108] Batch [5]#011Speed: 319.63 samples/sec#011loss=3.407836\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:09 INFO 140149161203520] processed a total of 604 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2226.9320487976074, \"sum\": 2226.9320487976074, \"min\": 2226.9320487976074}}, \"EndTime\": 1588627089.612175, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627087.384742}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:09 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=271.208986374 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:09 INFO 140149161203520] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:09 INFO 140149161203520] #quality_metric: host=algo-1, epoch=108, train loss <loss>=3.39122898579\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:09 INFO 140149161203520] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:09 INFO 140149161203520] Saved checkpoint to \"/opt/ml/model/state_22ec2fd2-0147-412f-9e68-a2347e845aef-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 69.41103935241699, \"sum\": 69.41103935241699, \"min\": 69.41103935241699}}, \"EndTime\": 1588627089.682206, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627089.612269}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:10 INFO 140149161203520] Epoch[109] Batch[0] avg_epoch_loss=3.612823\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:10 INFO 140149161203520] #quality_metric: host=algo-1, epoch=109, batch=0 train loss <loss>=3.61282253265\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:11 INFO 140149161203520] Epoch[109] Batch[5] avg_epoch_loss=3.538683\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:11 INFO 140149161203520] #quality_metric: host=algo-1, epoch=109, batch=5 train loss <loss>=3.53868349393\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:11 INFO 140149161203520] Epoch[109] Batch [5]#011Speed: 323.85 samples/sec#011loss=3.538683\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:11 INFO 140149161203520] processed a total of 612 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2277.40478515625, \"sum\": 2277.40478515625, \"min\": 2277.40478515625}}, \"EndTime\": 1588627091.959744, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627089.682274}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:11 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=268.710503153 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:11 INFO 140149161203520] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:11 INFO 140149161203520] #quality_metric: host=algo-1, epoch=109, train loss <loss>=3.57183556557\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:11 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:12 INFO 140149161203520] Epoch[110] Batch[0] avg_epoch_loss=3.585168\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:12 INFO 140149161203520] #quality_metric: host=algo-1, epoch=110, batch=0 train loss <loss>=3.58516764641\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:13 INFO 140149161203520] Epoch[110] Batch[5] avg_epoch_loss=3.495737\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:13 INFO 140149161203520] #quality_metric: host=algo-1, epoch=110, batch=5 train loss <loss>=3.4957373937\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:13 INFO 140149161203520] Epoch[110] Batch [5]#011Speed: 316.91 samples/sec#011loss=3.495737\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:14 INFO 140149161203520] processed a total of 604 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2265.7079696655273, \"sum\": 2265.7079696655273, \"min\": 2265.7079696655273}}, \"EndTime\": 1588627094.226054, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627091.959841}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:14 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=266.57049352 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:14 INFO 140149161203520] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:14 INFO 140149161203520] #quality_metric: host=algo-1, epoch=110, train loss <loss>=3.54707272053\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:14 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:14 INFO 140149161203520] Epoch[111] Batch[0] avg_epoch_loss=3.602435\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:14 INFO 140149161203520] #quality_metric: host=algo-1, epoch=111, batch=0 train loss <loss>=3.60243487358\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:15 INFO 140149161203520] Epoch[111] Batch[5] avg_epoch_loss=3.520984\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:15 INFO 140149161203520] #quality_metric: host=algo-1, epoch=111, batch=5 train loss <loss>=3.52098373572\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:15 INFO 140149161203520] Epoch[111] Batch [5]#011Speed: 322.31 samples/sec#011loss=3.520984\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:16 INFO 140149161203520] Epoch[111] Batch[10] avg_epoch_loss=3.445808\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:16 INFO 140149161203520] #quality_metric: host=algo-1, epoch=111, batch=10 train loss <loss>=3.35559735298\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:16 INFO 140149161203520] Epoch[111] Batch [10]#011Speed: 319.35 samples/sec#011loss=3.355597\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:16 INFO 140149161203520] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2445.6799030303955, \"sum\": 2445.6799030303955, \"min\": 2445.6799030303955}}, \"EndTime\": 1588627096.672305, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627094.226127}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:16 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=264.533824946 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:16 INFO 140149161203520] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:16 INFO 140149161203520] #quality_metric: host=algo-1, epoch=111, train loss <loss>=3.4458081072\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:16 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:17 INFO 140149161203520] Epoch[112] Batch[0] avg_epoch_loss=3.647395\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:17 INFO 140149161203520] #quality_metric: host=algo-1, epoch=112, batch=0 train loss <loss>=3.64739537239\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:18 INFO 140149161203520] Epoch[112] Batch[5] avg_epoch_loss=3.552199\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:18 INFO 140149161203520] #quality_metric: host=algo-1, epoch=112, batch=5 train loss <loss>=3.55219900608\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:18 INFO 140149161203520] Epoch[112] Batch [5]#011Speed: 310.44 samples/sec#011loss=3.552199\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:19 INFO 140149161203520] Epoch[112] Batch[10] avg_epoch_loss=3.505529\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:19 INFO 140149161203520] #quality_metric: host=algo-1, epoch=112, batch=10 train loss <loss>=3.44952392578\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:19 INFO 140149161203520] Epoch[112] Batch [10]#011Speed: 318.81 samples/sec#011loss=3.449524\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:19 INFO 140149161203520] processed a total of 690 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2496.243953704834, \"sum\": 2496.243953704834, \"min\": 2496.243953704834}}, \"EndTime\": 1588627099.169073, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627096.672396}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:19 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=276.401009422 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:19 INFO 140149161203520] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:19 INFO 140149161203520] #quality_metric: host=algo-1, epoch=112, train loss <loss>=3.50552851504\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:19 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:19 INFO 140149161203520] Epoch[113] Batch[0] avg_epoch_loss=3.250418\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:19 INFO 140149161203520] #quality_metric: host=algo-1, epoch=113, batch=0 train loss <loss>=3.25041794777\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:20 INFO 140149161203520] Epoch[113] Batch[5] avg_epoch_loss=3.432559\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:20 INFO 140149161203520] #quality_metric: host=algo-1, epoch=113, batch=5 train loss <loss>=3.4325590531\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:20 INFO 140149161203520] Epoch[113] Batch [5]#011Speed: 319.77 samples/sec#011loss=3.432559\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:21 INFO 140149161203520] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2257.16495513916, \"sum\": 2257.16495513916, \"min\": 2257.16495513916}}, \"EndTime\": 1588627101.426751, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627099.169161}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:21 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=275.993222509 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:21 INFO 140149161203520] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:21 INFO 140149161203520] #quality_metric: host=algo-1, epoch=113, train loss <loss>=3.4906131506\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:21 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:21 INFO 140149161203520] Epoch[114] Batch[0] avg_epoch_loss=3.213653\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:21 INFO 140149161203520] #quality_metric: host=algo-1, epoch=114, batch=0 train loss <loss>=3.21365308762\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:22 INFO 140149161203520] Epoch[114] Batch[5] avg_epoch_loss=3.442863\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:22 INFO 140149161203520] #quality_metric: host=algo-1, epoch=114, batch=5 train loss <loss>=3.44286262989\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:22 INFO 140149161203520] Epoch[114] Batch [5]#011Speed: 325.86 samples/sec#011loss=3.442863\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:23 INFO 140149161203520] processed a total of 612 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2241.875171661377, \"sum\": 2241.875171661377, \"min\": 2241.875171661377}}, \"EndTime\": 1588627103.669315, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627101.426848}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:23 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=272.968720693 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:23 INFO 140149161203520] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:23 INFO 140149161203520] #quality_metric: host=algo-1, epoch=114, train loss <loss>=3.51615698338\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:23 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:24 INFO 140149161203520] Epoch[115] Batch[0] avg_epoch_loss=3.512821\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:24 INFO 140149161203520] #quality_metric: host=algo-1, epoch=115, batch=0 train loss <loss>=3.51282143593\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:25 INFO 140149161203520] Epoch[115] Batch[5] avg_epoch_loss=3.416051\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:25 INFO 140149161203520] #quality_metric: host=algo-1, epoch=115, batch=5 train loss <loss>=3.41605071227\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:25 INFO 140149161203520] Epoch[115] Batch [5]#011Speed: 314.63 samples/sec#011loss=3.416051\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:26 INFO 140149161203520] Epoch[115] Batch[10] avg_epoch_loss=3.402209\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:26 INFO 140149161203520] #quality_metric: host=algo-1, epoch=115, batch=10 train loss <loss>=3.38559875488\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:26 INFO 140149161203520] Epoch[115] Batch [10]#011Speed: 316.53 samples/sec#011loss=3.385599\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:26 INFO 140149161203520] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2500.385046005249, \"sum\": 2500.385046005249, \"min\": 2500.385046005249}}, \"EndTime\": 1588627106.170279, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627103.669412}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:26 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=261.146504336 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:26 INFO 140149161203520] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:26 INFO 140149161203520] #quality_metric: host=algo-1, epoch=115, train loss <loss>=3.40220891346\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:26 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:26 INFO 140149161203520] Epoch[116] Batch[0] avg_epoch_loss=3.501971\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:26 INFO 140149161203520] #quality_metric: host=algo-1, epoch=116, batch=0 train loss <loss>=3.50197052956\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:27 INFO 140149161203520] Epoch[116] Batch[5] avg_epoch_loss=3.478144\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:27 INFO 140149161203520] #quality_metric: host=algo-1, epoch=116, batch=5 train loss <loss>=3.47814448675\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:27 INFO 140149161203520] Epoch[116] Batch [5]#011Speed: 326.26 samples/sec#011loss=3.478144\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:28 INFO 140149161203520] Epoch[116] Batch[10] avg_epoch_loss=3.456528\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:28 INFO 140149161203520] #quality_metric: host=algo-1, epoch=116, batch=10 train loss <loss>=3.4305876255\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:28 INFO 140149161203520] Epoch[116] Batch [10]#011Speed: 321.26 samples/sec#011loss=3.430588\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:28 INFO 140149161203520] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2433.3391189575195, \"sum\": 2433.3391189575195, \"min\": 2433.3391189575195}}, \"EndTime\": 1588627108.604141, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627106.170366}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:28 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=270.807157968 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:28 INFO 140149161203520] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:28 INFO 140149161203520] #quality_metric: host=algo-1, epoch=116, train loss <loss>=3.45652773164\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:28 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:29 INFO 140149161203520] Epoch[117] Batch[0] avg_epoch_loss=3.718157\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:29 INFO 140149161203520] #quality_metric: host=algo-1, epoch=117, batch=0 train loss <loss>=3.71815657616\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:30 INFO 140149161203520] Epoch[117] Batch[5] avg_epoch_loss=3.588603\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:30 INFO 140149161203520] #quality_metric: host=algo-1, epoch=117, batch=5 train loss <loss>=3.58860274156\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:30 INFO 140149161203520] Epoch[117] Batch [5]#011Speed: 325.63 samples/sec#011loss=3.588603\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:31 INFO 140149161203520] Epoch[117] Batch[10] avg_epoch_loss=3.596960\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:31 INFO 140149161203520] #quality_metric: host=algo-1, epoch=117, batch=10 train loss <loss>=3.60698838234\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:31 INFO 140149161203520] Epoch[117] Batch [10]#011Speed: 324.48 samples/sec#011loss=3.606988\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:31 INFO 140149161203520] processed a total of 676 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2455.456018447876, \"sum\": 2455.456018447876, \"min\": 2455.456018447876}}, \"EndTime\": 1588627111.060188, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627108.604229}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:31 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=275.291912202 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:31 INFO 140149161203520] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:31 INFO 140149161203520] #quality_metric: host=algo-1, epoch=117, train loss <loss>=3.596959851\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:31 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:31 INFO 140149161203520] Epoch[118] Batch[0] avg_epoch_loss=3.438893\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:31 INFO 140149161203520] #quality_metric: host=algo-1, epoch=118, batch=0 train loss <loss>=3.43889284134\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:32 INFO 140149161203520] Epoch[118] Batch[5] avg_epoch_loss=3.432630\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:32 INFO 140149161203520] #quality_metric: host=algo-1, epoch=118, batch=5 train loss <loss>=3.43262998263\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:32 INFO 140149161203520] Epoch[118] Batch [5]#011Speed: 317.93 samples/sec#011loss=3.432630\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:33 INFO 140149161203520] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2253.035068511963, \"sum\": 2253.035068511963, \"min\": 2253.035068511963}}, \"EndTime\": 1588627113.313725, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627111.060264}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:33 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=284.04302366 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:33 INFO 140149161203520] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:33 INFO 140149161203520] #quality_metric: host=algo-1, epoch=118, train loss <loss>=3.46309485435\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:33 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:33 INFO 140149161203520] Epoch[119] Batch[0] avg_epoch_loss=3.724417\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:33 INFO 140149161203520] #quality_metric: host=algo-1, epoch=119, batch=0 train loss <loss>=3.72441697121\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:34 INFO 140149161203520] Epoch[119] Batch[5] avg_epoch_loss=3.515888\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:34 INFO 140149161203520] #quality_metric: host=algo-1, epoch=119, batch=5 train loss <loss>=3.51588753859\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:34 INFO 140149161203520] Epoch[119] Batch [5]#011Speed: 317.40 samples/sec#011loss=3.515888\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:35 INFO 140149161203520] Epoch[119] Batch[10] avg_epoch_loss=3.450138\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:35 INFO 140149161203520] #quality_metric: host=algo-1, epoch=119, batch=10 train loss <loss>=3.37123775482\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:35 INFO 140149161203520] Epoch[119] Batch [10]#011Speed: 311.47 samples/sec#011loss=3.371238\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:35 INFO 140149161203520] processed a total of 666 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2502.506971359253, \"sum\": 2502.506971359253, \"min\": 2502.506971359253}}, \"EndTime\": 1588627115.816835, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627113.313829}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:35 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=266.119611219 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:35 INFO 140149161203520] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:35 INFO 140149161203520] #quality_metric: host=algo-1, epoch=119, train loss <loss>=3.45013763688\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:35 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:36 INFO 140149161203520] Epoch[120] Batch[0] avg_epoch_loss=3.528862\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:36 INFO 140149161203520] #quality_metric: host=algo-1, epoch=120, batch=0 train loss <loss>=3.52886152267\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:37 INFO 140149161203520] Epoch[120] Batch[5] avg_epoch_loss=3.447604\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:37 INFO 140149161203520] #quality_metric: host=algo-1, epoch=120, batch=5 train loss <loss>=3.44760366281\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:37 INFO 140149161203520] Epoch[120] Batch [5]#011Speed: 326.97 samples/sec#011loss=3.447604\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:38 INFO 140149161203520] processed a total of 595 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2245.3701496124268, \"sum\": 2245.3701496124268, \"min\": 2245.3701496124268}}, \"EndTime\": 1588627118.062735, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627115.816924}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:38 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=264.970500448 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:38 INFO 140149161203520] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:38 INFO 140149161203520] #quality_metric: host=algo-1, epoch=120, train loss <loss>=3.5430678606\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:38 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:38 INFO 140149161203520] Epoch[121] Batch[0] avg_epoch_loss=3.739848\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:38 INFO 140149161203520] #quality_metric: host=algo-1, epoch=121, batch=0 train loss <loss>=3.73984837532\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:39 INFO 140149161203520] Epoch[121] Batch[5] avg_epoch_loss=3.528770\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:39 INFO 140149161203520] #quality_metric: host=algo-1, epoch=121, batch=5 train loss <loss>=3.52876965205\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:39 INFO 140149161203520] Epoch[121] Batch [5]#011Speed: 321.27 samples/sec#011loss=3.528770\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:40 INFO 140149161203520] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2246.8369007110596, \"sum\": 2246.8369007110596, \"min\": 2246.8369007110596}}, \"EndTime\": 1588627120.310258, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627118.062853}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:40 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=284.827234885 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:40 INFO 140149161203520] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:40 INFO 140149161203520] #quality_metric: host=algo-1, epoch=121, train loss <loss>=3.46365530491\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:40 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:40 INFO 140149161203520] Epoch[122] Batch[0] avg_epoch_loss=3.377324\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:40 INFO 140149161203520] #quality_metric: host=algo-1, epoch=122, batch=0 train loss <loss>=3.37732410431\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:41 INFO 140149161203520] Epoch[122] Batch[5] avg_epoch_loss=3.396852\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:41 INFO 140149161203520] #quality_metric: host=algo-1, epoch=122, batch=5 train loss <loss>=3.39685217539\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:41 INFO 140149161203520] Epoch[122] Batch [5]#011Speed: 312.41 samples/sec#011loss=3.396852\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:42 INFO 140149161203520] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2282.5140953063965, \"sum\": 2282.5140953063965, \"min\": 2282.5140953063965}}, \"EndTime\": 1588627122.593355, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627120.310355}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:42 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=278.622424554 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:42 INFO 140149161203520] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:42 INFO 140149161203520] #quality_metric: host=algo-1, epoch=122, train loss <loss>=3.39807114601\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:42 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:43 INFO 140149161203520] Epoch[123] Batch[0] avg_epoch_loss=3.549369\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:43 INFO 140149161203520] #quality_metric: host=algo-1, epoch=123, batch=0 train loss <loss>=3.54936909676\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:44 INFO 140149161203520] Epoch[123] Batch[5] avg_epoch_loss=3.523204\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:44 INFO 140149161203520] #quality_metric: host=algo-1, epoch=123, batch=5 train loss <loss>=3.52320361137\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:44 INFO 140149161203520] Epoch[123] Batch [5]#011Speed: 319.59 samples/sec#011loss=3.523204\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:45 INFO 140149161203520] Epoch[123] Batch[10] avg_epoch_loss=3.553018\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:45 INFO 140149161203520] #quality_metric: host=algo-1, epoch=123, batch=10 train loss <loss>=3.58879432678\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:45 INFO 140149161203520] Epoch[123] Batch [10]#011Speed: 318.75 samples/sec#011loss=3.588794\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:45 INFO 140149161203520] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2459.400177001953, \"sum\": 2459.400177001953, \"min\": 2459.400177001953}}, \"EndTime\": 1588627125.053372, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627122.593454}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:45 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=263.464984157 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:45 INFO 140149161203520] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:45 INFO 140149161203520] #quality_metric: host=algo-1, epoch=123, train loss <loss>=3.55301757292\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:45 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:45 INFO 140149161203520] Epoch[124] Batch[0] avg_epoch_loss=3.437369\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:45 INFO 140149161203520] #quality_metric: host=algo-1, epoch=124, batch=0 train loss <loss>=3.43736934662\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:46 INFO 140149161203520] Epoch[124] Batch[5] avg_epoch_loss=3.450912\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:46 INFO 140149161203520] #quality_metric: host=algo-1, epoch=124, batch=5 train loss <loss>=3.45091160138\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:46 INFO 140149161203520] Epoch[124] Batch [5]#011Speed: 316.35 samples/sec#011loss=3.450912\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:47 INFO 140149161203520] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2270.8590030670166, \"sum\": 2270.8590030670166, \"min\": 2270.8590030670166}}, \"EndTime\": 1588627127.324827, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627125.053463}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:47 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=271.688261099 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:47 INFO 140149161203520] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:47 INFO 140149161203520] #quality_metric: host=algo-1, epoch=124, train loss <loss>=3.44061701298\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:47 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:47 INFO 140149161203520] Epoch[125] Batch[0] avg_epoch_loss=3.615879\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:47 INFO 140149161203520] #quality_metric: host=algo-1, epoch=125, batch=0 train loss <loss>=3.615878582\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:48 INFO 140149161203520] Epoch[125] Batch[5] avg_epoch_loss=3.499470\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:48 INFO 140149161203520] #quality_metric: host=algo-1, epoch=125, batch=5 train loss <loss>=3.49946971734\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:48 INFO 140149161203520] Epoch[125] Batch [5]#011Speed: 313.82 samples/sec#011loss=3.499470\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:49 INFO 140149161203520] Epoch[125] Batch[10] avg_epoch_loss=3.458956\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:49 INFO 140149161203520] #quality_metric: host=algo-1, epoch=125, batch=10 train loss <loss>=3.41033887863\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:49 INFO 140149161203520] Epoch[125] Batch [10]#011Speed: 315.93 samples/sec#011loss=3.410339\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:49 INFO 140149161203520] processed a total of 662 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2492.2759532928467, \"sum\": 2492.2759532928467, \"min\": 2492.2759532928467}}, \"EndTime\": 1588627129.817736, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627127.324915}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:49 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=265.606920439 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:49 INFO 140149161203520] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:49 INFO 140149161203520] #quality_metric: host=algo-1, epoch=125, train loss <loss>=3.45895569975\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:49 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:50 INFO 140149161203520] Epoch[126] Batch[0] avg_epoch_loss=3.436732\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:50 INFO 140149161203520] #quality_metric: host=algo-1, epoch=126, batch=0 train loss <loss>=3.43673181534\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:51 INFO 140149161203520] Epoch[126] Batch[5] avg_epoch_loss=3.443892\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:51 INFO 140149161203520] #quality_metric: host=algo-1, epoch=126, batch=5 train loss <loss>=3.44389247894\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:51 INFO 140149161203520] Epoch[126] Batch [5]#011Speed: 315.33 samples/sec#011loss=3.443892\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:52 INFO 140149161203520] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2264.6448612213135, \"sum\": 2264.6448612213135, \"min\": 2264.6448612213135}}, \"EndTime\": 1588627132.082952, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627129.817823}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:52 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=273.753147989 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:52 INFO 140149161203520] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:52 INFO 140149161203520] #quality_metric: host=algo-1, epoch=126, train loss <loss>=3.43149333\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:52 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:52 INFO 140149161203520] Epoch[127] Batch[0] avg_epoch_loss=3.363493\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:52 INFO 140149161203520] #quality_metric: host=algo-1, epoch=127, batch=0 train loss <loss>=3.3634929657\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:53 INFO 140149161203520] Epoch[127] Batch[5] avg_epoch_loss=3.489687\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:53 INFO 140149161203520] #quality_metric: host=algo-1, epoch=127, batch=5 train loss <loss>=3.48968692621\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:53 INFO 140149161203520] Epoch[127] Batch [5]#011Speed: 323.67 samples/sec#011loss=3.489687\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:54 INFO 140149161203520] Epoch[127] Batch[10] avg_epoch_loss=3.429770\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:54 INFO 140149161203520] #quality_metric: host=algo-1, epoch=127, batch=10 train loss <loss>=3.35786938667\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:54 INFO 140149161203520] Epoch[127] Batch [10]#011Speed: 318.35 samples/sec#011loss=3.357869\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:54 INFO 140149161203520] processed a total of 670 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2451.678991317749, \"sum\": 2451.678991317749, \"min\": 2451.678991317749}}, \"EndTime\": 1588627134.535296, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627132.083082}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:54 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=273.267384577 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:54 INFO 140149161203520] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:54 INFO 140149161203520] #quality_metric: host=algo-1, epoch=127, train loss <loss>=3.42976986278\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:54 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:55 INFO 140149161203520] Epoch[128] Batch[0] avg_epoch_loss=3.489623\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:55 INFO 140149161203520] #quality_metric: host=algo-1, epoch=128, batch=0 train loss <loss>=3.48962306976\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:55 INFO 140149161203520] Epoch[128] Batch[5] avg_epoch_loss=3.451138\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:55 INFO 140149161203520] #quality_metric: host=algo-1, epoch=128, batch=5 train loss <loss>=3.45113817851\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:55 INFO 140149161203520] Epoch[128] Batch [5]#011Speed: 326.19 samples/sec#011loss=3.451138\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:56 INFO 140149161203520] Epoch[128] Batch[10] avg_epoch_loss=3.399783\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:56 INFO 140149161203520] #quality_metric: host=algo-1, epoch=128, batch=10 train loss <loss>=3.33815608025\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:56 INFO 140149161203520] Epoch[128] Batch [10]#011Speed: 320.12 samples/sec#011loss=3.338156\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:56 INFO 140149161203520] processed a total of 665 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2445.76096534729, \"sum\": 2445.76096534729, \"min\": 2445.76096534729}}, \"EndTime\": 1588627136.981603, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627134.535386}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:56 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=271.884783428 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:56 INFO 140149161203520] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:56 INFO 140149161203520] #quality_metric: host=algo-1, epoch=128, train loss <loss>=3.3997826793\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:56 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:57 INFO 140149161203520] Epoch[129] Batch[0] avg_epoch_loss=3.308844\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:57 INFO 140149161203520] #quality_metric: host=algo-1, epoch=129, batch=0 train loss <loss>=3.30884432793\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:58 INFO 140149161203520] Epoch[129] Batch[5] avg_epoch_loss=3.333279\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:58 INFO 140149161203520] #quality_metric: host=algo-1, epoch=129, batch=5 train loss <loss>=3.33327933153\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:58 INFO 140149161203520] Epoch[129] Batch [5]#011Speed: 324.51 samples/sec#011loss=3.333279\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:59 INFO 140149161203520] Epoch[129] Batch[10] avg_epoch_loss=3.460873\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:59 INFO 140149161203520] #quality_metric: host=algo-1, epoch=129, batch=10 train loss <loss>=3.61398601532\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:59 INFO 140149161203520] Epoch[129] Batch [10]#011Speed: 316.92 samples/sec#011loss=3.613986\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:59 INFO 140149161203520] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2474.947929382324, \"sum\": 2474.947929382324, \"min\": 2474.947929382324}}, \"EndTime\": 1588627139.45709, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627136.981691}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:59 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=259.386603239 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:59 INFO 140149161203520] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:59 INFO 140149161203520] #quality_metric: host=algo-1, epoch=129, train loss <loss>=3.4608732787\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:59 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:59 INFO 140149161203520] Epoch[130] Batch[0] avg_epoch_loss=3.386206\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:18:59 INFO 140149161203520] #quality_metric: host=algo-1, epoch=130, batch=0 train loss <loss>=3.38620638847\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:00 INFO 140149161203520] Epoch[130] Batch[5] avg_epoch_loss=3.474439\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:00 INFO 140149161203520] #quality_metric: host=algo-1, epoch=130, batch=5 train loss <loss>=3.47443878651\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:00 INFO 140149161203520] Epoch[130] Batch [5]#011Speed: 322.25 samples/sec#011loss=3.474439\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:01 INFO 140149161203520] Epoch[130] Batch[10] avg_epoch_loss=3.481724\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:01 INFO 140149161203520] #quality_metric: host=algo-1, epoch=130, batch=10 train loss <loss>=3.49046592712\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:01 INFO 140149161203520] Epoch[130] Batch [10]#011Speed: 318.17 samples/sec#011loss=3.490466\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:01 INFO 140149161203520] processed a total of 694 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2486.133098602295, \"sum\": 2486.133098602295, \"min\": 2486.133098602295}}, \"EndTime\": 1588627141.943731, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627139.457174}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:01 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=279.134128171 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:01 INFO 140149161203520] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:01 INFO 140149161203520] #quality_metric: host=algo-1, epoch=130, train loss <loss>=3.48172385042\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:01 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:02 INFO 140149161203520] Epoch[131] Batch[0] avg_epoch_loss=3.240151\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:02 INFO 140149161203520] #quality_metric: host=algo-1, epoch=131, batch=0 train loss <loss>=3.2401509285\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:03 INFO 140149161203520] Epoch[131] Batch[5] avg_epoch_loss=3.393108\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:03 INFO 140149161203520] #quality_metric: host=algo-1, epoch=131, batch=5 train loss <loss>=3.39310789108\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:03 INFO 140149161203520] Epoch[131] Batch [5]#011Speed: 320.29 samples/sec#011loss=3.393108\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:04 INFO 140149161203520] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2267.728090286255, \"sum\": 2267.728090286255, \"min\": 2267.728090286255}}, \"EndTime\": 1588627144.212006, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627141.94382}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:04 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=275.148168255 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:04 INFO 140149161203520] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:04 INFO 140149161203520] #quality_metric: host=algo-1, epoch=131, train loss <loss>=3.47117028236\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:04 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:04 INFO 140149161203520] Epoch[132] Batch[0] avg_epoch_loss=3.515849\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:04 INFO 140149161203520] #quality_metric: host=algo-1, epoch=132, batch=0 train loss <loss>=3.51584935188\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:05 INFO 140149161203520] Epoch[132] Batch[5] avg_epoch_loss=3.434819\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:05 INFO 140149161203520] #quality_metric: host=algo-1, epoch=132, batch=5 train loss <loss>=3.43481906255\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:05 INFO 140149161203520] Epoch[132] Batch [5]#011Speed: 310.26 samples/sec#011loss=3.434819\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:06 INFO 140149161203520] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2333.815813064575, \"sum\": 2333.815813064575, \"min\": 2333.815813064575}}, \"EndTime\": 1588627146.546421, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627144.212105}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:06 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=271.64201778 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:06 INFO 140149161203520] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:06 INFO 140149161203520] #quality_metric: host=algo-1, epoch=132, train loss <loss>=3.44148259163\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:06 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:07 INFO 140149161203520] Epoch[133] Batch[0] avg_epoch_loss=3.318863\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:07 INFO 140149161203520] #quality_metric: host=algo-1, epoch=133, batch=0 train loss <loss>=3.31886291504\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:08 INFO 140149161203520] Epoch[133] Batch[5] avg_epoch_loss=3.446322\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:08 INFO 140149161203520] #quality_metric: host=algo-1, epoch=133, batch=5 train loss <loss>=3.44632224242\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:08 INFO 140149161203520] Epoch[133] Batch [5]#011Speed: 323.57 samples/sec#011loss=3.446322\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:09 INFO 140149161203520] Epoch[133] Batch[10] avg_epoch_loss=3.376485\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:09 INFO 140149161203520] #quality_metric: host=algo-1, epoch=133, batch=10 train loss <loss>=3.29268078804\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:09 INFO 140149161203520] Epoch[133] Batch [10]#011Speed: 318.16 samples/sec#011loss=3.292681\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:09 INFO 140149161203520] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2466.9060707092285, \"sum\": 2466.9060707092285, \"min\": 2466.9060707092285}}, \"EndTime\": 1588627149.013912, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627146.546517}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:09 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=264.692578098 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:09 INFO 140149161203520] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:09 INFO 140149161203520] #quality_metric: host=algo-1, epoch=133, train loss <loss>=3.3764852177\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:09 INFO 140149161203520] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:09 INFO 140149161203520] Saved checkpoint to \"/opt/ml/model/state_edba2e77-96b3-451e-a80c-8ec4f1d6c9b9-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 60.40000915527344, \"sum\": 60.40000915527344, \"min\": 60.40000915527344}}, \"EndTime\": 1588627149.074822, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627149.013979}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:09 INFO 140149161203520] Epoch[134] Batch[0] avg_epoch_loss=3.478561\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:09 INFO 140149161203520] #quality_metric: host=algo-1, epoch=134, batch=0 train loss <loss>=3.47856116295\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:10 INFO 140149161203520] Epoch[134] Batch[5] avg_epoch_loss=3.480962\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:10 INFO 140149161203520] #quality_metric: host=algo-1, epoch=134, batch=5 train loss <loss>=3.48096239567\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:10 INFO 140149161203520] Epoch[134] Batch [5]#011Speed: 321.25 samples/sec#011loss=3.480962\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:11 INFO 140149161203520] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2243.100881576538, \"sum\": 2243.100881576538, \"min\": 2243.100881576538}}, \"EndTime\": 1588627151.318064, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627149.074899}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:11 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=278.169210059 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:11 INFO 140149161203520] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:11 INFO 140149161203520] #quality_metric: host=algo-1, epoch=134, train loss <loss>=3.47295258045\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:11 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:11 INFO 140149161203520] Epoch[135] Batch[0] avg_epoch_loss=3.272233\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:11 INFO 140149161203520] #quality_metric: host=algo-1, epoch=135, batch=0 train loss <loss>=3.27223324776\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:12 INFO 140149161203520] Epoch[135] Batch[5] avg_epoch_loss=3.456787\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:12 INFO 140149161203520] #quality_metric: host=algo-1, epoch=135, batch=5 train loss <loss>=3.45678655306\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:12 INFO 140149161203520] Epoch[135] Batch [5]#011Speed: 324.62 samples/sec#011loss=3.456787\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:13 INFO 140149161203520] Epoch[135] Batch[10] avg_epoch_loss=3.381986\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:13 INFO 140149161203520] #quality_metric: host=algo-1, epoch=135, batch=10 train loss <loss>=3.29222602844\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:13 INFO 140149161203520] Epoch[135] Batch [10]#011Speed: 323.78 samples/sec#011loss=3.292226\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:13 INFO 140149161203520] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2436.290979385376, \"sum\": 2436.290979385376, \"min\": 2436.290979385376}}, \"EndTime\": 1588627153.754923, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627151.31816}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:13 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=263.088563875 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:13 INFO 140149161203520] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:13 INFO 140149161203520] #quality_metric: host=algo-1, epoch=135, train loss <loss>=3.3819863146\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:13 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:14 INFO 140149161203520] Epoch[136] Batch[0] avg_epoch_loss=3.516815\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:14 INFO 140149161203520] #quality_metric: host=algo-1, epoch=136, batch=0 train loss <loss>=3.51681494713\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:15 INFO 140149161203520] Epoch[136] Batch[5] avg_epoch_loss=3.477727\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:15 INFO 140149161203520] #quality_metric: host=algo-1, epoch=136, batch=5 train loss <loss>=3.47772681713\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:15 INFO 140149161203520] Epoch[136] Batch [5]#011Speed: 324.06 samples/sec#011loss=3.477727\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:16 INFO 140149161203520] processed a total of 627 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2260.230779647827, \"sum\": 2260.230779647827, \"min\": 2260.230779647827}}, \"EndTime\": 1588627156.015719, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627153.755032}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:16 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=277.388096195 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:16 INFO 140149161203520] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:16 INFO 140149161203520] #quality_metric: host=algo-1, epoch=136, train loss <loss>=3.45879814625\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:16 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:16 INFO 140149161203520] Epoch[137] Batch[0] avg_epoch_loss=3.395952\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:16 INFO 140149161203520] #quality_metric: host=algo-1, epoch=137, batch=0 train loss <loss>=3.39595150948\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:17 INFO 140149161203520] Epoch[137] Batch[5] avg_epoch_loss=3.402119\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:17 INFO 140149161203520] #quality_metric: host=algo-1, epoch=137, batch=5 train loss <loss>=3.40211904049\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:17 INFO 140149161203520] Epoch[137] Batch [5]#011Speed: 319.12 samples/sec#011loss=3.402119\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:18 INFO 140149161203520] Epoch[137] Batch[10] avg_epoch_loss=3.327119\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:18 INFO 140149161203520] #quality_metric: host=algo-1, epoch=137, batch=10 train loss <loss>=3.23711886406\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:18 INFO 140149161203520] Epoch[137] Batch [10]#011Speed: 319.94 samples/sec#011loss=3.237119\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:18 INFO 140149161203520] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2470.5700874328613, \"sum\": 2470.5700874328613, \"min\": 2470.5700874328613}}, \"EndTime\": 1588627158.486878, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627156.015816}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:18 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=260.655174274 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:18 INFO 140149161203520] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:18 INFO 140149161203520] #quality_metric: host=algo-1, epoch=137, train loss <loss>=3.32711896029\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:18 INFO 140149161203520] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:18 INFO 140149161203520] Saved checkpoint to \"/opt/ml/model/state_86dc3d32-cb4b-40da-b46d-2f5cd9187285-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 59.832096099853516, \"sum\": 59.832096099853516, \"min\": 59.832096099853516}}, \"EndTime\": 1588627158.547374, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627158.486961}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:19 INFO 140149161203520] Epoch[138] Batch[0] avg_epoch_loss=3.482156\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:19 INFO 140149161203520] #quality_metric: host=algo-1, epoch=138, batch=0 train loss <loss>=3.4821562767\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:19 INFO 140149161203520] Epoch[138] Batch[5] avg_epoch_loss=3.460502\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:19 INFO 140149161203520] #quality_metric: host=algo-1, epoch=138, batch=5 train loss <loss>=3.46050238609\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:20 INFO 140149161203520] Epoch[138] Batch [5]#011Speed: 323.81 samples/sec#011loss=3.460502\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:20 INFO 140149161203520] processed a total of 614 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2269.644021987915, \"sum\": 2269.644021987915, \"min\": 2269.644021987915}}, \"EndTime\": 1588627160.817164, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627158.547454}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:20 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=270.512569583 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:20 INFO 140149161203520] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:20 INFO 140149161203520] #quality_metric: host=algo-1, epoch=138, train loss <loss>=3.49088666439\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:20 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:21 INFO 140149161203520] Epoch[139] Batch[0] avg_epoch_loss=3.342031\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:21 INFO 140149161203520] #quality_metric: host=algo-1, epoch=139, batch=0 train loss <loss>=3.34203100204\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:22 INFO 140149161203520] Epoch[139] Batch[5] avg_epoch_loss=3.415603\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:22 INFO 140149161203520] #quality_metric: host=algo-1, epoch=139, batch=5 train loss <loss>=3.41560328007\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:22 INFO 140149161203520] Epoch[139] Batch [5]#011Speed: 319.94 samples/sec#011loss=3.415603\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:23 INFO 140149161203520] Epoch[139] Batch[10] avg_epoch_loss=3.397865\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:23 INFO 140149161203520] #quality_metric: host=algo-1, epoch=139, batch=10 train loss <loss>=3.3765791893\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:23 INFO 140149161203520] Epoch[139] Batch [10]#011Speed: 317.32 samples/sec#011loss=3.376579\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:23 INFO 140149161203520] processed a total of 678 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2475.5520820617676, \"sum\": 2475.5520820617676, \"min\": 2475.5520820617676}}, \"EndTime\": 1588627163.29332, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627160.817246}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:23 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=273.864586337 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:23 INFO 140149161203520] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:23 INFO 140149161203520] #quality_metric: host=algo-1, epoch=139, train loss <loss>=3.39786505699\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:23 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:23 INFO 140149161203520] Epoch[140] Batch[0] avg_epoch_loss=3.651563\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:23 INFO 140149161203520] #quality_metric: host=algo-1, epoch=140, batch=0 train loss <loss>=3.65156269073\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:24 INFO 140149161203520] Epoch[140] Batch[5] avg_epoch_loss=3.515987\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:24 INFO 140149161203520] #quality_metric: host=algo-1, epoch=140, batch=5 train loss <loss>=3.51598727703\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:24 INFO 140149161203520] Epoch[140] Batch [5]#011Speed: 313.35 samples/sec#011loss=3.515987\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:25 INFO 140149161203520] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2296.6129779815674, \"sum\": 2296.6129779815674, \"min\": 2296.6129779815674}}, \"EndTime\": 1588627165.59057, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627163.293402}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:25 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=274.303623846 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:25 INFO 140149161203520] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:25 INFO 140149161203520] #quality_metric: host=algo-1, epoch=140, train loss <loss>=3.49688193798\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:25 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:26 INFO 140149161203520] Epoch[141] Batch[0] avg_epoch_loss=3.561597\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:26 INFO 140149161203520] #quality_metric: host=algo-1, epoch=141, batch=0 train loss <loss>=3.561596632\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:27 INFO 140149161203520] Epoch[141] Batch[5] avg_epoch_loss=3.459511\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:27 INFO 140149161203520] #quality_metric: host=algo-1, epoch=141, batch=5 train loss <loss>=3.45951116085\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:27 INFO 140149161203520] Epoch[141] Batch [5]#011Speed: 316.90 samples/sec#011loss=3.459511\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:27 INFO 140149161203520] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2262.737989425659, \"sum\": 2262.737989425659, \"min\": 2262.737989425659}}, \"EndTime\": 1588627167.853937, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627165.590646}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:27 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=274.874715272 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:27 INFO 140149161203520] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:27 INFO 140149161203520] #quality_metric: host=algo-1, epoch=141, train loss <loss>=3.48227772713\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:27 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:28 INFO 140149161203520] Epoch[142] Batch[0] avg_epoch_loss=3.433974\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:28 INFO 140149161203520] #quality_metric: host=algo-1, epoch=142, batch=0 train loss <loss>=3.43397426605\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:29 INFO 140149161203520] Epoch[142] Batch[5] avg_epoch_loss=3.408462\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:29 INFO 140149161203520] #quality_metric: host=algo-1, epoch=142, batch=5 train loss <loss>=3.40846184889\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:29 INFO 140149161203520] Epoch[142] Batch [5]#011Speed: 300.88 samples/sec#011loss=3.408462\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:30 INFO 140149161203520] Epoch[142] Batch[10] avg_epoch_loss=3.427737\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:30 INFO 140149161203520] #quality_metric: host=algo-1, epoch=142, batch=10 train loss <loss>=3.45086636543\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:30 INFO 140149161203520] Epoch[142] Batch [10]#011Speed: 314.92 samples/sec#011loss=3.450866\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:30 INFO 140149161203520] processed a total of 691 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2543.3340072631836, \"sum\": 2543.3340072631836, \"min\": 2543.3340072631836}}, \"EndTime\": 1588627170.397907, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627167.85401}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:30 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=271.676844621 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:30 INFO 140149161203520] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:30 INFO 140149161203520] #quality_metric: host=algo-1, epoch=142, train loss <loss>=3.42773662914\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:30 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:30 INFO 140149161203520] Epoch[143] Batch[0] avg_epoch_loss=3.245671\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:30 INFO 140149161203520] #quality_metric: host=algo-1, epoch=143, batch=0 train loss <loss>=3.24567079544\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:31 INFO 140149161203520] Epoch[143] Batch[5] avg_epoch_loss=3.412797\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:31 INFO 140149161203520] #quality_metric: host=algo-1, epoch=143, batch=5 train loss <loss>=3.41279653708\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:31 INFO 140149161203520] Epoch[143] Batch [5]#011Speed: 322.56 samples/sec#011loss=3.412797\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:32 INFO 140149161203520] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2261.0719203948975, \"sum\": 2261.0719203948975, \"min\": 2261.0719203948975}}, \"EndTime\": 1588627172.659526, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627170.397996}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:32 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=276.400692436 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:32 INFO 140149161203520] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:32 INFO 140149161203520] #quality_metric: host=algo-1, epoch=143, train loss <loss>=3.45417673588\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:32 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:33 INFO 140149161203520] Epoch[144] Batch[0] avg_epoch_loss=3.346808\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:33 INFO 140149161203520] #quality_metric: host=algo-1, epoch=144, batch=0 train loss <loss>=3.34680843353\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:34 INFO 140149161203520] Epoch[144] Batch[5] avg_epoch_loss=3.393386\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:34 INFO 140149161203520] #quality_metric: host=algo-1, epoch=144, batch=5 train loss <loss>=3.39338580767\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:34 INFO 140149161203520] Epoch[144] Batch [5]#011Speed: 320.09 samples/sec#011loss=3.393386\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:34 INFO 140149161203520] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2275.550127029419, \"sum\": 2275.550127029419, \"min\": 2275.550127029419}}, \"EndTime\": 1588627174.935668, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627172.659623}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:34 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=279.036852607 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:34 INFO 140149161203520] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:34 INFO 140149161203520] #quality_metric: host=algo-1, epoch=144, train loss <loss>=3.40335652828\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:34 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:35 INFO 140149161203520] Epoch[145] Batch[0] avg_epoch_loss=3.308926\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:35 INFO 140149161203520] #quality_metric: host=algo-1, epoch=145, batch=0 train loss <loss>=3.30892634392\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:36 INFO 140149161203520] Epoch[145] Batch[5] avg_epoch_loss=3.386520\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:36 INFO 140149161203520] #quality_metric: host=algo-1, epoch=145, batch=5 train loss <loss>=3.38652018706\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:36 INFO 140149161203520] Epoch[145] Batch [5]#011Speed: 305.94 samples/sec#011loss=3.386520\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:37 INFO 140149161203520] Epoch[145] Batch[10] avg_epoch_loss=3.448118\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:37 INFO 140149161203520] #quality_metric: host=algo-1, epoch=145, batch=10 train loss <loss>=3.52203435898\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:37 INFO 140149161203520] Epoch[145] Batch [10]#011Speed: 317.51 samples/sec#011loss=3.522034\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:37 INFO 140149161203520] processed a total of 684 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2560.33992767334, \"sum\": 2560.33992767334, \"min\": 2560.33992767334}}, \"EndTime\": 1588627177.496599, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627174.935762}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:37 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=267.138543675 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:37 INFO 140149161203520] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:37 INFO 140149161203520] #quality_metric: host=algo-1, epoch=145, train loss <loss>=3.44811753793\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:37 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:38 INFO 140149161203520] Epoch[146] Batch[0] avg_epoch_loss=3.473289\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:38 INFO 140149161203520] #quality_metric: host=algo-1, epoch=146, batch=0 train loss <loss>=3.47328877449\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:38 INFO 140149161203520] Epoch[146] Batch[5] avg_epoch_loss=3.421216\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:38 INFO 140149161203520] #quality_metric: host=algo-1, epoch=146, batch=5 train loss <loss>=3.42121577263\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:38 INFO 140149161203520] Epoch[146] Batch [5]#011Speed: 325.28 samples/sec#011loss=3.421216\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:40 INFO 140149161203520] Epoch[146] Batch[10] avg_epoch_loss=3.497479\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:40 INFO 140149161203520] #quality_metric: host=algo-1, epoch=146, batch=10 train loss <loss>=3.5889957428\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:40 INFO 140149161203520] Epoch[146] Batch [10]#011Speed: 315.17 samples/sec#011loss=3.588996\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:40 INFO 140149161203520] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2506.767988204956, \"sum\": 2506.767988204956, \"min\": 2506.767988204956}}, \"EndTime\": 1588627180.003896, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627177.496687}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:40 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=256.891168958 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:40 INFO 140149161203520] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:40 INFO 140149161203520] #quality_metric: host=algo-1, epoch=146, train loss <loss>=3.49747939543\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:40 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:40 INFO 140149161203520] Epoch[147] Batch[0] avg_epoch_loss=3.606739\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:40 INFO 140149161203520] #quality_metric: host=algo-1, epoch=147, batch=0 train loss <loss>=3.60673904419\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:41 INFO 140149161203520] Epoch[147] Batch[5] avg_epoch_loss=3.444231\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:41 INFO 140149161203520] #quality_metric: host=algo-1, epoch=147, batch=5 train loss <loss>=3.44423095385\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:41 INFO 140149161203520] Epoch[147] Batch [5]#011Speed: 320.21 samples/sec#011loss=3.444231\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:42 INFO 140149161203520] Epoch[147] Batch[10] avg_epoch_loss=3.416908\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:42 INFO 140149161203520] #quality_metric: host=algo-1, epoch=147, batch=10 train loss <loss>=3.38412041664\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:42 INFO 140149161203520] Epoch[147] Batch [10]#011Speed: 323.63 samples/sec#011loss=3.384120\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:42 INFO 140149161203520] processed a total of 674 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2463.0939960479736, \"sum\": 2463.0939960479736, \"min\": 2463.0939960479736}}, \"EndTime\": 1588627182.467543, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627180.003985}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:42 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=273.625142501 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:42 INFO 140149161203520] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:42 INFO 140149161203520] #quality_metric: host=algo-1, epoch=147, train loss <loss>=3.41690798239\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:42 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:42 INFO 140149161203520] Epoch[148] Batch[0] avg_epoch_loss=3.396374\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:42 INFO 140149161203520] #quality_metric: host=algo-1, epoch=148, batch=0 train loss <loss>=3.3963739872\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:43 INFO 140149161203520] Epoch[148] Batch[5] avg_epoch_loss=3.418514\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:43 INFO 140149161203520] #quality_metric: host=algo-1, epoch=148, batch=5 train loss <loss>=3.41851353645\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:43 INFO 140149161203520] Epoch[148] Batch [5]#011Speed: 326.26 samples/sec#011loss=3.418514\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:44 INFO 140149161203520] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2269.752025604248, \"sum\": 2269.752025604248, \"min\": 2269.752025604248}}, \"EndTime\": 1588627184.73785, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627182.467634}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:44 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=275.784164738 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:44 INFO 140149161203520] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:44 INFO 140149161203520] #quality_metric: host=algo-1, epoch=148, train loss <loss>=3.38120570183\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:44 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:45 INFO 140149161203520] Epoch[149] Batch[0] avg_epoch_loss=3.456085\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:45 INFO 140149161203520] #quality_metric: host=algo-1, epoch=149, batch=0 train loss <loss>=3.45608472824\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:46 INFO 140149161203520] Epoch[149] Batch[5] avg_epoch_loss=3.391129\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:46 INFO 140149161203520] #quality_metric: host=algo-1, epoch=149, batch=5 train loss <loss>=3.39112925529\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:46 INFO 140149161203520] Epoch[149] Batch [5]#011Speed: 323.54 samples/sec#011loss=3.391129\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:47 INFO 140149161203520] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2264.4081115722656, \"sum\": 2264.4081115722656, \"min\": 2264.4081115722656}}, \"EndTime\": 1588627187.002874, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627184.737946}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:47 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=280.409032989 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:47 INFO 140149161203520] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:47 INFO 140149161203520] #quality_metric: host=algo-1, epoch=149, train loss <loss>=3.37583949566\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:47 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:47 INFO 140149161203520] Epoch[150] Batch[0] avg_epoch_loss=3.411975\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:47 INFO 140149161203520] #quality_metric: host=algo-1, epoch=150, batch=0 train loss <loss>=3.4119746685\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:48 INFO 140149161203520] Epoch[150] Batch[5] avg_epoch_loss=3.336944\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:48 INFO 140149161203520] #quality_metric: host=algo-1, epoch=150, batch=5 train loss <loss>=3.33694430192\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:48 INFO 140149161203520] Epoch[150] Batch [5]#011Speed: 320.79 samples/sec#011loss=3.336944\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:49 INFO 140149161203520] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2307.007074356079, \"sum\": 2307.007074356079, \"min\": 2307.007074356079}}, \"EndTime\": 1588627189.310503, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627187.002971}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:49 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=275.665962684 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:49 INFO 140149161203520] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:49 INFO 140149161203520] #quality_metric: host=algo-1, epoch=150, train loss <loss>=3.33786723614\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:49 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:49 INFO 140149161203520] Epoch[151] Batch[0] avg_epoch_loss=3.541784\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:49 INFO 140149161203520] #quality_metric: host=algo-1, epoch=151, batch=0 train loss <loss>=3.5417842865\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:50 INFO 140149161203520] Epoch[151] Batch[5] avg_epoch_loss=3.473149\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:50 INFO 140149161203520] #quality_metric: host=algo-1, epoch=151, batch=5 train loss <loss>=3.47314925989\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:50 INFO 140149161203520] Epoch[151] Batch [5]#011Speed: 318.89 samples/sec#011loss=3.473149\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:51 INFO 140149161203520] Epoch[151] Batch[10] avg_epoch_loss=3.511730\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:51 INFO 140149161203520] #quality_metric: host=algo-1, epoch=151, batch=10 train loss <loss>=3.55802717209\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:51 INFO 140149161203520] Epoch[151] Batch [10]#011Speed: 322.52 samples/sec#011loss=3.558027\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:51 INFO 140149161203520] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2472.576141357422, \"sum\": 2472.576141357422, \"min\": 2472.576141357422}}, \"EndTime\": 1588627191.783669, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627189.310595}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:51 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=259.634703387 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:51 INFO 140149161203520] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:51 INFO 140149161203520] #quality_metric: host=algo-1, epoch=151, train loss <loss>=3.51173012907\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:51 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:52 INFO 140149161203520] Epoch[152] Batch[0] avg_epoch_loss=3.277262\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:52 INFO 140149161203520] #quality_metric: host=algo-1, epoch=152, batch=0 train loss <loss>=3.27726244926\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:53 INFO 140149161203520] Epoch[152] Batch[5] avg_epoch_loss=3.357662\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:53 INFO 140149161203520] #quality_metric: host=algo-1, epoch=152, batch=5 train loss <loss>=3.35766232014\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:53 INFO 140149161203520] Epoch[152] Batch [5]#011Speed: 320.86 samples/sec#011loss=3.357662\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:54 INFO 140149161203520] processed a total of 627 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2304.245948791504, \"sum\": 2304.245948791504, \"min\": 2304.245948791504}}, \"EndTime\": 1588627194.088437, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627191.783757}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:54 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=272.089703165 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:54 INFO 140149161203520] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:54 INFO 140149161203520] #quality_metric: host=algo-1, epoch=152, train loss <loss>=3.41915516853\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:54 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:54 INFO 140149161203520] Epoch[153] Batch[0] avg_epoch_loss=3.499726\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:54 INFO 140149161203520] #quality_metric: host=algo-1, epoch=153, batch=0 train loss <loss>=3.49972605705\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:55 INFO 140149161203520] Epoch[153] Batch[5] avg_epoch_loss=3.408009\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:55 INFO 140149161203520] #quality_metric: host=algo-1, epoch=153, batch=5 train loss <loss>=3.4080094099\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:55 INFO 140149161203520] Epoch[153] Batch [5]#011Speed: 323.69 samples/sec#011loss=3.408009\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:56 INFO 140149161203520] Epoch[153] Batch[10] avg_epoch_loss=3.359437\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:56 INFO 140149161203520] #quality_metric: host=algo-1, epoch=153, batch=10 train loss <loss>=3.3011505127\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:56 INFO 140149161203520] Epoch[153] Batch [10]#011Speed: 320.96 samples/sec#011loss=3.301151\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:56 INFO 140149161203520] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2471.94504737854, \"sum\": 2471.94504737854, \"min\": 2471.94504737854}}, \"EndTime\": 1588627196.560976, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627194.088535}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:56 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=265.768739747 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:56 INFO 140149161203520] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:56 INFO 140149161203520] #quality_metric: host=algo-1, epoch=153, train loss <loss>=3.3594371839\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:56 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:57 INFO 140149161203520] Epoch[154] Batch[0] avg_epoch_loss=3.529356\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:57 INFO 140149161203520] #quality_metric: host=algo-1, epoch=154, batch=0 train loss <loss>=3.52935600281\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:58 INFO 140149161203520] Epoch[154] Batch[5] avg_epoch_loss=3.395541\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:58 INFO 140149161203520] #quality_metric: host=algo-1, epoch=154, batch=5 train loss <loss>=3.39554115136\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:58 INFO 140149161203520] Epoch[154] Batch [5]#011Speed: 323.75 samples/sec#011loss=3.395541\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:59 INFO 140149161203520] Epoch[154] Batch[10] avg_epoch_loss=3.337505\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:59 INFO 140149161203520] #quality_metric: host=algo-1, epoch=154, batch=10 train loss <loss>=3.26786251068\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:59 INFO 140149161203520] Epoch[154] Batch [10]#011Speed: 323.56 samples/sec#011loss=3.267863\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:59 INFO 140149161203520] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2454.848051071167, \"sum\": 2454.848051071167, \"min\": 2454.848051071167}}, \"EndTime\": 1588627199.016355, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627196.561064}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:59 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=262.324475369 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:59 INFO 140149161203520] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:59 INFO 140149161203520] #quality_metric: host=algo-1, epoch=154, train loss <loss>=3.3375054056\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:59 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:59 INFO 140149161203520] Epoch[155] Batch[0] avg_epoch_loss=3.468467\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:19:59 INFO 140149161203520] #quality_metric: host=algo-1, epoch=155, batch=0 train loss <loss>=3.46846675873\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:00 INFO 140149161203520] Epoch[155] Batch[5] avg_epoch_loss=3.424614\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:00 INFO 140149161203520] #quality_metric: host=algo-1, epoch=155, batch=5 train loss <loss>=3.42461351554\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:00 INFO 140149161203520] Epoch[155] Batch [5]#011Speed: 324.54 samples/sec#011loss=3.424614\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:01 INFO 140149161203520] Epoch[155] Batch[10] avg_epoch_loss=3.488730\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:01 INFO 140149161203520] #quality_metric: host=algo-1, epoch=155, batch=10 train loss <loss>=3.56567072868\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:01 INFO 140149161203520] Epoch[155] Batch [10]#011Speed: 318.10 samples/sec#011loss=3.565671\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:01 INFO 140149161203520] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2478.691816329956, \"sum\": 2478.691816329956, \"min\": 2478.691816329956}}, \"EndTime\": 1588627201.495574, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627199.016443}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:01 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=260.206019855 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:01 INFO 140149161203520] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:01 INFO 140149161203520] #quality_metric: host=algo-1, epoch=155, train loss <loss>=3.4887304306\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:01 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:01 INFO 140149161203520] Epoch[156] Batch[0] avg_epoch_loss=3.325397\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:01 INFO 140149161203520] #quality_metric: host=algo-1, epoch=156, batch=0 train loss <loss>=3.32539725304\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:02 INFO 140149161203520] Epoch[156] Batch[5] avg_epoch_loss=3.359113\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:02 INFO 140149161203520] #quality_metric: host=algo-1, epoch=156, batch=5 train loss <loss>=3.35911289851\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:02 INFO 140149161203520] Epoch[156] Batch [5]#011Speed: 321.18 samples/sec#011loss=3.359113\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:03 INFO 140149161203520] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2259.6960067749023, \"sum\": 2259.6960067749023, \"min\": 2259.6960067749023}}, \"EndTime\": 1588627203.755807, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627201.495645}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:03 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=281.436594728 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:03 INFO 140149161203520] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:03 INFO 140149161203520] #quality_metric: host=algo-1, epoch=156, train loss <loss>=3.39624676704\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:03 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:04 INFO 140149161203520] Epoch[157] Batch[0] avg_epoch_loss=3.392087\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:04 INFO 140149161203520] #quality_metric: host=algo-1, epoch=157, batch=0 train loss <loss>=3.39208745956\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:05 INFO 140149161203520] Epoch[157] Batch[5] avg_epoch_loss=3.359278\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:05 INFO 140149161203520] #quality_metric: host=algo-1, epoch=157, batch=5 train loss <loss>=3.35927784443\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:05 INFO 140149161203520] Epoch[157] Batch [5]#011Speed: 316.69 samples/sec#011loss=3.359278\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:06 INFO 140149161203520] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2294.3899631500244, \"sum\": 2294.3899631500244, \"min\": 2294.3899631500244}}, \"EndTime\": 1588627206.050838, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627203.755904}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:06 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=277.182772502 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:06 INFO 140149161203520] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:06 INFO 140149161203520] #quality_metric: host=algo-1, epoch=157, train loss <loss>=3.37193222046\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:06 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:06 INFO 140149161203520] Epoch[158] Batch[0] avg_epoch_loss=3.527897\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:06 INFO 140149161203520] #quality_metric: host=algo-1, epoch=158, batch=0 train loss <loss>=3.5278968811\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:07 INFO 140149161203520] Epoch[158] Batch[5] avg_epoch_loss=3.438918\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:07 INFO 140149161203520] #quality_metric: host=algo-1, epoch=158, batch=5 train loss <loss>=3.43891839186\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:07 INFO 140149161203520] Epoch[158] Batch [5]#011Speed: 317.82 samples/sec#011loss=3.438918\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:08 INFO 140149161203520] Epoch[158] Batch[10] avg_epoch_loss=3.315841\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:08 INFO 140149161203520] #quality_metric: host=algo-1, epoch=158, batch=10 train loss <loss>=3.16814713478\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:08 INFO 140149161203520] Epoch[158] Batch [10]#011Speed: 316.69 samples/sec#011loss=3.168147\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:08 INFO 140149161203520] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2470.9179401397705, \"sum\": 2470.9179401397705, \"min\": 2470.9179401397705}}, \"EndTime\": 1588627208.522258, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627206.050921}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:08 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=263.451291036 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:08 INFO 140149161203520] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:08 INFO 140149161203520] #quality_metric: host=algo-1, epoch=158, train loss <loss>=3.31584054774\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:08 INFO 140149161203520] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:08 INFO 140149161203520] Saved checkpoint to \"/opt/ml/model/state_9460e7a4-1206-4df2-8cc9-3ed66b2ef161-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 59.70406532287598, \"sum\": 59.70406532287598, \"min\": 59.70406532287598}}, \"EndTime\": 1588627208.58257, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627208.522342}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:09 INFO 140149161203520] Epoch[159] Batch[0] avg_epoch_loss=3.455117\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:09 INFO 140149161203520] #quality_metric: host=algo-1, epoch=159, batch=0 train loss <loss>=3.45511698723\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:10 INFO 140149161203520] Epoch[159] Batch[5] avg_epoch_loss=3.312071\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:10 INFO 140149161203520] #quality_metric: host=algo-1, epoch=159, batch=5 train loss <loss>=3.31207124392\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:10 INFO 140149161203520] Epoch[159] Batch [5]#011Speed: 320.98 samples/sec#011loss=3.312071\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:10 INFO 140149161203520] processed a total of 596 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2234.2288494110107, \"sum\": 2234.2288494110107, \"min\": 2234.2288494110107}}, \"EndTime\": 1588627210.816956, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627208.582656}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:10 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=266.741856457 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:10 INFO 140149161203520] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:10 INFO 140149161203520] #quality_metric: host=algo-1, epoch=159, train loss <loss>=3.25657908916\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:10 INFO 140149161203520] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:10 INFO 140149161203520] Saved checkpoint to \"/opt/ml/model/state_e8d336df-ae17-48cc-9989-97079aa54b26-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 63.96818161010742, \"sum\": 63.96818161010742, \"min\": 63.96818161010742}}, \"EndTime\": 1588627210.881591, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627210.817056}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:11 INFO 140149161203520] Epoch[160] Batch[0] avg_epoch_loss=3.507303\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:11 INFO 140149161203520] #quality_metric: host=algo-1, epoch=160, batch=0 train loss <loss>=3.50730276108\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:12 INFO 140149161203520] Epoch[160] Batch[5] avg_epoch_loss=3.384970\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:12 INFO 140149161203520] #quality_metric: host=algo-1, epoch=160, batch=5 train loss <loss>=3.38496955236\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:12 INFO 140149161203520] Epoch[160] Batch [5]#011Speed: 323.97 samples/sec#011loss=3.384970\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:13 INFO 140149161203520] Epoch[160] Batch[10] avg_epoch_loss=3.256392\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:13 INFO 140149161203520] #quality_metric: host=algo-1, epoch=160, batch=10 train loss <loss>=3.10209856033\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:13 INFO 140149161203520] Epoch[160] Batch [10]#011Speed: 325.24 samples/sec#011loss=3.102099\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:13 INFO 140149161203520] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2417.9158210754395, \"sum\": 2417.9158210754395, \"min\": 2417.9158210754395}}, \"EndTime\": 1588627213.299663, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627210.881678}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:13 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=268.812302727 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:13 INFO 140149161203520] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:13 INFO 140149161203520] #quality_metric: host=algo-1, epoch=160, train loss <loss>=3.25639182871\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:13 INFO 140149161203520] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:13 INFO 140149161203520] Saved checkpoint to \"/opt/ml/model/state_1b5ef402-bf55-4526-8f63-b8335f939ad4-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 68.56107711791992, \"sum\": 68.56107711791992, \"min\": 68.56107711791992}}, \"EndTime\": 1588627213.36881, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627213.299751}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:13 INFO 140149161203520] Epoch[161] Batch[0] avg_epoch_loss=3.355828\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:13 INFO 140149161203520] #quality_metric: host=algo-1, epoch=161, batch=0 train loss <loss>=3.35582756996\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:14 INFO 140149161203520] Epoch[161] Batch[5] avg_epoch_loss=3.352962\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:14 INFO 140149161203520] #quality_metric: host=algo-1, epoch=161, batch=5 train loss <loss>=3.35296209653\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:14 INFO 140149161203520] Epoch[161] Batch [5]#011Speed: 315.69 samples/sec#011loss=3.352962\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:15 INFO 140149161203520] processed a total of 621 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2262.294054031372, \"sum\": 2262.294054031372, \"min\": 2262.294054031372}}, \"EndTime\": 1588627215.63126, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627213.368894}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:15 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=274.48450395 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:15 INFO 140149161203520] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:15 INFO 140149161203520] #quality_metric: host=algo-1, epoch=161, train loss <loss>=3.28643655777\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:15 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:16 INFO 140149161203520] Epoch[162] Batch[0] avg_epoch_loss=3.300388\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:16 INFO 140149161203520] #quality_metric: host=algo-1, epoch=162, batch=0 train loss <loss>=3.30038762093\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:17 INFO 140149161203520] Epoch[162] Batch[5] avg_epoch_loss=3.415717\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:17 INFO 140149161203520] #quality_metric: host=algo-1, epoch=162, batch=5 train loss <loss>=3.41571652889\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:17 INFO 140149161203520] Epoch[162] Batch [5]#011Speed: 319.03 samples/sec#011loss=3.415717\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:17 INFO 140149161203520] processed a total of 598 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2229.383945465088, \"sum\": 2229.383945465088, \"min\": 2229.383945465088}}, \"EndTime\": 1588627217.861266, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627215.631346}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:17 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=268.218928869 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:17 INFO 140149161203520] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:17 INFO 140149161203520] #quality_metric: host=algo-1, epoch=162, train loss <loss>=3.44068129063\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:17 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:18 INFO 140149161203520] Epoch[163] Batch[0] avg_epoch_loss=3.610035\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:18 INFO 140149161203520] #quality_metric: host=algo-1, epoch=163, batch=0 train loss <loss>=3.61003518105\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:19 INFO 140149161203520] Epoch[163] Batch[5] avg_epoch_loss=3.431037\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:19 INFO 140149161203520] #quality_metric: host=algo-1, epoch=163, batch=5 train loss <loss>=3.43103714784\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:19 INFO 140149161203520] Epoch[163] Batch [5]#011Speed: 324.96 samples/sec#011loss=3.431037\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:20 INFO 140149161203520] Epoch[163] Batch[10] avg_epoch_loss=3.403962\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:20 INFO 140149161203520] #quality_metric: host=algo-1, epoch=163, batch=10 train loss <loss>=3.3714720726\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:20 INFO 140149161203520] Epoch[163] Batch [10]#011Speed: 319.70 samples/sec#011loss=3.371472\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:20 INFO 140149161203520] processed a total of 727 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2671.082019805908, \"sum\": 2671.082019805908, \"min\": 2671.082019805908}}, \"EndTime\": 1588627220.533, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627217.861362}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:20 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=272.16037694 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:20 INFO 140149161203520] #progress_metric: host=algo-1, completed 41 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:20 INFO 140149161203520] #quality_metric: host=algo-1, epoch=163, train loss <loss>=3.3617100517\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:20 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:21 INFO 140149161203520] Epoch[164] Batch[0] avg_epoch_loss=3.311528\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:21 INFO 140149161203520] #quality_metric: host=algo-1, epoch=164, batch=0 train loss <loss>=3.31152820587\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:22 INFO 140149161203520] Epoch[164] Batch[5] avg_epoch_loss=3.350888\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:22 INFO 140149161203520] #quality_metric: host=algo-1, epoch=164, batch=5 train loss <loss>=3.35088777542\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:22 INFO 140149161203520] Epoch[164] Batch [5]#011Speed: 323.78 samples/sec#011loss=3.350888\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:22 INFO 140149161203520] processed a total of 616 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2308.488130569458, \"sum\": 2308.488130569458, \"min\": 2308.488130569458}}, \"EndTime\": 1588627222.842067, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627220.533095}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:22 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=266.825493348 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:22 INFO 140149161203520] #progress_metric: host=algo-1, completed 41 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:22 INFO 140149161203520] #quality_metric: host=algo-1, epoch=164, train loss <loss>=3.30582065582\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:22 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:23 INFO 140149161203520] Epoch[165] Batch[0] avg_epoch_loss=3.437277\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:23 INFO 140149161203520] #quality_metric: host=algo-1, epoch=165, batch=0 train loss <loss>=3.43727707863\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:24 INFO 140149161203520] Epoch[165] Batch[5] avg_epoch_loss=3.287751\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:24 INFO 140149161203520] #quality_metric: host=algo-1, epoch=165, batch=5 train loss <loss>=3.28775135676\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:24 INFO 140149161203520] Epoch[165] Batch [5]#011Speed: 327.12 samples/sec#011loss=3.287751\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:25 INFO 140149161203520] Epoch[165] Batch[10] avg_epoch_loss=3.397550\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:25 INFO 140149161203520] #quality_metric: host=algo-1, epoch=165, batch=10 train loss <loss>=3.52930865288\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:25 INFO 140149161203520] Epoch[165] Batch [10]#011Speed: 314.65 samples/sec#011loss=3.529309\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:25 INFO 140149161203520] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2474.261999130249, \"sum\": 2474.261999130249, \"min\": 2474.261999130249}}, \"EndTime\": 1588627225.3169, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627222.842163}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:25 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=266.732403551 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:25 INFO 140149161203520] #progress_metric: host=algo-1, completed 41 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:25 INFO 140149161203520] #quality_metric: host=algo-1, epoch=165, train loss <loss>=3.39755012772\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:25 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:25 INFO 140149161203520] Epoch[166] Batch[0] avg_epoch_loss=3.476460\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:25 INFO 140149161203520] #quality_metric: host=algo-1, epoch=166, batch=0 train loss <loss>=3.47646021843\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:26 INFO 140149161203520] Epoch[166] Batch[5] avg_epoch_loss=3.405612\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:26 INFO 140149161203520] #quality_metric: host=algo-1, epoch=166, batch=5 train loss <loss>=3.40561159452\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:26 INFO 140149161203520] Epoch[166] Batch [5]#011Speed: 323.28 samples/sec#011loss=3.405612\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:27 INFO 140149161203520] Epoch[166] Batch[10] avg_epoch_loss=3.420015\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:27 INFO 140149161203520] #quality_metric: host=algo-1, epoch=166, batch=10 train loss <loss>=3.4373000145\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:27 INFO 140149161203520] Epoch[166] Batch [10]#011Speed: 321.38 samples/sec#011loss=3.437300\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:27 INFO 140149161203520] processed a total of 671 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2441.4010047912598, \"sum\": 2441.4010047912598, \"min\": 2441.4010047912598}}, \"EndTime\": 1588627227.75885, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627225.316987}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:27 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=274.829683557 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:27 INFO 140149161203520] #progress_metric: host=algo-1, completed 41 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:27 INFO 140149161203520] #quality_metric: host=algo-1, epoch=166, train loss <loss>=3.42001542178\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:27 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:28 INFO 140149161203520] Epoch[167] Batch[0] avg_epoch_loss=3.366411\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:28 INFO 140149161203520] #quality_metric: host=algo-1, epoch=167, batch=0 train loss <loss>=3.36641120911\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:29 INFO 140149161203520] Epoch[167] Batch[5] avg_epoch_loss=3.442497\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:29 INFO 140149161203520] #quality_metric: host=algo-1, epoch=167, batch=5 train loss <loss>=3.442497015\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:29 INFO 140149161203520] Epoch[167] Batch [5]#011Speed: 321.04 samples/sec#011loss=3.442497\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:30 INFO 140149161203520] processed a total of 584 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2243.499994277954, \"sum\": 2243.499994277954, \"min\": 2243.499994277954}}, \"EndTime\": 1588627230.002896, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627227.758923}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:30 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=260.294001612 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:30 INFO 140149161203520] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:30 INFO 140149161203520] #quality_metric: host=algo-1, epoch=167, train loss <loss>=3.32073814869\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:30 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:30 INFO 140149161203520] Epoch[168] Batch[0] avg_epoch_loss=3.559490\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:30 INFO 140149161203520] #quality_metric: host=algo-1, epoch=168, batch=0 train loss <loss>=3.55948972702\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:31 INFO 140149161203520] Epoch[168] Batch[5] avg_epoch_loss=3.347024\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:31 INFO 140149161203520] #quality_metric: host=algo-1, epoch=168, batch=5 train loss <loss>=3.34702364604\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:31 INFO 140149161203520] Epoch[168] Batch [5]#011Speed: 320.10 samples/sec#011loss=3.347024\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:32 INFO 140149161203520] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2236.170768737793, \"sum\": 2236.170768737793, \"min\": 2236.170768737793}}, \"EndTime\": 1588627232.239729, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627230.002971}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:32 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=279.033193944 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:32 INFO 140149161203520] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:32 INFO 140149161203520] #quality_metric: host=algo-1, epoch=168, train loss <loss>=3.39292516708\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:32 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:32 INFO 140149161203520] Epoch[169] Batch[0] avg_epoch_loss=3.329047\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:32 INFO 140149161203520] #quality_metric: host=algo-1, epoch=169, batch=0 train loss <loss>=3.32904672623\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:33 INFO 140149161203520] Epoch[169] Batch[5] avg_epoch_loss=3.361696\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:33 INFO 140149161203520] #quality_metric: host=algo-1, epoch=169, batch=5 train loss <loss>=3.36169576645\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:33 INFO 140149161203520] Epoch[169] Batch [5]#011Speed: 319.58 samples/sec#011loss=3.361696\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:34 INFO 140149161203520] Epoch[169] Batch[10] avg_epoch_loss=3.432016\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:34 INFO 140149161203520] #quality_metric: host=algo-1, epoch=169, batch=10 train loss <loss>=3.51640014648\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:34 INFO 140149161203520] Epoch[169] Batch [10]#011Speed: 313.09 samples/sec#011loss=3.516400\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:34 INFO 140149161203520] processed a total of 664 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2473.6311435699463, \"sum\": 2473.6311435699463, \"min\": 2473.6311435699463}}, \"EndTime\": 1588627234.713955, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627232.239807}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:34 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=268.417087216 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:34 INFO 140149161203520] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:34 INFO 140149161203520] #quality_metric: host=algo-1, epoch=169, train loss <loss>=3.43201593919\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:34 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:35 INFO 140149161203520] Epoch[170] Batch[0] avg_epoch_loss=3.317994\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:35 INFO 140149161203520] #quality_metric: host=algo-1, epoch=170, batch=0 train loss <loss>=3.31799411774\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:36 INFO 140149161203520] Epoch[170] Batch[5] avg_epoch_loss=3.402274\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:36 INFO 140149161203520] #quality_metric: host=algo-1, epoch=170, batch=5 train loss <loss>=3.40227433046\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:36 INFO 140149161203520] Epoch[170] Batch [5]#011Speed: 312.92 samples/sec#011loss=3.402274\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:37 INFO 140149161203520] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2292.311906814575, \"sum\": 2292.311906814575, \"min\": 2292.311906814575}}, \"EndTime\": 1588627237.006812, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627234.714045}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:37 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=272.634569274 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:37 INFO 140149161203520] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:37 INFO 140149161203520] #quality_metric: host=algo-1, epoch=170, train loss <loss>=3.3885591507\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:37 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:37 INFO 140149161203520] Epoch[171] Batch[0] avg_epoch_loss=3.400203\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:37 INFO 140149161203520] #quality_metric: host=algo-1, epoch=171, batch=0 train loss <loss>=3.40020251274\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:38 INFO 140149161203520] Epoch[171] Batch[5] avg_epoch_loss=3.410254\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:38 INFO 140149161203520] #quality_metric: host=algo-1, epoch=171, batch=5 train loss <loss>=3.41025356452\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:38 INFO 140149161203520] Epoch[171] Batch [5]#011Speed: 321.92 samples/sec#011loss=3.410254\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:39 INFO 140149161203520] Epoch[171] Batch[10] avg_epoch_loss=3.334233\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:39 INFO 140149161203520] #quality_metric: host=algo-1, epoch=171, batch=10 train loss <loss>=3.24300880432\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:39 INFO 140149161203520] Epoch[171] Batch [10]#011Speed: 321.15 samples/sec#011loss=3.243009\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:39 INFO 140149161203520] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2448.6441612243652, \"sum\": 2448.6441612243652, \"min\": 2448.6441612243652}}, \"EndTime\": 1588627239.456046, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627237.006905}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:39 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=269.522956628 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:39 INFO 140149161203520] #progress_metric: host=algo-1, completed 43 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:39 INFO 140149161203520] #quality_metric: host=algo-1, epoch=171, train loss <loss>=3.33423321897\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:39 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:39 INFO 140149161203520] Epoch[172] Batch[0] avg_epoch_loss=3.469552\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:39 INFO 140149161203520] #quality_metric: host=algo-1, epoch=172, batch=0 train loss <loss>=3.4695520401\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:40 INFO 140149161203520] Epoch[172] Batch[5] avg_epoch_loss=3.330802\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:40 INFO 140149161203520] #quality_metric: host=algo-1, epoch=172, batch=5 train loss <loss>=3.33080176512\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:40 INFO 140149161203520] Epoch[172] Batch [5]#011Speed: 321.43 samples/sec#011loss=3.330802\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:41 INFO 140149161203520] Epoch[172] Batch[10] avg_epoch_loss=3.309008\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:41 INFO 140149161203520] #quality_metric: host=algo-1, epoch=172, batch=10 train loss <loss>=3.28285622597\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:41 INFO 140149161203520] Epoch[172] Batch [10]#011Speed: 318.71 samples/sec#011loss=3.282856\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:41 INFO 140149161203520] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2458.7550163269043, \"sum\": 2458.7550163269043, \"min\": 2458.7550163269043}}, \"EndTime\": 1588627241.915369, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627239.456131}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:41 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=265.567193661 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:41 INFO 140149161203520] #progress_metric: host=algo-1, completed 43 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:41 INFO 140149161203520] #quality_metric: host=algo-1, epoch=172, train loss <loss>=3.30900833823\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:41 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:42 INFO 140149161203520] Epoch[173] Batch[0] avg_epoch_loss=3.375921\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:42 INFO 140149161203520] #quality_metric: host=algo-1, epoch=173, batch=0 train loss <loss>=3.37592053413\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:43 INFO 140149161203520] Epoch[173] Batch[5] avg_epoch_loss=3.407013\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:43 INFO 140149161203520] #quality_metric: host=algo-1, epoch=173, batch=5 train loss <loss>=3.40701313814\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:43 INFO 140149161203520] Epoch[173] Batch [5]#011Speed: 325.34 samples/sec#011loss=3.407013\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:44 INFO 140149161203520] processed a total of 627 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2223.843812942505, \"sum\": 2223.843812942505, \"min\": 2223.843812942505}}, \"EndTime\": 1588627244.139755, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627241.91546}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:44 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=281.926730079 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:44 INFO 140149161203520] #progress_metric: host=algo-1, completed 43 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:44 INFO 140149161203520] #quality_metric: host=algo-1, epoch=173, train loss <loss>=3.34350941181\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:44 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:44 INFO 140149161203520] Epoch[174] Batch[0] avg_epoch_loss=3.404269\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:44 INFO 140149161203520] #quality_metric: host=algo-1, epoch=174, batch=0 train loss <loss>=3.40426874161\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:45 INFO 140149161203520] Epoch[174] Batch[5] avg_epoch_loss=3.333155\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:45 INFO 140149161203520] #quality_metric: host=algo-1, epoch=174, batch=5 train loss <loss>=3.33315507571\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:45 INFO 140149161203520] Epoch[174] Batch [5]#011Speed: 320.00 samples/sec#011loss=3.333155\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:46 INFO 140149161203520] Epoch[174] Batch[10] avg_epoch_loss=3.313464\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:46 INFO 140149161203520] #quality_metric: host=algo-1, epoch=174, batch=10 train loss <loss>=3.28983540535\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:46 INFO 140149161203520] Epoch[174] Batch [10]#011Speed: 320.03 samples/sec#011loss=3.289835\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:46 INFO 140149161203520] processed a total of 675 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2468.881845474243, \"sum\": 2468.881845474243, \"min\": 2468.881845474243}}, \"EndTime\": 1588627246.609224, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627244.139851}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:46 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=273.389022067 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:46 INFO 140149161203520] #progress_metric: host=algo-1, completed 43 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:46 INFO 140149161203520] #quality_metric: host=algo-1, epoch=174, train loss <loss>=3.31346431645\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:46 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:47 INFO 140149161203520] Epoch[175] Batch[0] avg_epoch_loss=3.245672\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:47 INFO 140149161203520] #quality_metric: host=algo-1, epoch=175, batch=0 train loss <loss>=3.24567246437\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:48 INFO 140149161203520] Epoch[175] Batch[5] avg_epoch_loss=3.358752\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:48 INFO 140149161203520] #quality_metric: host=algo-1, epoch=175, batch=5 train loss <loss>=3.35875177383\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:48 INFO 140149161203520] Epoch[175] Batch [5]#011Speed: 322.47 samples/sec#011loss=3.358752\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:49 INFO 140149161203520] Epoch[175] Batch[10] avg_epoch_loss=3.378205\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:49 INFO 140149161203520] #quality_metric: host=algo-1, epoch=175, batch=10 train loss <loss>=3.40154976845\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:49 INFO 140149161203520] Epoch[175] Batch [10]#011Speed: 321.88 samples/sec#011loss=3.401550\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:49 INFO 140149161203520] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2465.6848907470703, \"sum\": 2465.6848907470703, \"min\": 2465.6848907470703}}, \"EndTime\": 1588627249.075427, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627246.609311}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:49 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=264.010194179 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:49 INFO 140149161203520] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:49 INFO 140149161203520] #quality_metric: host=algo-1, epoch=175, train loss <loss>=3.37820540775\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:49 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:49 INFO 140149161203520] Epoch[176] Batch[0] avg_epoch_loss=3.170247\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:49 INFO 140149161203520] #quality_metric: host=algo-1, epoch=176, batch=0 train loss <loss>=3.1702466011\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:50 INFO 140149161203520] Epoch[176] Batch[5] avg_epoch_loss=3.366575\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:50 INFO 140149161203520] #quality_metric: host=algo-1, epoch=176, batch=5 train loss <loss>=3.36657504241\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:50 INFO 140149161203520] Epoch[176] Batch [5]#011Speed: 320.69 samples/sec#011loss=3.366575\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:51 INFO 140149161203520] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2308.4700107574463, \"sum\": 2308.4700107574463, \"min\": 2308.4700107574463}}, \"EndTime\": 1588627251.384436, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627249.075516}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:51 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=274.624384192 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:51 INFO 140149161203520] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:51 INFO 140149161203520] #quality_metric: host=algo-1, epoch=176, train loss <loss>=3.33316206932\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:51 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:51 INFO 140149161203520] Epoch[177] Batch[0] avg_epoch_loss=3.221802\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:51 INFO 140149161203520] #quality_metric: host=algo-1, epoch=177, batch=0 train loss <loss>=3.22180175781\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:52 INFO 140149161203520] Epoch[177] Batch[5] avg_epoch_loss=3.299846\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:52 INFO 140149161203520] #quality_metric: host=algo-1, epoch=177, batch=5 train loss <loss>=3.29984633128\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:52 INFO 140149161203520] Epoch[177] Batch [5]#011Speed: 323.17 samples/sec#011loss=3.299846\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:53 INFO 140149161203520] processed a total of 618 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2236.5009784698486, \"sum\": 2236.5009784698486, \"min\": 2236.5009784698486}}, \"EndTime\": 1588627253.621517, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627251.384532}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:53 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=276.30746829 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:53 INFO 140149161203520] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:53 INFO 140149161203520] #quality_metric: host=algo-1, epoch=177, train loss <loss>=3.3601336956\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:53 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:54 INFO 140149161203520] Epoch[178] Batch[0] avg_epoch_loss=3.206197\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:54 INFO 140149161203520] #quality_metric: host=algo-1, epoch=178, batch=0 train loss <loss>=3.20619726181\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:55 INFO 140149161203520] Epoch[178] Batch[5] avg_epoch_loss=3.359215\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:55 INFO 140149161203520] #quality_metric: host=algo-1, epoch=178, batch=5 train loss <loss>=3.35921482245\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:55 INFO 140149161203520] Epoch[178] Batch [5]#011Speed: 319.13 samples/sec#011loss=3.359215\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:55 INFO 140149161203520] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2274.6479511260986, \"sum\": 2274.6479511260986, \"min\": 2274.6479511260986}}, \"EndTime\": 1588627255.896788, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627253.621614}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:55 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=280.905397872 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:55 INFO 140149161203520] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:55 INFO 140149161203520] #quality_metric: host=algo-1, epoch=178, train loss <loss>=3.35576176643\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:55 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:56 INFO 140149161203520] Epoch[179] Batch[0] avg_epoch_loss=3.159141\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:56 INFO 140149161203520] #quality_metric: host=algo-1, epoch=179, batch=0 train loss <loss>=3.15914082527\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:57 INFO 140149161203520] Epoch[179] Batch[5] avg_epoch_loss=3.303700\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:57 INFO 140149161203520] #quality_metric: host=algo-1, epoch=179, batch=5 train loss <loss>=3.30370008945\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:57 INFO 140149161203520] Epoch[179] Batch [5]#011Speed: 322.13 samples/sec#011loss=3.303700\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:58 INFO 140149161203520] processed a total of 631 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2240.64302444458, \"sum\": 2240.64302444458, \"min\": 2240.64302444458}}, \"EndTime\": 1588627258.138045, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627255.896885}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:58 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=281.597878136 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:58 INFO 140149161203520] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:58 INFO 140149161203520] #quality_metric: host=algo-1, epoch=179, train loss <loss>=3.30929493904\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:58 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:58 INFO 140149161203520] Epoch[180] Batch[0] avg_epoch_loss=3.526356\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:58 INFO 140149161203520] #quality_metric: host=algo-1, epoch=180, batch=0 train loss <loss>=3.52635598183\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:59 INFO 140149161203520] Epoch[180] Batch[5] avg_epoch_loss=3.359237\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:59 INFO 140149161203520] #quality_metric: host=algo-1, epoch=180, batch=5 train loss <loss>=3.35923719406\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:20:59 INFO 140149161203520] Epoch[180] Batch [5]#011Speed: 327.72 samples/sec#011loss=3.359237\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:00 INFO 140149161203520] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2244.9140548706055, \"sum\": 2244.9140548706055, \"min\": 2244.9140548706055}}, \"EndTime\": 1588627260.383549, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627258.138142}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:00 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=278.835548009 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:00 INFO 140149161203520] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:00 INFO 140149161203520] #quality_metric: host=algo-1, epoch=180, train loss <loss>=3.40161004066\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:00 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:00 INFO 140149161203520] Epoch[181] Batch[0] avg_epoch_loss=3.351567\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:00 INFO 140149161203520] #quality_metric: host=algo-1, epoch=181, batch=0 train loss <loss>=3.35156655312\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:01 INFO 140149161203520] Epoch[181] Batch[5] avg_epoch_loss=3.284630\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:01 INFO 140149161203520] #quality_metric: host=algo-1, epoch=181, batch=5 train loss <loss>=3.28463033835\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:01 INFO 140149161203520] Epoch[181] Batch [5]#011Speed: 308.23 samples/sec#011loss=3.284630\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:02 INFO 140149161203520] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2307.199001312256, \"sum\": 2307.199001312256, \"min\": 2307.199001312256}}, \"EndTime\": 1588627262.691336, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627260.383645}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:02 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=274.778359914 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:02 INFO 140149161203520] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:02 INFO 140149161203520] #quality_metric: host=algo-1, epoch=181, train loss <loss>=3.30280058384\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:02 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:03 INFO 140149161203520] Epoch[182] Batch[0] avg_epoch_loss=3.234715\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:03 INFO 140149161203520] #quality_metric: host=algo-1, epoch=182, batch=0 train loss <loss>=3.23471546173\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:04 INFO 140149161203520] Epoch[182] Batch[5] avg_epoch_loss=3.325794\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:04 INFO 140149161203520] #quality_metric: host=algo-1, epoch=182, batch=5 train loss <loss>=3.32579394182\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:04 INFO 140149161203520] Epoch[182] Batch [5]#011Speed: 322.89 samples/sec#011loss=3.325794\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:05 INFO 140149161203520] Epoch[182] Batch[10] avg_epoch_loss=3.269522\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:05 INFO 140149161203520] #quality_metric: host=algo-1, epoch=182, batch=10 train loss <loss>=3.20199666023\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:05 INFO 140149161203520] Epoch[182] Batch [10]#011Speed: 317.24 samples/sec#011loss=3.201997\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:05 INFO 140149161203520] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2450.005054473877, \"sum\": 2450.005054473877, \"min\": 2450.005054473877}}, \"EndTime\": 1588627265.141968, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627262.691409}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:05 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=264.06829053 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:05 INFO 140149161203520] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:05 INFO 140149161203520] #quality_metric: host=algo-1, epoch=182, train loss <loss>=3.26952245019\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:05 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:05 INFO 140149161203520] Epoch[183] Batch[0] avg_epoch_loss=3.264806\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:05 INFO 140149161203520] #quality_metric: host=algo-1, epoch=183, batch=0 train loss <loss>=3.2648062706\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:06 INFO 140149161203520] Epoch[183] Batch[5] avg_epoch_loss=3.315540\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:06 INFO 140149161203520] #quality_metric: host=algo-1, epoch=183, batch=5 train loss <loss>=3.31554047267\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:06 INFO 140149161203520] Epoch[183] Batch [5]#011Speed: 311.25 samples/sec#011loss=3.315540\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:07 INFO 140149161203520] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2269.601821899414, \"sum\": 2269.601821899414, \"min\": 2269.601821899414}}, \"EndTime\": 1588627267.412144, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627265.142046}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:07 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=279.769330116 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:07 INFO 140149161203520] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:07 INFO 140149161203520] #quality_metric: host=algo-1, epoch=183, train loss <loss>=3.30206735134\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:07 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:07 INFO 140149161203520] Epoch[184] Batch[0] avg_epoch_loss=3.487205\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:07 INFO 140149161203520] #quality_metric: host=algo-1, epoch=184, batch=0 train loss <loss>=3.48720479012\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:08 INFO 140149161203520] Epoch[184] Batch[5] avg_epoch_loss=3.323908\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:08 INFO 140149161203520] #quality_metric: host=algo-1, epoch=184, batch=5 train loss <loss>=3.32390769323\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:08 INFO 140149161203520] Epoch[184] Batch [5]#011Speed: 320.69 samples/sec#011loss=3.323908\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:09 INFO 140149161203520] Epoch[184] Batch[10] avg_epoch_loss=3.409989\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:09 INFO 140149161203520] #quality_metric: host=algo-1, epoch=184, batch=10 train loss <loss>=3.51328759193\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:09 INFO 140149161203520] Epoch[184] Batch [10]#011Speed: 321.43 samples/sec#011loss=3.513288\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:09 INFO 140149161203520] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2442.5570964813232, \"sum\": 2442.5570964813232, \"min\": 2442.5570964813232}}, \"EndTime\": 1588627269.855275, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627267.412225}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:09 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=265.283598639 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:09 INFO 140149161203520] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:09 INFO 140149161203520] #quality_metric: host=algo-1, epoch=184, train loss <loss>=3.40998946537\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:09 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:10 INFO 140149161203520] Epoch[185] Batch[0] avg_epoch_loss=3.230659\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:10 INFO 140149161203520] #quality_metric: host=algo-1, epoch=185, batch=0 train loss <loss>=3.23065876961\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:11 INFO 140149161203520] Epoch[185] Batch[5] avg_epoch_loss=3.319052\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:11 INFO 140149161203520] #quality_metric: host=algo-1, epoch=185, batch=5 train loss <loss>=3.31905178229\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:11 INFO 140149161203520] Epoch[185] Batch [5]#011Speed: 321.79 samples/sec#011loss=3.319052\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:12 INFO 140149161203520] processed a total of 596 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2249.206066131592, \"sum\": 2249.206066131592, \"min\": 2249.206066131592}}, \"EndTime\": 1588627272.105039, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627269.855346}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:12 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=264.966268649 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:12 INFO 140149161203520] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:12 INFO 140149161203520] #quality_metric: host=algo-1, epoch=185, train loss <loss>=3.25431461334\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:12 INFO 140149161203520] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:12 INFO 140149161203520] Saved checkpoint to \"/opt/ml/model/state_591e1d2c-47e4-4350-b95d-ec27625408a7-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 59.187889099121094, \"sum\": 59.187889099121094, \"min\": 59.187889099121094}}, \"EndTime\": 1588627272.165037, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627272.105131}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:12 INFO 140149161203520] Epoch[186] Batch[0] avg_epoch_loss=3.222657\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:12 INFO 140149161203520] #quality_metric: host=algo-1, epoch=186, batch=0 train loss <loss>=3.22265744209\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:13 INFO 140149161203520] Epoch[186] Batch[5] avg_epoch_loss=3.257475\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:13 INFO 140149161203520] #quality_metric: host=algo-1, epoch=186, batch=5 train loss <loss>=3.2574745814\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:13 INFO 140149161203520] Epoch[186] Batch [5]#011Speed: 321.48 samples/sec#011loss=3.257475\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:14 INFO 140149161203520] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2239.5148277282715, \"sum\": 2239.5148277282715, \"min\": 2239.5148277282715}}, \"EndTime\": 1588627274.404693, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627272.165112}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:14 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=282.186466824 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:14 INFO 140149161203520] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:14 INFO 140149161203520] #quality_metric: host=algo-1, epoch=186, train loss <loss>=3.26698260307\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:14 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:14 INFO 140149161203520] Epoch[187] Batch[0] avg_epoch_loss=3.338597\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:14 INFO 140149161203520] #quality_metric: host=algo-1, epoch=187, batch=0 train loss <loss>=3.33859658241\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:15 INFO 140149161203520] Epoch[187] Batch[5] avg_epoch_loss=3.270897\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:15 INFO 140149161203520] #quality_metric: host=algo-1, epoch=187, batch=5 train loss <loss>=3.27089675268\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:15 INFO 140149161203520] Epoch[187] Batch [5]#011Speed: 315.77 samples/sec#011loss=3.270897\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:16 INFO 140149161203520] processed a total of 627 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2266.965866088867, \"sum\": 2266.965866088867, \"min\": 2266.965866088867}}, \"EndTime\": 1588627276.672261, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627274.404789}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:16 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=276.56426826 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:16 INFO 140149161203520] #progress_metric: host=algo-1, completed 47 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:16 INFO 140149161203520] #quality_metric: host=algo-1, epoch=187, train loss <loss>=3.30404961109\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:16 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:17 INFO 140149161203520] Epoch[188] Batch[0] avg_epoch_loss=3.348073\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:17 INFO 140149161203520] #quality_metric: host=algo-1, epoch=188, batch=0 train loss <loss>=3.34807276726\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:18 INFO 140149161203520] Epoch[188] Batch[5] avg_epoch_loss=3.283449\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:18 INFO 140149161203520] #quality_metric: host=algo-1, epoch=188, batch=5 train loss <loss>=3.28344881535\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:18 INFO 140149161203520] Epoch[188] Batch [5]#011Speed: 319.85 samples/sec#011loss=3.283449\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:18 INFO 140149161203520] processed a total of 619 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2242.957830429077, \"sum\": 2242.957830429077, \"min\": 2242.957830429077}}, \"EndTime\": 1588627278.915849, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627276.672356}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:18 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=275.959589157 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:18 INFO 140149161203520] #progress_metric: host=algo-1, completed 47 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:18 INFO 140149161203520] #quality_metric: host=algo-1, epoch=188, train loss <loss>=3.3053984642\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:18 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:19 INFO 140149161203520] Epoch[189] Batch[0] avg_epoch_loss=3.368478\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:19 INFO 140149161203520] #quality_metric: host=algo-1, epoch=189, batch=0 train loss <loss>=3.36847758293\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:20 INFO 140149161203520] Epoch[189] Batch[5] avg_epoch_loss=3.322357\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:20 INFO 140149161203520] #quality_metric: host=algo-1, epoch=189, batch=5 train loss <loss>=3.32235682011\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:20 INFO 140149161203520] Epoch[189] Batch [5]#011Speed: 318.35 samples/sec#011loss=3.322357\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:21 INFO 140149161203520] Epoch[189] Batch[10] avg_epoch_loss=3.392416\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:21 INFO 140149161203520] #quality_metric: host=algo-1, epoch=189, batch=10 train loss <loss>=3.47648673058\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:21 INFO 140149161203520] Epoch[189] Batch [10]#011Speed: 321.44 samples/sec#011loss=3.476487\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:21 INFO 140149161203520] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2448.2429027557373, \"sum\": 2448.2429027557373, \"min\": 2448.2429027557373}}, \"EndTime\": 1588627281.364664, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627278.915928}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:21 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=262.214837829 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:21 INFO 140149161203520] #progress_metric: host=algo-1, completed 47 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:21 INFO 140149161203520] #quality_metric: host=algo-1, epoch=189, train loss <loss>=3.39241587032\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:21 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:21 INFO 140149161203520] Epoch[190] Batch[0] avg_epoch_loss=3.175827\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:21 INFO 140149161203520] #quality_metric: host=algo-1, epoch=190, batch=0 train loss <loss>=3.17582654953\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:22 INFO 140149161203520] Epoch[190] Batch[5] avg_epoch_loss=3.270499\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:22 INFO 140149161203520] #quality_metric: host=algo-1, epoch=190, batch=5 train loss <loss>=3.27049914996\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:22 INFO 140149161203520] Epoch[190] Batch [5]#011Speed: 320.74 samples/sec#011loss=3.270499\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:23 INFO 140149161203520] Epoch[190] Batch[10] avg_epoch_loss=3.217797\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:23 INFO 140149161203520] #quality_metric: host=algo-1, epoch=190, batch=10 train loss <loss>=3.15455393791\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:23 INFO 140149161203520] Epoch[190] Batch [10]#011Speed: 320.10 samples/sec#011loss=3.154554\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:23 INFO 140149161203520] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2458.8921070098877, \"sum\": 2458.8921070098877, \"min\": 2458.8921070098877}}, \"EndTime\": 1588627283.824084, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627281.364754}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:23 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=263.518857791 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:23 INFO 140149161203520] #progress_metric: host=algo-1, completed 47 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:23 INFO 140149161203520] #quality_metric: host=algo-1, epoch=190, train loss <loss>=3.21779678085\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:23 INFO 140149161203520] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:23 INFO 140149161203520] Saved checkpoint to \"/opt/ml/model/state_a592ae91-4b81-4f41-be85-c8924e937f67-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 60.16707420349121, \"sum\": 60.16707420349121, \"min\": 60.16707420349121}}, \"EndTime\": 1588627283.884865, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627283.824176}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:24 INFO 140149161203520] Epoch[191] Batch[0] avg_epoch_loss=3.485460\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:24 INFO 140149161203520] #quality_metric: host=algo-1, epoch=191, batch=0 train loss <loss>=3.48545956612\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:25 INFO 140149161203520] Epoch[191] Batch[5] avg_epoch_loss=3.324932\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:25 INFO 140149161203520] #quality_metric: host=algo-1, epoch=191, batch=5 train loss <loss>=3.32493233681\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:25 INFO 140149161203520] Epoch[191] Batch [5]#011Speed: 317.19 samples/sec#011loss=3.324932\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:26 INFO 140149161203520] Epoch[191] Batch[10] avg_epoch_loss=3.369870\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:26 INFO 140149161203520] #quality_metric: host=algo-1, epoch=191, batch=10 train loss <loss>=3.42379593849\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:26 INFO 140149161203520] Epoch[191] Batch [10]#011Speed: 318.60 samples/sec#011loss=3.423796\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:26 INFO 140149161203520] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2462.137222290039, \"sum\": 2462.137222290039, \"min\": 2462.137222290039}}, \"EndTime\": 1588627286.347162, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627283.884952}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:26 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=267.639208642 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:26 INFO 140149161203520] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:26 INFO 140149161203520] #quality_metric: host=algo-1, epoch=191, train loss <loss>=3.36987033757\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:26 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:26 INFO 140149161203520] Epoch[192] Batch[0] avg_epoch_loss=3.378141\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:26 INFO 140149161203520] #quality_metric: host=algo-1, epoch=192, batch=0 train loss <loss>=3.37814068794\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:27 INFO 140149161203520] Epoch[192] Batch[5] avg_epoch_loss=3.221850\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:27 INFO 140149161203520] #quality_metric: host=algo-1, epoch=192, batch=5 train loss <loss>=3.22185031573\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:27 INFO 140149161203520] Epoch[192] Batch [5]#011Speed: 323.58 samples/sec#011loss=3.221850\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:28 INFO 140149161203520] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2308.3951473236084, \"sum\": 2308.3951473236084, \"min\": 2308.3951473236084}}, \"EndTime\": 1588627288.656108, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627286.347253}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:28 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=272.46715905 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:28 INFO 140149161203520] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:28 INFO 140149161203520] #quality_metric: host=algo-1, epoch=192, train loss <loss>=3.2548157692\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:28 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:29 INFO 140149161203520] Epoch[193] Batch[0] avg_epoch_loss=3.369126\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:29 INFO 140149161203520] #quality_metric: host=algo-1, epoch=193, batch=0 train loss <loss>=3.36912631989\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:30 INFO 140149161203520] Epoch[193] Batch[5] avg_epoch_loss=3.301984\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:30 INFO 140149161203520] #quality_metric: host=algo-1, epoch=193, batch=5 train loss <loss>=3.3019841512\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:30 INFO 140149161203520] Epoch[193] Batch [5]#011Speed: 316.29 samples/sec#011loss=3.301984\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:30 INFO 140149161203520] processed a total of 590 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2291.0101413726807, \"sum\": 2291.0101413726807, \"min\": 2291.0101413726807}}, \"EndTime\": 1588627290.947713, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627288.656207}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:30 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=257.51259238 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:30 INFO 140149161203520] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:30 INFO 140149161203520] #quality_metric: host=algo-1, epoch=193, train loss <loss>=3.37082483768\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:30 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:31 INFO 140149161203520] Epoch[194] Batch[0] avg_epoch_loss=3.140250\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:31 INFO 140149161203520] #quality_metric: host=algo-1, epoch=194, batch=0 train loss <loss>=3.14025020599\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:32 INFO 140149161203520] Epoch[194] Batch[5] avg_epoch_loss=3.240749\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:32 INFO 140149161203520] #quality_metric: host=algo-1, epoch=194, batch=5 train loss <loss>=3.2407485644\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:32 INFO 140149161203520] Epoch[194] Batch [5]#011Speed: 322.49 samples/sec#011loss=3.240749\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:33 INFO 140149161203520] Epoch[194] Batch[10] avg_epoch_loss=3.198930\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:33 INFO 140149161203520] #quality_metric: host=algo-1, epoch=194, batch=10 train loss <loss>=3.14874744415\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:33 INFO 140149161203520] Epoch[194] Batch [10]#011Speed: 320.16 samples/sec#011loss=3.148747\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:33 INFO 140149161203520] processed a total of 684 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2467.6718711853027, \"sum\": 2467.6718711853027, \"min\": 2467.6718711853027}}, \"EndTime\": 1588627293.415969, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627290.94781}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:33 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=277.170281288 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:33 INFO 140149161203520] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:33 INFO 140149161203520] #quality_metric: host=algo-1, epoch=194, train loss <loss>=3.19892987338\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:33 INFO 140149161203520] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:33 INFO 140149161203520] Saved checkpoint to \"/opt/ml/model/state_1d25fdd1-7315-4356-b3fd-32f057e69e53-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 59.93986129760742, \"sum\": 59.93986129760742, \"min\": 59.93986129760742}}, \"EndTime\": 1588627293.476464, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627293.416056}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:33 INFO 140149161203520] Epoch[195] Batch[0] avg_epoch_loss=3.263666\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:33 INFO 140149161203520] #quality_metric: host=algo-1, epoch=195, batch=0 train loss <loss>=3.26366639137\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:34 INFO 140149161203520] Epoch[195] Batch[5] avg_epoch_loss=3.329999\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:34 INFO 140149161203520] #quality_metric: host=algo-1, epoch=195, batch=5 train loss <loss>=3.3299986124\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:34 INFO 140149161203520] Epoch[195] Batch [5]#011Speed: 326.16 samples/sec#011loss=3.329999\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:35 INFO 140149161203520] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2233.793020248413, \"sum\": 2233.793020248413, \"min\": 2233.793020248413}}, \"EndTime\": 1588627295.710415, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627293.476549}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:35 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=276.1944999 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:35 INFO 140149161203520] #progress_metric: host=algo-1, completed 49 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:35 INFO 140149161203520] #quality_metric: host=algo-1, epoch=195, train loss <loss>=3.27745184898\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:35 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:36 INFO 140149161203520] Epoch[196] Batch[0] avg_epoch_loss=3.327405\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:36 INFO 140149161203520] #quality_metric: host=algo-1, epoch=196, batch=0 train loss <loss>=3.32740521431\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:37 INFO 140149161203520] Epoch[196] Batch[5] avg_epoch_loss=3.284552\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:37 INFO 140149161203520] #quality_metric: host=algo-1, epoch=196, batch=5 train loss <loss>=3.28455201785\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:37 INFO 140149161203520] Epoch[196] Batch [5]#011Speed: 311.81 samples/sec#011loss=3.284552\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:38 INFO 140149161203520] Epoch[196] Batch[10] avg_epoch_loss=3.189767\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:38 INFO 140149161203520] #quality_metric: host=algo-1, epoch=196, batch=10 train loss <loss>=3.07602438927\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:38 INFO 140149161203520] Epoch[196] Batch [10]#011Speed: 324.73 samples/sec#011loss=3.076024\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:38 INFO 140149161203520] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2477.6859283447266, \"sum\": 2477.6859283447266, \"min\": 2477.6859283447266}}, \"EndTime\": 1588627298.188691, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627295.710512}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:38 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=263.538549751 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:38 INFO 140149161203520] #progress_metric: host=algo-1, completed 49 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:38 INFO 140149161203520] #quality_metric: host=algo-1, epoch=196, train loss <loss>=3.18976673213\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:38 INFO 140149161203520] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:38 INFO 140149161203520] Saved checkpoint to \"/opt/ml/model/state_f8c1e4a1-3176-4889-a959-5a2e66c8995e-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 69.0310001373291, \"sum\": 69.0310001373291, \"min\": 69.0310001373291}}, \"EndTime\": 1588627298.258329, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627298.188781}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:38 INFO 140149161203520] Epoch[197] Batch[0] avg_epoch_loss=3.482209\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:38 INFO 140149161203520] #quality_metric: host=algo-1, epoch=197, batch=0 train loss <loss>=3.48220920563\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:39 INFO 140149161203520] Epoch[197] Batch[5] avg_epoch_loss=3.303199\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:39 INFO 140149161203520] #quality_metric: host=algo-1, epoch=197, batch=5 train loss <loss>=3.30319857597\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:39 INFO 140149161203520] Epoch[197] Batch [5]#011Speed: 321.89 samples/sec#011loss=3.303199\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:40 INFO 140149161203520] Epoch[197] Batch[10] avg_epoch_loss=3.268435\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:40 INFO 140149161203520] #quality_metric: host=algo-1, epoch=197, batch=10 train loss <loss>=3.22671952248\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:40 INFO 140149161203520] Epoch[197] Batch [10]#011Speed: 319.12 samples/sec#011loss=3.226720\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:40 INFO 140149161203520] processed a total of 677 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2448.019027709961, \"sum\": 2448.019027709961, \"min\": 2448.019027709961}}, \"EndTime\": 1588627300.706509, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627298.258419}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:40 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=276.535567554 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:40 INFO 140149161203520] #progress_metric: host=algo-1, completed 49 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:40 INFO 140149161203520] #quality_metric: host=algo-1, epoch=197, train loss <loss>=3.26843536984\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:40 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:41 INFO 140149161203520] Epoch[198] Batch[0] avg_epoch_loss=3.414893\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:41 INFO 140149161203520] #quality_metric: host=algo-1, epoch=198, batch=0 train loss <loss>=3.41489267349\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:42 INFO 140149161203520] Epoch[198] Batch[5] avg_epoch_loss=3.288658\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:42 INFO 140149161203520] #quality_metric: host=algo-1, epoch=198, batch=5 train loss <loss>=3.28865802288\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:42 INFO 140149161203520] Epoch[198] Batch [5]#011Speed: 319.88 samples/sec#011loss=3.288658\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:42 INFO 140149161203520] processed a total of 609 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2250.452995300293, \"sum\": 2250.452995300293, \"min\": 2250.452995300293}}, \"EndTime\": 1588627302.95753, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627300.706598}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:42 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=270.598136738 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:42 INFO 140149161203520] #progress_metric: host=algo-1, completed 49 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:42 INFO 140149161203520] #quality_metric: host=algo-1, epoch=198, train loss <loss>=3.34513444901\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:42 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:43 INFO 140149161203520] Epoch[199] Batch[0] avg_epoch_loss=3.109113\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:43 INFO 140149161203520] #quality_metric: host=algo-1, epoch=199, batch=0 train loss <loss>=3.10911297798\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:44 INFO 140149161203520] Epoch[199] Batch[5] avg_epoch_loss=3.244513\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:44 INFO 140149161203520] #quality_metric: host=algo-1, epoch=199, batch=5 train loss <loss>=3.24451303482\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:44 INFO 140149161203520] Epoch[199] Batch [5]#011Speed: 320.37 samples/sec#011loss=3.244513\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:45 INFO 140149161203520] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2241.932153701782, \"sum\": 2241.932153701782, \"min\": 2241.932153701782}}, \"EndTime\": 1588627305.200052, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627302.957609}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:45 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=275.192009466 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:45 INFO 140149161203520] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:45 INFO 140149161203520] #quality_metric: host=algo-1, epoch=199, train loss <loss>=3.24133419991\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:45 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:45 INFO 140149161203520] Epoch[200] Batch[0] avg_epoch_loss=3.230594\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:45 INFO 140149161203520] #quality_metric: host=algo-1, epoch=200, batch=0 train loss <loss>=3.23059368134\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:46 INFO 140149161203520] Epoch[200] Batch[5] avg_epoch_loss=3.296737\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:46 INFO 140149161203520] #quality_metric: host=algo-1, epoch=200, batch=5 train loss <loss>=3.29673731327\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:46 INFO 140149161203520] Epoch[200] Batch [5]#011Speed: 320.07 samples/sec#011loss=3.296737\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:47 INFO 140149161203520] processed a total of 553 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2048.877954483032, \"sum\": 2048.877954483032, \"min\": 2048.877954483032}}, \"EndTime\": 1588627307.249584, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627305.20015}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:47 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=269.88802949 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:47 INFO 140149161203520] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:47 INFO 140149161203520] #quality_metric: host=algo-1, epoch=200, train loss <loss>=3.34341356489\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:47 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:47 INFO 140149161203520] Epoch[201] Batch[0] avg_epoch_loss=3.263652\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:47 INFO 140149161203520] #quality_metric: host=algo-1, epoch=201, batch=0 train loss <loss>=3.26365160942\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:48 INFO 140149161203520] Epoch[201] Batch[5] avg_epoch_loss=3.277447\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:48 INFO 140149161203520] #quality_metric: host=algo-1, epoch=201, batch=5 train loss <loss>=3.27744710445\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:48 INFO 140149161203520] Epoch[201] Batch [5]#011Speed: 325.22 samples/sec#011loss=3.277447\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:49 INFO 140149161203520] Epoch[201] Batch[10] avg_epoch_loss=3.344042\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:49 INFO 140149161203520] #quality_metric: host=algo-1, epoch=201, batch=10 train loss <loss>=3.42395625114\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:49 INFO 140149161203520] Epoch[201] Batch [10]#011Speed: 320.54 samples/sec#011loss=3.423956\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:49 INFO 140149161203520] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2429.297924041748, \"sum\": 2429.297924041748, \"min\": 2429.297924041748}}, \"EndTime\": 1588627309.679517, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627307.249666}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:49 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=265.905863065 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:49 INFO 140149161203520] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:49 INFO 140149161203520] #quality_metric: host=algo-1, epoch=201, train loss <loss>=3.34404217113\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:49 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:50 INFO 140149161203520] Epoch[202] Batch[0] avg_epoch_loss=3.271335\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:50 INFO 140149161203520] #quality_metric: host=algo-1, epoch=202, batch=0 train loss <loss>=3.27133512497\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:51 INFO 140149161203520] Epoch[202] Batch[5] avg_epoch_loss=3.326524\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:51 INFO 140149161203520] #quality_metric: host=algo-1, epoch=202, batch=5 train loss <loss>=3.32652405898\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:51 INFO 140149161203520] Epoch[202] Batch [5]#011Speed: 317.59 samples/sec#011loss=3.326524\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:52 INFO 140149161203520] Epoch[202] Batch[10] avg_epoch_loss=3.262107\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:52 INFO 140149161203520] #quality_metric: host=algo-1, epoch=202, batch=10 train loss <loss>=3.18480548859\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:52 INFO 140149161203520] Epoch[202] Batch [10]#011Speed: 316.77 samples/sec#011loss=3.184805\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:52 INFO 140149161203520] processed a total of 678 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2469.1638946533203, \"sum\": 2469.1638946533203, \"min\": 2469.1638946533203}}, \"EndTime\": 1588627312.149197, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627309.679607}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:52 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=274.573646246 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:52 INFO 140149161203520] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:52 INFO 140149161203520] #quality_metric: host=algo-1, epoch=202, train loss <loss>=3.26210652698\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:52 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:52 INFO 140149161203520] Epoch[203] Batch[0] avg_epoch_loss=3.175174\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:52 INFO 140149161203520] #quality_metric: host=algo-1, epoch=203, batch=0 train loss <loss>=3.17517375946\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:53 INFO 140149161203520] Epoch[203] Batch[5] avg_epoch_loss=3.299566\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:53 INFO 140149161203520] #quality_metric: host=algo-1, epoch=203, batch=5 train loss <loss>=3.29956638813\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:53 INFO 140149161203520] Epoch[203] Batch [5]#011Speed: 322.61 samples/sec#011loss=3.299566\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:54 INFO 140149161203520] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2229.475975036621, \"sum\": 2229.475975036621, \"min\": 2229.475975036621}}, \"EndTime\": 1588627314.379224, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627312.149278}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:54 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=283.905399315 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:54 INFO 140149161203520] #progress_metric: host=algo-1, completed 51 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:54 INFO 140149161203520] #quality_metric: host=algo-1, epoch=203, train loss <loss>=3.23638093472\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:54 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:54 INFO 140149161203520] Epoch[204] Batch[0] avg_epoch_loss=3.320380\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:54 INFO 140149161203520] #quality_metric: host=algo-1, epoch=204, batch=0 train loss <loss>=3.32038021088\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:55 INFO 140149161203520] Epoch[204] Batch[5] avg_epoch_loss=3.238873\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:55 INFO 140149161203520] #quality_metric: host=algo-1, epoch=204, batch=5 train loss <loss>=3.23887300491\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:55 INFO 140149161203520] Epoch[204] Batch [5]#011Speed: 318.88 samples/sec#011loss=3.238873\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:56 INFO 140149161203520] processed a total of 615 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2258.558988571167, \"sum\": 2258.558988571167, \"min\": 2258.558988571167}}, \"EndTime\": 1588627316.638396, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627314.379321}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:56 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=272.280643088 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:56 INFO 140149161203520] #progress_metric: host=algo-1, completed 51 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:56 INFO 140149161203520] #quality_metric: host=algo-1, epoch=204, train loss <loss>=3.32591047287\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:56 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:57 INFO 140149161203520] Epoch[205] Batch[0] avg_epoch_loss=3.399646\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:57 INFO 140149161203520] #quality_metric: host=algo-1, epoch=205, batch=0 train loss <loss>=3.39964604378\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:58 INFO 140149161203520] Epoch[205] Batch[5] avg_epoch_loss=3.362285\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:58 INFO 140149161203520] #quality_metric: host=algo-1, epoch=205, batch=5 train loss <loss>=3.36228509744\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:58 INFO 140149161203520] Epoch[205] Batch [5]#011Speed: 325.19 samples/sec#011loss=3.362285\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:58 INFO 140149161203520] processed a total of 616 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2239.131212234497, \"sum\": 2239.131212234497, \"min\": 2239.131212234497}}, \"EndTime\": 1588627318.878119, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627316.638494}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:58 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=275.090268286 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:58 INFO 140149161203520] #progress_metric: host=algo-1, completed 51 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:58 INFO 140149161203520] #quality_metric: host=algo-1, epoch=205, train loss <loss>=3.2695981741\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:58 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:59 INFO 140149161203520] Epoch[206] Batch[0] avg_epoch_loss=3.384937\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:21:59 INFO 140149161203520] #quality_metric: host=algo-1, epoch=206, batch=0 train loss <loss>=3.38493657112\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:00 INFO 140149161203520] Epoch[206] Batch[5] avg_epoch_loss=3.298635\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:00 INFO 140149161203520] #quality_metric: host=algo-1, epoch=206, batch=5 train loss <loss>=3.29863536358\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:00 INFO 140149161203520] Epoch[206] Batch [5]#011Speed: 317.66 samples/sec#011loss=3.298635\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:01 INFO 140149161203520] Epoch[206] Batch[10] avg_epoch_loss=3.226132\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:01 INFO 140149161203520] #quality_metric: host=algo-1, epoch=206, batch=10 train loss <loss>=3.13912858963\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:01 INFO 140149161203520] Epoch[206] Batch [10]#011Speed: 315.51 samples/sec#011loss=3.139129\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:01 INFO 140149161203520] processed a total of 682 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2493.6580657958984, \"sum\": 2493.6580657958984, \"min\": 2493.6580657958984}}, \"EndTime\": 1588627321.372365, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627318.878213}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:01 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=273.47977685 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:01 INFO 140149161203520] #progress_metric: host=algo-1, completed 51 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:01 INFO 140149161203520] #quality_metric: host=algo-1, epoch=206, train loss <loss>=3.22613228451\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:01 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:01 INFO 140149161203520] Epoch[207] Batch[0] avg_epoch_loss=3.358034\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:01 INFO 140149161203520] #quality_metric: host=algo-1, epoch=207, batch=0 train loss <loss>=3.35803437233\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:02 INFO 140149161203520] Epoch[207] Batch[5] avg_epoch_loss=3.271405\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:02 INFO 140149161203520] #quality_metric: host=algo-1, epoch=207, batch=5 train loss <loss>=3.27140466372\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:02 INFO 140149161203520] Epoch[207] Batch [5]#011Speed: 324.68 samples/sec#011loss=3.271405\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:03 INFO 140149161203520] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2307.57999420166, \"sum\": 2307.57999420166, \"min\": 2307.57999420166}}, \"EndTime\": 1588627323.680444, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627321.372453}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:03 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=270.832887789 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:03 INFO 140149161203520] #progress_metric: host=algo-1, completed 52 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:03 INFO 140149161203520] #quality_metric: host=algo-1, epoch=207, train loss <loss>=3.29862675667\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:03 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:04 INFO 140149161203520] Epoch[208] Batch[0] avg_epoch_loss=3.244014\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:04 INFO 140149161203520] #quality_metric: host=algo-1, epoch=208, batch=0 train loss <loss>=3.24401402473\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:05 INFO 140149161203520] Epoch[208] Batch[5] avg_epoch_loss=3.358631\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:05 INFO 140149161203520] #quality_metric: host=algo-1, epoch=208, batch=5 train loss <loss>=3.35863069693\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:05 INFO 140149161203520] Epoch[208] Batch [5]#011Speed: 316.28 samples/sec#011loss=3.358631\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:06 INFO 140149161203520] Epoch[208] Batch[10] avg_epoch_loss=3.404548\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:06 INFO 140149161203520] #quality_metric: host=algo-1, epoch=208, batch=10 train loss <loss>=3.45964865685\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:06 INFO 140149161203520] Epoch[208] Batch [10]#011Speed: 317.25 samples/sec#011loss=3.459649\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:06 INFO 140149161203520] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2471.492052078247, \"sum\": 2471.492052078247, \"min\": 2471.492052078247}}, \"EndTime\": 1588627326.152514, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627323.680521}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:06 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=264.19800802 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:06 INFO 140149161203520] #progress_metric: host=algo-1, completed 52 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:06 INFO 140149161203520] #quality_metric: host=algo-1, epoch=208, train loss <loss>=3.40454795144\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:06 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:06 INFO 140149161203520] Epoch[209] Batch[0] avg_epoch_loss=3.573146\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:06 INFO 140149161203520] #quality_metric: host=algo-1, epoch=209, batch=0 train loss <loss>=3.57314610481\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:07 INFO 140149161203520] Epoch[209] Batch[5] avg_epoch_loss=3.330330\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:07 INFO 140149161203520] #quality_metric: host=algo-1, epoch=209, batch=5 train loss <loss>=3.33032957713\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:07 INFO 140149161203520] Epoch[209] Batch [5]#011Speed: 299.53 samples/sec#011loss=3.330330\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:08 INFO 140149161203520] Epoch[209] Batch[10] avg_epoch_loss=3.276284\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:08 INFO 140149161203520] #quality_metric: host=algo-1, epoch=209, batch=10 train loss <loss>=3.21142873764\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:08 INFO 140149161203520] Epoch[209] Batch [10]#011Speed: 321.67 samples/sec#011loss=3.211429\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:08 INFO 140149161203520] processed a total of 676 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2522.390842437744, \"sum\": 2522.390842437744, \"min\": 2522.390842437744}}, \"EndTime\": 1588627328.675507, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627326.152609}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:08 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=267.985874018 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:08 INFO 140149161203520] #progress_metric: host=algo-1, completed 52 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:08 INFO 140149161203520] #quality_metric: host=algo-1, epoch=209, train loss <loss>=3.276283741\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:08 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:09 INFO 140149161203520] Epoch[210] Batch[0] avg_epoch_loss=3.266394\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:09 INFO 140149161203520] #quality_metric: host=algo-1, epoch=210, batch=0 train loss <loss>=3.26639437675\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:10 INFO 140149161203520] Epoch[210] Batch[5] avg_epoch_loss=3.306754\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:10 INFO 140149161203520] #quality_metric: host=algo-1, epoch=210, batch=5 train loss <loss>=3.30675427119\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:10 INFO 140149161203520] Epoch[210] Batch [5]#011Speed: 320.86 samples/sec#011loss=3.306754\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:11 INFO 140149161203520] Epoch[210] Batch[10] avg_epoch_loss=3.321878\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:11 INFO 140149161203520] #quality_metric: host=algo-1, epoch=210, batch=10 train loss <loss>=3.34002742767\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:11 INFO 140149161203520] Epoch[210] Batch [10]#011Speed: 318.91 samples/sec#011loss=3.340027\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:11 INFO 140149161203520] processed a total of 712 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2693.0699348449707, \"sum\": 2693.0699348449707, \"min\": 2693.0699348449707}}, \"EndTime\": 1588627331.369107, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627328.675596}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:11 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=264.368739993 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:11 INFO 140149161203520] #progress_metric: host=algo-1, completed 52 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:11 INFO 140149161203520] #quality_metric: host=algo-1, epoch=210, train loss <loss>=3.37295800447\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:11 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:11 INFO 140149161203520] Epoch[211] Batch[0] avg_epoch_loss=3.239637\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:11 INFO 140149161203520] #quality_metric: host=algo-1, epoch=211, batch=0 train loss <loss>=3.23963737488\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:12 INFO 140149161203520] Epoch[211] Batch[5] avg_epoch_loss=3.248111\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:12 INFO 140149161203520] #quality_metric: host=algo-1, epoch=211, batch=5 train loss <loss>=3.24811108907\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:12 INFO 140149161203520] Epoch[211] Batch [5]#011Speed: 320.36 samples/sec#011loss=3.248111\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:13 INFO 140149161203520] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2319.0829753875732, \"sum\": 2319.0829753875732, \"min\": 2319.0829753875732}}, \"EndTime\": 1588627333.68878, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627331.369202}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:13 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=272.938238419 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:13 INFO 140149161203520] #progress_metric: host=algo-1, completed 53 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:13 INFO 140149161203520] #quality_metric: host=algo-1, epoch=211, train loss <loss>=3.26103577614\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:13 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:14 INFO 140149161203520] Epoch[212] Batch[0] avg_epoch_loss=3.305375\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:14 INFO 140149161203520] #quality_metric: host=algo-1, epoch=212, batch=0 train loss <loss>=3.30537486076\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:15 INFO 140149161203520] Epoch[212] Batch[5] avg_epoch_loss=3.247892\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:15 INFO 140149161203520] #quality_metric: host=algo-1, epoch=212, batch=5 train loss <loss>=3.24789162477\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:15 INFO 140149161203520] Epoch[212] Batch [5]#011Speed: 313.85 samples/sec#011loss=3.247892\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:16 INFO 140149161203520] Epoch[212] Batch[10] avg_epoch_loss=3.342227\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:16 INFO 140149161203520] #quality_metric: host=algo-1, epoch=212, batch=10 train loss <loss>=3.45542879105\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:16 INFO 140149161203520] Epoch[212] Batch [10]#011Speed: 308.90 samples/sec#011loss=3.455429\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:16 INFO 140149161203520] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2568.142890930176, \"sum\": 2568.142890930176, \"min\": 2568.142890930176}}, \"EndTime\": 1588627336.257515, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627333.68886}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:16 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=251.142801523 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:16 INFO 140149161203520] #progress_metric: host=algo-1, completed 53 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:16 INFO 140149161203520] #quality_metric: host=algo-1, epoch=212, train loss <loss>=3.34222670035\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:16 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:16 INFO 140149161203520] Epoch[213] Batch[0] avg_epoch_loss=3.280103\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:16 INFO 140149161203520] #quality_metric: host=algo-1, epoch=213, batch=0 train loss <loss>=3.28010320663\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:17 INFO 140149161203520] Epoch[213] Batch[5] avg_epoch_loss=3.337408\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:17 INFO 140149161203520] #quality_metric: host=algo-1, epoch=213, batch=5 train loss <loss>=3.33740782738\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:17 INFO 140149161203520] Epoch[213] Batch [5]#011Speed: 320.61 samples/sec#011loss=3.337408\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:18 INFO 140149161203520] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2243.551015853882, \"sum\": 2243.551015853882, \"min\": 2243.551015853882}}, \"EndTime\": 1588627338.501619, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627336.257588}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:18 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=279.898441742 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:18 INFO 140149161203520] #progress_metric: host=algo-1, completed 53 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:18 INFO 140149161203520] #quality_metric: host=algo-1, epoch=213, train loss <loss>=3.31930301189\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:18 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:18 INFO 140149161203520] Epoch[214] Batch[0] avg_epoch_loss=3.277775\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:18 INFO 140149161203520] #quality_metric: host=algo-1, epoch=214, batch=0 train loss <loss>=3.27777481079\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:19 INFO 140149161203520] Epoch[214] Batch[5] avg_epoch_loss=3.349293\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:19 INFO 140149161203520] #quality_metric: host=algo-1, epoch=214, batch=5 train loss <loss>=3.34929307302\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:19 INFO 140149161203520] Epoch[214] Batch [5]#011Speed: 321.74 samples/sec#011loss=3.349293\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:20 INFO 140149161203520] Epoch[214] Batch[10] avg_epoch_loss=3.233160\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:20 INFO 140149161203520] #quality_metric: host=algo-1, epoch=214, batch=10 train loss <loss>=3.09380087852\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:20 INFO 140149161203520] Epoch[214] Batch [10]#011Speed: 306.93 samples/sec#011loss=3.093801\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:20 INFO 140149161203520] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2482.8269481658936, \"sum\": 2482.8269481658936, \"min\": 2482.8269481658936}}, \"EndTime\": 1588627340.98501, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627338.501697}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:20 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=259.36807691 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:20 INFO 140149161203520] #progress_metric: host=algo-1, completed 53 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:20 INFO 140149161203520] #quality_metric: host=algo-1, epoch=214, train loss <loss>=3.23316025734\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:20 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:21 INFO 140149161203520] Epoch[215] Batch[0] avg_epoch_loss=3.251686\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:21 INFO 140149161203520] #quality_metric: host=algo-1, epoch=215, batch=0 train loss <loss>=3.25168585777\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:22 INFO 140149161203520] Epoch[215] Batch[5] avg_epoch_loss=3.219950\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:22 INFO 140149161203520] #quality_metric: host=algo-1, epoch=215, batch=5 train loss <loss>=3.21995019913\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:22 INFO 140149161203520] Epoch[215] Batch [5]#011Speed: 320.98 samples/sec#011loss=3.219950\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:23 INFO 140149161203520] Epoch[215] Batch[10] avg_epoch_loss=3.221130\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:23 INFO 140149161203520] #quality_metric: host=algo-1, epoch=215, batch=10 train loss <loss>=3.2225464344\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:23 INFO 140149161203520] Epoch[215] Batch [10]#011Speed: 318.91 samples/sec#011loss=3.222546\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:23 INFO 140149161203520] processed a total of 676 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2475.5728244781494, \"sum\": 2475.5728244781494, \"min\": 2475.5728244781494}}, \"EndTime\": 1588627343.461142, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627340.9851}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:23 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=273.053965088 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:23 INFO 140149161203520] #progress_metric: host=algo-1, completed 54 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:23 INFO 140149161203520] #quality_metric: host=algo-1, epoch=215, train loss <loss>=3.22113030607\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:23 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:23 INFO 140149161203520] Epoch[216] Batch[0] avg_epoch_loss=3.151763\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:23 INFO 140149161203520] #quality_metric: host=algo-1, epoch=216, batch=0 train loss <loss>=3.15176272392\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:24 INFO 140149161203520] Epoch[216] Batch[5] avg_epoch_loss=3.188510\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:24 INFO 140149161203520] #quality_metric: host=algo-1, epoch=216, batch=5 train loss <loss>=3.18850970268\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:24 INFO 140149161203520] Epoch[216] Batch [5]#011Speed: 321.65 samples/sec#011loss=3.188510\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:25 INFO 140149161203520] Epoch[216] Batch[10] avg_epoch_loss=3.224313\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:25 INFO 140149161203520] #quality_metric: host=algo-1, epoch=216, batch=10 train loss <loss>=3.26727776527\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:25 INFO 140149161203520] Epoch[216] Batch [10]#011Speed: 316.61 samples/sec#011loss=3.267278\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:25 INFO 140149161203520] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2471.743106842041, \"sum\": 2471.743106842041, \"min\": 2471.743106842041}}, \"EndTime\": 1588627345.933418, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627343.461229}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:25 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=267.408787495 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:25 INFO 140149161203520] #progress_metric: host=algo-1, completed 54 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:25 INFO 140149161203520] #quality_metric: host=algo-1, epoch=216, train loss <loss>=3.2243133675\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:25 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:26 INFO 140149161203520] Epoch[217] Batch[0] avg_epoch_loss=3.328020\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:26 INFO 140149161203520] #quality_metric: host=algo-1, epoch=217, batch=0 train loss <loss>=3.32801961899\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:27 INFO 140149161203520] Epoch[217] Batch[5] avg_epoch_loss=3.248133\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:27 INFO 140149161203520] #quality_metric: host=algo-1, epoch=217, batch=5 train loss <loss>=3.24813262622\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:27 INFO 140149161203520] Epoch[217] Batch [5]#011Speed: 324.77 samples/sec#011loss=3.248133\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:28 INFO 140149161203520] processed a total of 621 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2237.701892852783, \"sum\": 2237.701892852783, \"min\": 2237.701892852783}}, \"EndTime\": 1588627348.17168, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627345.933505}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:28 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=277.499473588 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:28 INFO 140149161203520] #progress_metric: host=algo-1, completed 54 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:28 INFO 140149161203520] #quality_metric: host=algo-1, epoch=217, train loss <loss>=3.31090741158\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:28 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:28 INFO 140149161203520] Epoch[218] Batch[0] avg_epoch_loss=3.119605\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:28 INFO 140149161203520] #quality_metric: host=algo-1, epoch=218, batch=0 train loss <loss>=3.11960482597\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:29 INFO 140149161203520] Epoch[218] Batch[5] avg_epoch_loss=3.158645\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:29 INFO 140149161203520] #quality_metric: host=algo-1, epoch=218, batch=5 train loss <loss>=3.15864539146\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:29 INFO 140149161203520] Epoch[218] Batch [5]#011Speed: 325.26 samples/sec#011loss=3.158645\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:30 INFO 140149161203520] Epoch[218] Batch[10] avg_epoch_loss=3.292020\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:30 INFO 140149161203520] #quality_metric: host=algo-1, epoch=218, batch=10 train loss <loss>=3.45207023621\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:30 INFO 140149161203520] Epoch[218] Batch [10]#011Speed: 318.77 samples/sec#011loss=3.452070\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:30 INFO 140149161203520] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2492.511987686157, \"sum\": 2492.511987686157, \"min\": 2492.511987686157}}, \"EndTime\": 1588627350.664779, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627348.171778}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:30 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=259.163097825 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:30 INFO 140149161203520] #progress_metric: host=algo-1, completed 54 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:30 INFO 140149161203520] #quality_metric: host=algo-1, epoch=218, train loss <loss>=3.29202032089\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:30 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:31 INFO 140149161203520] Epoch[219] Batch[0] avg_epoch_loss=3.160215\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:31 INFO 140149161203520] #quality_metric: host=algo-1, epoch=219, batch=0 train loss <loss>=3.16021466255\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:32 INFO 140149161203520] Epoch[219] Batch[5] avg_epoch_loss=3.228333\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:32 INFO 140149161203520] #quality_metric: host=algo-1, epoch=219, batch=5 train loss <loss>=3.22833283742\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:32 INFO 140149161203520] Epoch[219] Batch [5]#011Speed: 321.23 samples/sec#011loss=3.228333\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:33 INFO 140149161203520] Epoch[219] Batch[10] avg_epoch_loss=3.207693\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:33 INFO 140149161203520] #quality_metric: host=algo-1, epoch=219, batch=10 train loss <loss>=3.18292422295\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:33 INFO 140149161203520] Epoch[219] Batch [10]#011Speed: 324.36 samples/sec#011loss=3.182924\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:33 INFO 140149161203520] processed a total of 668 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2438.1489753723145, \"sum\": 2438.1489753723145, \"min\": 2438.1489753723145}}, \"EndTime\": 1588627353.103453, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627350.664866}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:33 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=273.963628619 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:33 INFO 140149161203520] #progress_metric: host=algo-1, completed 55 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:33 INFO 140149161203520] #quality_metric: host=algo-1, epoch=219, train loss <loss>=3.20769255812\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:33 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:33 INFO 140149161203520] Epoch[220] Batch[0] avg_epoch_loss=3.181545\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:33 INFO 140149161203520] #quality_metric: host=algo-1, epoch=220, batch=0 train loss <loss>=3.18154525757\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:34 INFO 140149161203520] Epoch[220] Batch[5] avg_epoch_loss=3.279010\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:34 INFO 140149161203520] #quality_metric: host=algo-1, epoch=220, batch=5 train loss <loss>=3.27900997798\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:34 INFO 140149161203520] Epoch[220] Batch [5]#011Speed: 325.81 samples/sec#011loss=3.279010\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:35 INFO 140149161203520] Epoch[220] Batch[10] avg_epoch_loss=3.342588\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:35 INFO 140149161203520] #quality_metric: host=algo-1, epoch=220, batch=10 train loss <loss>=3.41888108253\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:35 INFO 140149161203520] Epoch[220] Batch [10]#011Speed: 312.47 samples/sec#011loss=3.418881\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:35 INFO 140149161203520] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2471.127986907959, \"sum\": 2471.127986907959, \"min\": 2471.127986907959}}, \"EndTime\": 1588627355.575119, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627353.103545}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:35 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=265.04666564 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:35 INFO 140149161203520] #progress_metric: host=algo-1, completed 55 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:35 INFO 140149161203520] #quality_metric: host=algo-1, epoch=220, train loss <loss>=3.34258775278\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:35 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:36 INFO 140149161203520] Epoch[221] Batch[0] avg_epoch_loss=3.374241\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:36 INFO 140149161203520] #quality_metric: host=algo-1, epoch=221, batch=0 train loss <loss>=3.37424087524\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:37 INFO 140149161203520] Epoch[221] Batch[5] avg_epoch_loss=3.249383\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:37 INFO 140149161203520] #quality_metric: host=algo-1, epoch=221, batch=5 train loss <loss>=3.24938265483\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:37 INFO 140149161203520] Epoch[221] Batch [5]#011Speed: 305.39 samples/sec#011loss=3.249383\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:38 INFO 140149161203520] Epoch[221] Batch[10] avg_epoch_loss=3.279754\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:38 INFO 140149161203520] #quality_metric: host=algo-1, epoch=221, batch=10 train loss <loss>=3.31620006561\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:38 INFO 140149161203520] Epoch[221] Batch [10]#011Speed: 321.55 samples/sec#011loss=3.316200\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:38 INFO 140149161203520] processed a total of 684 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2543.7750816345215, \"sum\": 2543.7750816345215, \"min\": 2543.7750816345215}}, \"EndTime\": 1588627358.119417, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627355.575211}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:38 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=268.879248275 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:38 INFO 140149161203520] #progress_metric: host=algo-1, completed 55 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:38 INFO 140149161203520] #quality_metric: host=algo-1, epoch=221, train loss <loss>=3.27975420518\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:38 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:38 INFO 140149161203520] Epoch[222] Batch[0] avg_epoch_loss=3.345855\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:38 INFO 140149161203520] #quality_metric: host=algo-1, epoch=222, batch=0 train loss <loss>=3.34585499763\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:39 INFO 140149161203520] Epoch[222] Batch[5] avg_epoch_loss=3.255097\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:39 INFO 140149161203520] #quality_metric: host=algo-1, epoch=222, batch=5 train loss <loss>=3.25509707133\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:39 INFO 140149161203520] Epoch[222] Batch [5]#011Speed: 322.24 samples/sec#011loss=3.255097\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:40 INFO 140149161203520] processed a total of 609 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2230.7770252227783, \"sum\": 2230.7770252227783, \"min\": 2230.7770252227783}}, \"EndTime\": 1588627360.350769, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627358.119493}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:40 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=272.984487429 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:40 INFO 140149161203520] #progress_metric: host=algo-1, completed 55 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:40 INFO 140149161203520] #quality_metric: host=algo-1, epoch=222, train loss <loss>=3.17618300915\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:40 INFO 140149161203520] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:40 INFO 140149161203520] Saved checkpoint to \"/opt/ml/model/state_e4df20aa-175d-49b0-be52-70133f8593a3-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 60.0740909576416, \"sum\": 60.0740909576416, \"min\": 60.0740909576416}}, \"EndTime\": 1588627360.411522, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627360.350844}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:40 INFO 140149161203520] Epoch[223] Batch[0] avg_epoch_loss=3.135649\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:40 INFO 140149161203520] #quality_metric: host=algo-1, epoch=223, batch=0 train loss <loss>=3.13564872742\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:41 INFO 140149161203520] Epoch[223] Batch[5] avg_epoch_loss=3.253340\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:41 INFO 140149161203520] #quality_metric: host=algo-1, epoch=223, batch=5 train loss <loss>=3.25334048271\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:41 INFO 140149161203520] Epoch[223] Batch [5]#011Speed: 320.19 samples/sec#011loss=3.253340\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:42 INFO 140149161203520] Epoch[223] Batch[10] avg_epoch_loss=3.167200\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:42 INFO 140149161203520] #quality_metric: host=algo-1, epoch=223, batch=10 train loss <loss>=3.06383051872\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:42 INFO 140149161203520] Epoch[223] Batch [10]#011Speed: 320.11 samples/sec#011loss=3.063831\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:42 INFO 140149161203520] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2457.4949741363525, \"sum\": 2457.4949741363525, \"min\": 2457.4949741363525}}, \"EndTime\": 1588627362.869156, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627360.411598}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:42 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=268.554127628 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:42 INFO 140149161203520] #progress_metric: host=algo-1, completed 56 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:42 INFO 140149161203520] #quality_metric: host=algo-1, epoch=223, train loss <loss>=3.16719958999\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:42 INFO 140149161203520] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:42 INFO 140149161203520] Saved checkpoint to \"/opt/ml/model/state_5c2b4c06-2613-4de3-b6ba-aa24bb1617e9-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 59.35382843017578, \"sum\": 59.35382843017578, \"min\": 59.35382843017578}}, \"EndTime\": 1588627362.929132, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627362.869228}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:43 INFO 140149161203520] Epoch[224] Batch[0] avg_epoch_loss=3.462111\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:43 INFO 140149161203520] #quality_metric: host=algo-1, epoch=224, batch=0 train loss <loss>=3.46211075783\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:44 INFO 140149161203520] Epoch[224] Batch[5] avg_epoch_loss=3.231259\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:44 INFO 140149161203520] #quality_metric: host=algo-1, epoch=224, batch=5 train loss <loss>=3.23125902812\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:44 INFO 140149161203520] Epoch[224] Batch [5]#011Speed: 320.43 samples/sec#011loss=3.231259\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:45 INFO 140149161203520] processed a total of 590 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2228.813886642456, \"sum\": 2228.813886642456, \"min\": 2228.813886642456}}, \"EndTime\": 1588627365.1581, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627362.929216}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:45 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=264.70207638 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:45 INFO 140149161203520] #progress_metric: host=algo-1, completed 56 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:45 INFO 140149161203520] #quality_metric: host=algo-1, epoch=224, train loss <loss>=3.1406835556\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:45 INFO 140149161203520] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:45 INFO 140149161203520] Saved checkpoint to \"/opt/ml/model/state_7d322f62-70cf-4b71-9f14-e7f025db5792-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 58.875083923339844, \"sum\": 58.875083923339844, \"min\": 58.875083923339844}}, \"EndTime\": 1588627365.217576, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627365.158171}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:45 INFO 140149161203520] Epoch[225] Batch[0] avg_epoch_loss=3.436111\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:45 INFO 140149161203520] #quality_metric: host=algo-1, epoch=225, batch=0 train loss <loss>=3.4361114502\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:46 INFO 140149161203520] Epoch[225] Batch[5] avg_epoch_loss=3.215993\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:46 INFO 140149161203520] #quality_metric: host=algo-1, epoch=225, batch=5 train loss <loss>=3.21599292755\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:46 INFO 140149161203520] Epoch[225] Batch [5]#011Speed: 322.58 samples/sec#011loss=3.215993\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:47 INFO 140149161203520] Epoch[225] Batch[10] avg_epoch_loss=3.151558\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:47 INFO 140149161203520] #quality_metric: host=algo-1, epoch=225, batch=10 train loss <loss>=3.07423596382\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:47 INFO 140149161203520] Epoch[225] Batch [10]#011Speed: 319.80 samples/sec#011loss=3.074236\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:47 INFO 140149161203520] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2445.6331729888916, \"sum\": 2445.6331729888916, \"min\": 2445.6331729888916}}, \"EndTime\": 1588627367.663342, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627365.217636}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:47 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=262.085812656 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:47 INFO 140149161203520] #progress_metric: host=algo-1, completed 56 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:47 INFO 140149161203520] #quality_metric: host=algo-1, epoch=225, train loss <loss>=3.15155794404\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:47 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:48 INFO 140149161203520] Epoch[226] Batch[0] avg_epoch_loss=3.256232\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:48 INFO 140149161203520] #quality_metric: host=algo-1, epoch=226, batch=0 train loss <loss>=3.25623178482\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:49 INFO 140149161203520] Epoch[226] Batch[5] avg_epoch_loss=3.209958\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:49 INFO 140149161203520] #quality_metric: host=algo-1, epoch=226, batch=5 train loss <loss>=3.20995827516\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:49 INFO 140149161203520] Epoch[226] Batch [5]#011Speed: 314.64 samples/sec#011loss=3.209958\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:49 INFO 140149161203520] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2317.531108856201, \"sum\": 2317.531108856201, \"min\": 2317.531108856201}}, \"EndTime\": 1588627369.981412, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627367.663431}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:49 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=271.824707981 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:49 INFO 140149161203520] #progress_metric: host=algo-1, completed 56 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:49 INFO 140149161203520] #quality_metric: host=algo-1, epoch=226, train loss <loss>=3.23422470093\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:49 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:50 INFO 140149161203520] Epoch[227] Batch[0] avg_epoch_loss=3.327174\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:50 INFO 140149161203520] #quality_metric: host=algo-1, epoch=227, batch=0 train loss <loss>=3.32717442513\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:51 INFO 140149161203520] Epoch[227] Batch[5] avg_epoch_loss=3.233089\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:51 INFO 140149161203520] #quality_metric: host=algo-1, epoch=227, batch=5 train loss <loss>=3.23308908939\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:51 INFO 140149161203520] Epoch[227] Batch [5]#011Speed: 325.95 samples/sec#011loss=3.233089\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:52 INFO 140149161203520] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2276.8850326538086, \"sum\": 2276.8850326538086, \"min\": 2276.8850326538086}}, \"EndTime\": 1588627372.258882, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627369.98151}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:52 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=272.285110729 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:52 INFO 140149161203520] #progress_metric: host=algo-1, completed 57 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:52 INFO 140149161203520] #quality_metric: host=algo-1, epoch=227, train loss <loss>=3.23124535084\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:52 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:52 INFO 140149161203520] Epoch[228] Batch[0] avg_epoch_loss=3.238695\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:52 INFO 140149161203520] #quality_metric: host=algo-1, epoch=228, batch=0 train loss <loss>=3.23869490623\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:53 INFO 140149161203520] Epoch[228] Batch[5] avg_epoch_loss=3.303315\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:53 INFO 140149161203520] #quality_metric: host=algo-1, epoch=228, batch=5 train loss <loss>=3.30331452688\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:53 INFO 140149161203520] Epoch[228] Batch [5]#011Speed: 323.74 samples/sec#011loss=3.303315\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:54 INFO 140149161203520] processed a total of 621 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2270.141839981079, \"sum\": 2270.141839981079, \"min\": 2270.141839981079}}, \"EndTime\": 1588627374.529633, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627372.258979}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:54 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=273.534326124 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:54 INFO 140149161203520] #progress_metric: host=algo-1, completed 57 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:54 INFO 140149161203520] #quality_metric: host=algo-1, epoch=228, train loss <loss>=3.30995893478\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:54 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:55 INFO 140149161203520] Epoch[229] Batch[0] avg_epoch_loss=3.042006\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:55 INFO 140149161203520] #quality_metric: host=algo-1, epoch=229, batch=0 train loss <loss>=3.04200601578\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:56 INFO 140149161203520] Epoch[229] Batch[5] avg_epoch_loss=3.205700\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:56 INFO 140149161203520] #quality_metric: host=algo-1, epoch=229, batch=5 train loss <loss>=3.20570031802\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:56 INFO 140149161203520] Epoch[229] Batch [5]#011Speed: 320.61 samples/sec#011loss=3.205700\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:57 INFO 140149161203520] Epoch[229] Batch[10] avg_epoch_loss=3.238539\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:57 INFO 140149161203520] #quality_metric: host=algo-1, epoch=229, batch=10 train loss <loss>=3.27794551849\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:57 INFO 140149161203520] Epoch[229] Batch [10]#011Speed: 321.88 samples/sec#011loss=3.277946\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:57 INFO 140149161203520] processed a total of 716 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2675.2898693084717, \"sum\": 2675.2898693084717, \"min\": 2675.2898693084717}}, \"EndTime\": 1588627377.20551, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627374.529732}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:57 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=267.620430687 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:57 INFO 140149161203520] #progress_metric: host=algo-1, completed 57 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:57 INFO 140149161203520] #quality_metric: host=algo-1, epoch=229, train loss <loss>=3.18963742256\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:57 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:57 INFO 140149161203520] Epoch[230] Batch[0] avg_epoch_loss=3.282066\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:57 INFO 140149161203520] #quality_metric: host=algo-1, epoch=230, batch=0 train loss <loss>=3.2820661068\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:58 INFO 140149161203520] Epoch[230] Batch[5] avg_epoch_loss=3.259369\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:58 INFO 140149161203520] #quality_metric: host=algo-1, epoch=230, batch=5 train loss <loss>=3.25936881701\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:58 INFO 140149161203520] Epoch[230] Batch [5]#011Speed: 318.38 samples/sec#011loss=3.259369\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:59 INFO 140149161203520] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2278.1829833984375, \"sum\": 2278.1829833984375, \"min\": 2278.1829833984375}}, \"EndTime\": 1588627379.484298, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627377.205606}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:59 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=270.813686491 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:59 INFO 140149161203520] #progress_metric: host=algo-1, completed 57 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:59 INFO 140149161203520] #quality_metric: host=algo-1, epoch=230, train loss <loss>=3.30661146641\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:59 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:59 INFO 140149161203520] Epoch[231] Batch[0] avg_epoch_loss=3.354344\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:22:59 INFO 140149161203520] #quality_metric: host=algo-1, epoch=231, batch=0 train loss <loss>=3.35434412956\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:00 INFO 140149161203520] Epoch[231] Batch[5] avg_epoch_loss=3.280924\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:00 INFO 140149161203520] #quality_metric: host=algo-1, epoch=231, batch=5 train loss <loss>=3.28092392286\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:00 INFO 140149161203520] Epoch[231] Batch [5]#011Speed: 321.75 samples/sec#011loss=3.280924\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:02 INFO 140149161203520] Epoch[231] Batch[10] avg_epoch_loss=3.312295\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:02 INFO 140149161203520] #quality_metric: host=algo-1, epoch=231, batch=10 train loss <loss>=3.34994058609\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:02 INFO 140149161203520] Epoch[231] Batch [10]#011Speed: 306.66 samples/sec#011loss=3.349941\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:02 INFO 140149161203520] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2532.4959754943848, \"sum\": 2532.4959754943848, \"min\": 2532.4959754943848}}, \"EndTime\": 1588627382.017371, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627379.484392}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:02 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=257.045656358 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:02 INFO 140149161203520] #progress_metric: host=algo-1, completed 58 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:02 INFO 140149161203520] #quality_metric: host=algo-1, epoch=231, train loss <loss>=3.31229513342\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:02 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:02 INFO 140149161203520] Epoch[232] Batch[0] avg_epoch_loss=3.276158\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:02 INFO 140149161203520] #quality_metric: host=algo-1, epoch=232, batch=0 train loss <loss>=3.27615833282\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:03 INFO 140149161203520] Epoch[232] Batch[5] avg_epoch_loss=3.192771\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:03 INFO 140149161203520] #quality_metric: host=algo-1, epoch=232, batch=5 train loss <loss>=3.19277087847\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:03 INFO 140149161203520] Epoch[232] Batch [5]#011Speed: 319.63 samples/sec#011loss=3.192771\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:04 INFO 140149161203520] Epoch[232] Batch[10] avg_epoch_loss=3.250552\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:04 INFO 140149161203520] #quality_metric: host=algo-1, epoch=232, batch=10 train loss <loss>=3.31989035606\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:04 INFO 140149161203520] Epoch[232] Batch [10]#011Speed: 319.81 samples/sec#011loss=3.319890\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:04 INFO 140149161203520] processed a total of 667 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2461.40718460083, \"sum\": 2461.40718460083, \"min\": 2461.40718460083}}, \"EndTime\": 1588627384.479325, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627382.017459}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:04 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=270.96957976 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:04 INFO 140149161203520] #progress_metric: host=algo-1, completed 58 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:04 INFO 140149161203520] #quality_metric: host=algo-1, epoch=232, train loss <loss>=3.2505524592\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:04 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:04 INFO 140149161203520] Epoch[233] Batch[0] avg_epoch_loss=3.213324\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:04 INFO 140149161203520] #quality_metric: host=algo-1, epoch=233, batch=0 train loss <loss>=3.21332383156\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:05 INFO 140149161203520] Epoch[233] Batch[5] avg_epoch_loss=3.258024\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:05 INFO 140149161203520] #quality_metric: host=algo-1, epoch=233, batch=5 train loss <loss>=3.25802429517\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:05 INFO 140149161203520] Epoch[233] Batch [5]#011Speed: 321.94 samples/sec#011loss=3.258024\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:06 INFO 140149161203520] processed a total of 599 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2242.558002471924, \"sum\": 2242.558002471924, \"min\": 2242.558002471924}}, \"EndTime\": 1588627386.722396, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627384.47941}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:06 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=267.089843989 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:06 INFO 140149161203520] #progress_metric: host=algo-1, completed 58 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:06 INFO 140149161203520] #quality_metric: host=algo-1, epoch=233, train loss <loss>=3.31546764374\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:06 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:07 INFO 140149161203520] Epoch[234] Batch[0] avg_epoch_loss=3.187996\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:07 INFO 140149161203520] #quality_metric: host=algo-1, epoch=234, batch=0 train loss <loss>=3.18799638748\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:08 INFO 140149161203520] Epoch[234] Batch[5] avg_epoch_loss=3.281179\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:08 INFO 140149161203520] #quality_metric: host=algo-1, epoch=234, batch=5 train loss <loss>=3.28117903074\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:08 INFO 140149161203520] Epoch[234] Batch [5]#011Speed: 319.70 samples/sec#011loss=3.281179\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:09 INFO 140149161203520] processed a total of 594 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2338.002920150757, \"sum\": 2338.002920150757, \"min\": 2338.002920150757}}, \"EndTime\": 1588627389.060964, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627386.722489}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:09 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=254.049487214 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:09 INFO 140149161203520] #progress_metric: host=algo-1, completed 58 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:09 INFO 140149161203520] #quality_metric: host=algo-1, epoch=234, train loss <loss>=3.33673303127\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:09 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:09 INFO 140149161203520] Epoch[235] Batch[0] avg_epoch_loss=3.289367\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:09 INFO 140149161203520] #quality_metric: host=algo-1, epoch=235, batch=0 train loss <loss>=3.28936719894\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:10 INFO 140149161203520] Epoch[235] Batch[5] avg_epoch_loss=3.220598\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:10 INFO 140149161203520] #quality_metric: host=algo-1, epoch=235, batch=5 train loss <loss>=3.2205978632\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:10 INFO 140149161203520] Epoch[235] Batch [5]#011Speed: 318.51 samples/sec#011loss=3.220598\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:11 INFO 140149161203520] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2260.993003845215, \"sum\": 2260.993003845215, \"min\": 2260.993003845215}}, \"EndTime\": 1588627391.32257, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627389.061053}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:11 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=281.277029649 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:11 INFO 140149161203520] #progress_metric: host=algo-1, completed 59 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:11 INFO 140149161203520] #quality_metric: host=algo-1, epoch=235, train loss <loss>=3.19355666637\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:11 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:11 INFO 140149161203520] Epoch[236] Batch[0] avg_epoch_loss=3.240663\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:11 INFO 140149161203520] #quality_metric: host=algo-1, epoch=236, batch=0 train loss <loss>=3.24066281319\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:12 INFO 140149161203520] Epoch[236] Batch[5] avg_epoch_loss=3.272555\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:12 INFO 140149161203520] #quality_metric: host=algo-1, epoch=236, batch=5 train loss <loss>=3.27255491416\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:12 INFO 140149161203520] Epoch[236] Batch [5]#011Speed: 313.67 samples/sec#011loss=3.272555\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:13 INFO 140149161203520] Epoch[236] Batch[10] avg_epoch_loss=3.304845\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:13 INFO 140149161203520] #quality_metric: host=algo-1, epoch=236, batch=10 train loss <loss>=3.34359269142\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:13 INFO 140149161203520] Epoch[236] Batch [10]#011Speed: 318.62 samples/sec#011loss=3.343593\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:13 INFO 140149161203520] processed a total of 665 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2484.035015106201, \"sum\": 2484.035015106201, \"min\": 2484.035015106201}}, \"EndTime\": 1588627393.807275, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627391.322656}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:13 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=267.695049333 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:13 INFO 140149161203520] #progress_metric: host=algo-1, completed 59 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:13 INFO 140149161203520] #quality_metric: host=algo-1, epoch=236, train loss <loss>=3.30484481291\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:13 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:14 INFO 140149161203520] Epoch[237] Batch[0] avg_epoch_loss=3.365078\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:14 INFO 140149161203520] #quality_metric: host=algo-1, epoch=237, batch=0 train loss <loss>=3.36507797241\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:15 INFO 140149161203520] Epoch[237] Batch[5] avg_epoch_loss=3.273521\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:15 INFO 140149161203520] #quality_metric: host=algo-1, epoch=237, batch=5 train loss <loss>=3.27352106571\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:15 INFO 140149161203520] Epoch[237] Batch [5]#011Speed: 320.25 samples/sec#011loss=3.273521\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:16 INFO 140149161203520] Epoch[237] Batch[10] avg_epoch_loss=3.253352\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:16 INFO 140149161203520] #quality_metric: host=algo-1, epoch=237, batch=10 train loss <loss>=3.22914924622\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:16 INFO 140149161203520] Epoch[237] Batch [10]#011Speed: 320.34 samples/sec#011loss=3.229149\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:16 INFO 140149161203520] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2454.6890258789062, \"sum\": 2454.6890258789062, \"min\": 2454.6890258789062}}, \"EndTime\": 1588627396.262548, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627393.807367}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:16 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=267.636843952 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:16 INFO 140149161203520] #progress_metric: host=algo-1, completed 59 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:16 INFO 140149161203520] #quality_metric: host=algo-1, epoch=237, train loss <loss>=3.25335205685\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:16 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:16 INFO 140149161203520] Epoch[238] Batch[0] avg_epoch_loss=3.347154\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:16 INFO 140149161203520] #quality_metric: host=algo-1, epoch=238, batch=0 train loss <loss>=3.34715414047\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:17 INFO 140149161203520] Epoch[238] Batch[5] avg_epoch_loss=3.282809\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:17 INFO 140149161203520] #quality_metric: host=algo-1, epoch=238, batch=5 train loss <loss>=3.28280897935\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:17 INFO 140149161203520] Epoch[238] Batch [5]#011Speed: 325.47 samples/sec#011loss=3.282809\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:18 INFO 140149161203520] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2286.591053009033, \"sum\": 2286.591053009033, \"min\": 2286.591053009033}}, \"EndTime\": 1588627398.549704, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627396.262636}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:18 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=276.376112547 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:18 INFO 140149161203520] #progress_metric: host=algo-1, completed 59 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:18 INFO 140149161203520] #quality_metric: host=algo-1, epoch=238, train loss <loss>=3.24662983418\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:18 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:19 INFO 140149161203520] Epoch[239] Batch[0] avg_epoch_loss=3.335594\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:19 INFO 140149161203520] #quality_metric: host=algo-1, epoch=239, batch=0 train loss <loss>=3.33559370041\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:19 INFO 140149161203520] Epoch[239] Batch[5] avg_epoch_loss=3.220895\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:19 INFO 140149161203520] #quality_metric: host=algo-1, epoch=239, batch=5 train loss <loss>=3.22089473406\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:19 INFO 140149161203520] Epoch[239] Batch [5]#011Speed: 326.01 samples/sec#011loss=3.220895\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:21 INFO 140149161203520] Epoch[239] Batch[10] avg_epoch_loss=3.166547\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:21 INFO 140149161203520] #quality_metric: host=algo-1, epoch=239, batch=10 train loss <loss>=3.10132894516\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:21 INFO 140149161203520] Epoch[239] Batch [10]#011Speed: 316.39 samples/sec#011loss=3.101329\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:21 INFO 140149161203520] processed a total of 664 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2454.0650844573975, \"sum\": 2454.0650844573975, \"min\": 2454.0650844573975}}, \"EndTime\": 1588627401.004381, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627398.549802}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:21 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=270.557355849 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:21 INFO 140149161203520] #progress_metric: host=algo-1, completed 60 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:21 INFO 140149161203520] #quality_metric: host=algo-1, epoch=239, train loss <loss>=3.1665466482\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:21 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:21 INFO 140149161203520] Epoch[240] Batch[0] avg_epoch_loss=3.307160\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:21 INFO 140149161203520] #quality_metric: host=algo-1, epoch=240, batch=0 train loss <loss>=3.30716013908\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:22 INFO 140149161203520] Epoch[240] Batch[5] avg_epoch_loss=3.241987\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:22 INFO 140149161203520] #quality_metric: host=algo-1, epoch=240, batch=5 train loss <loss>=3.24198663235\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:22 INFO 140149161203520] Epoch[240] Batch [5]#011Speed: 323.90 samples/sec#011loss=3.241987\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:23 INFO 140149161203520] Epoch[240] Batch[10] avg_epoch_loss=3.181716\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:23 INFO 140149161203520] #quality_metric: host=algo-1, epoch=240, batch=10 train loss <loss>=3.1093916893\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:23 INFO 140149161203520] Epoch[240] Batch [10]#011Speed: 321.36 samples/sec#011loss=3.109392\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:23 INFO 140149161203520] processed a total of 665 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2451.47705078125, \"sum\": 2451.47705078125, \"min\": 2451.47705078125}}, \"EndTime\": 1588627403.456402, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627401.00447}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:23 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=271.250891783 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:23 INFO 140149161203520] #progress_metric: host=algo-1, completed 60 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:23 INFO 140149161203520] #quality_metric: host=algo-1, epoch=240, train loss <loss>=3.18171620369\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:23 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:23 INFO 140149161203520] Epoch[241] Batch[0] avg_epoch_loss=3.037129\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:23 INFO 140149161203520] #quality_metric: host=algo-1, epoch=241, batch=0 train loss <loss>=3.03712940216\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:24 INFO 140149161203520] Epoch[241] Batch[5] avg_epoch_loss=3.165777\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:24 INFO 140149161203520] #quality_metric: host=algo-1, epoch=241, batch=5 train loss <loss>=3.16577728589\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:24 INFO 140149161203520] Epoch[241] Batch [5]#011Speed: 318.35 samples/sec#011loss=3.165777\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:25 INFO 140149161203520] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2265.4049396514893, \"sum\": 2265.4049396514893, \"min\": 2265.4049396514893}}, \"EndTime\": 1588627405.722351, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627403.456491}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:25 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=272.340549344 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:25 INFO 140149161203520] #progress_metric: host=algo-1, completed 60 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:25 INFO 140149161203520] #quality_metric: host=algo-1, epoch=241, train loss <loss>=3.18430345058\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:25 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:26 INFO 140149161203520] Epoch[242] Batch[0] avg_epoch_loss=3.253709\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:26 INFO 140149161203520] #quality_metric: host=algo-1, epoch=242, batch=0 train loss <loss>=3.25370883942\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:27 INFO 140149161203520] Epoch[242] Batch[5] avg_epoch_loss=3.257199\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:27 INFO 140149161203520] #quality_metric: host=algo-1, epoch=242, batch=5 train loss <loss>=3.25719924768\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:27 INFO 140149161203520] Epoch[242] Batch [5]#011Speed: 322.43 samples/sec#011loss=3.257199\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:28 INFO 140149161203520] Epoch[242] Batch[10] avg_epoch_loss=3.204481\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:28 INFO 140149161203520] #quality_metric: host=algo-1, epoch=242, batch=10 train loss <loss>=3.1412194252\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:28 INFO 140149161203520] Epoch[242] Batch [10]#011Speed: 321.46 samples/sec#011loss=3.141219\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:28 INFO 140149161203520] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2474.0049839019775, \"sum\": 2474.0049839019775, \"min\": 2474.0049839019775}}, \"EndTime\": 1588627408.196954, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627405.72245}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:28 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=262.313781484 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:28 INFO 140149161203520] #progress_metric: host=algo-1, completed 60 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:28 INFO 140149161203520] #quality_metric: host=algo-1, epoch=242, train loss <loss>=3.20448114655\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:28 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:28 INFO 140149161203520] Epoch[243] Batch[0] avg_epoch_loss=3.116166\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:28 INFO 140149161203520] #quality_metric: host=algo-1, epoch=243, batch=0 train loss <loss>=3.11616563797\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:29 INFO 140149161203520] Epoch[243] Batch[5] avg_epoch_loss=3.204824\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:29 INFO 140149161203520] #quality_metric: host=algo-1, epoch=243, batch=5 train loss <loss>=3.20482444763\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:29 INFO 140149161203520] Epoch[243] Batch [5]#011Speed: 320.38 samples/sec#011loss=3.204824\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:30 INFO 140149161203520] Epoch[243] Batch[10] avg_epoch_loss=3.138875\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:30 INFO 140149161203520] #quality_metric: host=algo-1, epoch=243, batch=10 train loss <loss>=3.0597345829\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:30 INFO 140149161203520] Epoch[243] Batch [10]#011Speed: 324.05 samples/sec#011loss=3.059735\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:30 INFO 140149161203520] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2448.1050968170166, \"sum\": 2448.1050968170166, \"min\": 2448.1050968170166}}, \"EndTime\": 1588627410.645611, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627408.197045}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:30 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=265.088953446 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:30 INFO 140149161203520] #progress_metric: host=algo-1, completed 61 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:30 INFO 140149161203520] #quality_metric: host=algo-1, epoch=243, train loss <loss>=3.13887450912\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:30 INFO 140149161203520] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:30 INFO 140149161203520] Saved checkpoint to \"/opt/ml/model/state_2323d53f-8471-4d37-ab86-ab432a636169-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 58.80093574523926, \"sum\": 58.80093574523926, \"min\": 58.80093574523926}}, \"EndTime\": 1588627410.705032, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627410.645701}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:31 INFO 140149161203520] Epoch[244] Batch[0] avg_epoch_loss=3.233399\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:31 INFO 140149161203520] #quality_metric: host=algo-1, epoch=244, batch=0 train loss <loss>=3.23339891434\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:32 INFO 140149161203520] Epoch[244] Batch[5] avg_epoch_loss=3.239745\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:32 INFO 140149161203520] #quality_metric: host=algo-1, epoch=244, batch=5 train loss <loss>=3.23974478245\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:32 INFO 140149161203520] Epoch[244] Batch [5]#011Speed: 312.68 samples/sec#011loss=3.239745\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:33 INFO 140149161203520] Epoch[244] Batch[10] avg_epoch_loss=3.144100\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:33 INFO 140149161203520] #quality_metric: host=algo-1, epoch=244, batch=10 train loss <loss>=3.02932548523\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:33 INFO 140149161203520] Epoch[244] Batch [10]#011Speed: 316.52 samples/sec#011loss=3.029325\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:33 INFO 140149161203520] processed a total of 670 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2488.3711338043213, \"sum\": 2488.3711338043213, \"min\": 2488.3711338043213}}, \"EndTime\": 1588627413.193543, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627410.705108}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:33 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=269.238381111 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:33 INFO 140149161203520] #progress_metric: host=algo-1, completed 61 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:33 INFO 140149161203520] #quality_metric: host=algo-1, epoch=244, train loss <loss>=3.14409964735\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:33 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:33 INFO 140149161203520] Epoch[245] Batch[0] avg_epoch_loss=3.287588\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:33 INFO 140149161203520] #quality_metric: host=algo-1, epoch=245, batch=0 train loss <loss>=3.28758788109\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:34 INFO 140149161203520] Epoch[245] Batch[5] avg_epoch_loss=3.241566\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:34 INFO 140149161203520] #quality_metric: host=algo-1, epoch=245, batch=5 train loss <loss>=3.24156570435\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:34 INFO 140149161203520] Epoch[245] Batch [5]#011Speed: 317.25 samples/sec#011loss=3.241566\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:35 INFO 140149161203520] Epoch[245] Batch[10] avg_epoch_loss=3.179318\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:35 INFO 140149161203520] #quality_metric: host=algo-1, epoch=245, batch=10 train loss <loss>=3.10462031364\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:35 INFO 140149161203520] Epoch[245] Batch [10]#011Speed: 320.25 samples/sec#011loss=3.104620\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:35 INFO 140149161203520] processed a total of 667 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2490.870952606201, \"sum\": 2490.870952606201, \"min\": 2490.870952606201}}, \"EndTime\": 1588627415.68495, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627413.193633}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:35 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=267.766212289 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:35 INFO 140149161203520] #progress_metric: host=algo-1, completed 61 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:35 INFO 140149161203520] #quality_metric: host=algo-1, epoch=245, train loss <loss>=3.17931779948\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:35 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:36 INFO 140149161203520] Epoch[246] Batch[0] avg_epoch_loss=3.175227\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:36 INFO 140149161203520] #quality_metric: host=algo-1, epoch=246, batch=0 train loss <loss>=3.17522740364\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:37 INFO 140149161203520] Epoch[246] Batch[5] avg_epoch_loss=3.090381\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:37 INFO 140149161203520] #quality_metric: host=algo-1, epoch=246, batch=5 train loss <loss>=3.09038130442\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:37 INFO 140149161203520] Epoch[246] Batch [5]#011Speed: 315.93 samples/sec#011loss=3.090381\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:37 INFO 140149161203520] processed a total of 609 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2285.775899887085, \"sum\": 2285.775899887085, \"min\": 2285.775899887085}}, \"EndTime\": 1588627417.971278, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627415.685017}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:37 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=266.413867328 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:37 INFO 140149161203520] #progress_metric: host=algo-1, completed 61 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:37 INFO 140149161203520] #quality_metric: host=algo-1, epoch=246, train loss <loss>=3.09039604664\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:37 INFO 140149161203520] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:38 INFO 140149161203520] Saved checkpoint to \"/opt/ml/model/state_6939bc2a-3f53-4c24-a719-27c18022ee50-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 59.94415283203125, \"sum\": 59.94415283203125, \"min\": 59.94415283203125}}, \"EndTime\": 1588627418.031922, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627417.971374}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:38 INFO 140149161203520] Epoch[247] Batch[0] avg_epoch_loss=3.369344\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:38 INFO 140149161203520] #quality_metric: host=algo-1, epoch=247, batch=0 train loss <loss>=3.36934375763\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:39 INFO 140149161203520] Epoch[247] Batch[5] avg_epoch_loss=3.203801\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:39 INFO 140149161203520] #quality_metric: host=algo-1, epoch=247, batch=5 train loss <loss>=3.20380103588\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:39 INFO 140149161203520] Epoch[247] Batch [5]#011Speed: 318.48 samples/sec#011loss=3.203801\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:40 INFO 140149161203520] Epoch[247] Batch[10] avg_epoch_loss=3.126619\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:40 INFO 140149161203520] #quality_metric: host=algo-1, epoch=247, batch=10 train loss <loss>=3.03400101662\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:40 INFO 140149161203520] Epoch[247] Batch [10]#011Speed: 319.44 samples/sec#011loss=3.034001\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:40 INFO 140149161203520] processed a total of 664 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2463.813066482544, \"sum\": 2463.813066482544, \"min\": 2463.813066482544}}, \"EndTime\": 1588627420.495885, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627418.031999}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:40 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=269.486335824 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:40 INFO 140149161203520] #progress_metric: host=algo-1, completed 62 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:40 INFO 140149161203520] #quality_metric: host=algo-1, epoch=247, train loss <loss>=3.12661920894\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:40 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:40 INFO 140149161203520] Epoch[248] Batch[0] avg_epoch_loss=3.523352\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:40 INFO 140149161203520] #quality_metric: host=algo-1, epoch=248, batch=0 train loss <loss>=3.52335238457\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:41 INFO 140149161203520] Epoch[248] Batch[5] avg_epoch_loss=3.185266\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:41 INFO 140149161203520] #quality_metric: host=algo-1, epoch=248, batch=5 train loss <loss>=3.18526601791\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:41 INFO 140149161203520] Epoch[248] Batch [5]#011Speed: 312.22 samples/sec#011loss=3.185266\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:42 INFO 140149161203520] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2268.7041759490967, \"sum\": 2268.7041759490967, \"min\": 2268.7041759490967}}, \"EndTime\": 1588627422.765176, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627420.495976}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:42 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=275.472633167 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:42 INFO 140149161203520] #progress_metric: host=algo-1, completed 62 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:42 INFO 140149161203520] #quality_metric: host=algo-1, epoch=248, train loss <loss>=3.16549725533\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:42 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:43 INFO 140149161203520] Epoch[249] Batch[0] avg_epoch_loss=3.298079\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:43 INFO 140149161203520] #quality_metric: host=algo-1, epoch=249, batch=0 train loss <loss>=3.29807925224\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:44 INFO 140149161203520] Epoch[249] Batch[5] avg_epoch_loss=3.125186\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:44 INFO 140149161203520] #quality_metric: host=algo-1, epoch=249, batch=5 train loss <loss>=3.12518552939\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:44 INFO 140149161203520] Epoch[249] Batch [5]#011Speed: 320.53 samples/sec#011loss=3.125186\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:45 INFO 140149161203520] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2283.4579944610596, \"sum\": 2283.4579944610596, \"min\": 2283.4579944610596}}, \"EndTime\": 1588627425.049212, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627422.765259}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:45 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=273.691852984 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:45 INFO 140149161203520] #progress_metric: host=algo-1, completed 62 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:45 INFO 140149161203520] #quality_metric: host=algo-1, epoch=249, train loss <loss>=3.14990634918\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:45 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:45 INFO 140149161203520] Epoch[250] Batch[0] avg_epoch_loss=3.328143\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:45 INFO 140149161203520] #quality_metric: host=algo-1, epoch=250, batch=0 train loss <loss>=3.32814264297\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:46 INFO 140149161203520] Epoch[250] Batch[5] avg_epoch_loss=3.170574\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:46 INFO 140149161203520] #quality_metric: host=algo-1, epoch=250, batch=5 train loss <loss>=3.1705741485\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:46 INFO 140149161203520] Epoch[250] Batch [5]#011Speed: 318.84 samples/sec#011loss=3.170574\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:47 INFO 140149161203520] Epoch[250] Batch[10] avg_epoch_loss=3.281257\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:47 INFO 140149161203520] #quality_metric: host=algo-1, epoch=250, batch=10 train loss <loss>=3.41407651901\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:47 INFO 140149161203520] Epoch[250] Batch [10]#011Speed: 317.70 samples/sec#011loss=3.414077\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:47 INFO 140149161203520] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2465.512990951538, \"sum\": 2465.512990951538, \"min\": 2465.512990951538}}, \"EndTime\": 1588627427.515301, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627425.049308}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:47 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=260.784009363 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:47 INFO 140149161203520] #progress_metric: host=algo-1, completed 62 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:47 INFO 140149161203520] #quality_metric: host=algo-1, epoch=250, train loss <loss>=3.28125704419\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:47 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:47 INFO 140149161203520] Epoch[251] Batch[0] avg_epoch_loss=3.130735\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:47 INFO 140149161203520] #quality_metric: host=algo-1, epoch=251, batch=0 train loss <loss>=3.1307349205\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:48 INFO 140149161203520] Epoch[251] Batch[5] avg_epoch_loss=3.228177\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:48 INFO 140149161203520] #quality_metric: host=algo-1, epoch=251, batch=5 train loss <loss>=3.22817718983\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:48 INFO 140149161203520] Epoch[251] Batch [5]#011Speed: 320.72 samples/sec#011loss=3.228177\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:49 INFO 140149161203520] Epoch[251] Batch[10] avg_epoch_loss=3.151856\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:49 INFO 140149161203520] #quality_metric: host=algo-1, epoch=251, batch=10 train loss <loss>=3.06027035713\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:49 INFO 140149161203520] Epoch[251] Batch [10]#011Speed: 316.45 samples/sec#011loss=3.060270\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:49 INFO 140149161203520] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2459.8958492279053, \"sum\": 2459.8958492279053, \"min\": 2459.8958492279053}}, \"EndTime\": 1588627429.975813, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627427.515387}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:49 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=261.379880513 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:49 INFO 140149161203520] #progress_metric: host=algo-1, completed 63 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:49 INFO 140149161203520] #quality_metric: host=algo-1, epoch=251, train loss <loss>=3.15185590224\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:49 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:50 INFO 140149161203520] Epoch[252] Batch[0] avg_epoch_loss=3.038408\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:50 INFO 140149161203520] #quality_metric: host=algo-1, epoch=252, batch=0 train loss <loss>=3.038408041\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:51 INFO 140149161203520] Epoch[252] Batch[5] avg_epoch_loss=3.157385\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:51 INFO 140149161203520] #quality_metric: host=algo-1, epoch=252, batch=5 train loss <loss>=3.15738523006\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:51 INFO 140149161203520] Epoch[252] Batch [5]#011Speed: 318.05 samples/sec#011loss=3.157385\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:52 INFO 140149161203520] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2250.2360343933105, \"sum\": 2250.2360343933105, \"min\": 2250.2360343933105}}, \"EndTime\": 1588627432.226567, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627429.975899}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:52 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=283.509201188 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:52 INFO 140149161203520] #progress_metric: host=algo-1, completed 63 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:52 INFO 140149161203520] #quality_metric: host=algo-1, epoch=252, train loss <loss>=3.18651130199\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:52 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:52 INFO 140149161203520] Epoch[253] Batch[0] avg_epoch_loss=3.133428\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:52 INFO 140149161203520] #quality_metric: host=algo-1, epoch=253, batch=0 train loss <loss>=3.13342785835\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:53 INFO 140149161203520] Epoch[253] Batch[5] avg_epoch_loss=3.125986\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:53 INFO 140149161203520] #quality_metric: host=algo-1, epoch=253, batch=5 train loss <loss>=3.12598574162\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:53 INFO 140149161203520] Epoch[253] Batch [5]#011Speed: 319.35 samples/sec#011loss=3.125986\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:54 INFO 140149161203520] Epoch[253] Batch[10] avg_epoch_loss=3.069334\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:54 INFO 140149161203520] #quality_metric: host=algo-1, epoch=253, batch=10 train loss <loss>=3.00135164261\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:54 INFO 140149161203520] Epoch[253] Batch [10]#011Speed: 320.56 samples/sec#011loss=3.001352\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:54 INFO 140149161203520] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2469.489097595215, \"sum\": 2469.489097595215, \"min\": 2469.489097595215}}, \"EndTime\": 1588627434.696646, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627432.22666}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:54 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=266.843858316 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:54 INFO 140149161203520] #progress_metric: host=algo-1, completed 63 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:54 INFO 140149161203520] #quality_metric: host=algo-1, epoch=253, train loss <loss>=3.06933387843\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:54 INFO 140149161203520] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:54 INFO 140149161203520] Saved checkpoint to \"/opt/ml/model/state_2415764f-c307-4aa0-a9d0-4324c3656167-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 59.741973876953125, \"sum\": 59.741973876953125, \"min\": 59.741973876953125}}, \"EndTime\": 1588627434.756984, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627434.696726}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:55 INFO 140149161203520] Epoch[254] Batch[0] avg_epoch_loss=3.087696\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:55 INFO 140149161203520] #quality_metric: host=algo-1, epoch=254, batch=0 train loss <loss>=3.08769631386\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:56 INFO 140149161203520] Epoch[254] Batch[5] avg_epoch_loss=3.147295\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:56 INFO 140149161203520] #quality_metric: host=algo-1, epoch=254, batch=5 train loss <loss>=3.14729475975\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:56 INFO 140149161203520] Epoch[254] Batch [5]#011Speed: 318.41 samples/sec#011loss=3.147295\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:57 INFO 140149161203520] Epoch[254] Batch[10] avg_epoch_loss=3.074606\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:57 INFO 140149161203520] #quality_metric: host=algo-1, epoch=254, batch=10 train loss <loss>=2.98737897873\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:57 INFO 140149161203520] Epoch[254] Batch [10]#011Speed: 320.31 samples/sec#011loss=2.987379\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:57 INFO 140149161203520] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2449.6240615844727, \"sum\": 2449.6240615844727, \"min\": 2449.6240615844727}}, \"EndTime\": 1588627437.206742, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627434.757056}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:57 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=267.374435863 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:57 INFO 140149161203520] #progress_metric: host=algo-1, completed 63 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:57 INFO 140149161203520] #quality_metric: host=algo-1, epoch=254, train loss <loss>=3.07460576838\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:57 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:57 INFO 140149161203520] Epoch[255] Batch[0] avg_epoch_loss=3.353824\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:57 INFO 140149161203520] #quality_metric: host=algo-1, epoch=255, batch=0 train loss <loss>=3.35382390022\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:58 INFO 140149161203520] Epoch[255] Batch[5] avg_epoch_loss=3.129380\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:58 INFO 140149161203520] #quality_metric: host=algo-1, epoch=255, batch=5 train loss <loss>=3.12938010693\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:58 INFO 140149161203520] Epoch[255] Batch [5]#011Speed: 322.05 samples/sec#011loss=3.129380\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:59 INFO 140149161203520] Epoch[255] Batch[10] avg_epoch_loss=3.099900\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:59 INFO 140149161203520] #quality_metric: host=algo-1, epoch=255, batch=10 train loss <loss>=3.06452307701\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:59 INFO 140149161203520] Epoch[255] Batch [10]#011Speed: 315.26 samples/sec#011loss=3.064523\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:59 INFO 140149161203520] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2456.972122192383, \"sum\": 2456.972122192383, \"min\": 2456.972122192383}}, \"EndTime\": 1588627439.664309, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627437.206825}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:59 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=264.13229246 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:59 INFO 140149161203520] #progress_metric: host=algo-1, completed 64 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:59 INFO 140149161203520] #quality_metric: host=algo-1, epoch=255, train loss <loss>=3.09989963878\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:23:59 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:00 INFO 140149161203520] Epoch[256] Batch[0] avg_epoch_loss=3.048821\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:00 INFO 140149161203520] #quality_metric: host=algo-1, epoch=256, batch=0 train loss <loss>=3.04882073402\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:01 INFO 140149161203520] Epoch[256] Batch[5] avg_epoch_loss=3.131458\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:01 INFO 140149161203520] #quality_metric: host=algo-1, epoch=256, batch=5 train loss <loss>=3.13145832221\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:01 INFO 140149161203520] Epoch[256] Batch [5]#011Speed: 322.41 samples/sec#011loss=3.131458\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:01 INFO 140149161203520] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2302.776098251343, \"sum\": 2302.776098251343, \"min\": 2302.776098251343}}, \"EndTime\": 1588627441.96762, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627439.664399}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:01 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=274.438730273 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:01 INFO 140149161203520] #progress_metric: host=algo-1, completed 64 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:01 INFO 140149161203520] #quality_metric: host=algo-1, epoch=256, train loss <loss>=3.11604020596\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:01 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:02 INFO 140149161203520] Epoch[257] Batch[0] avg_epoch_loss=3.361416\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:02 INFO 140149161203520] #quality_metric: host=algo-1, epoch=257, batch=0 train loss <loss>=3.36141586304\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:03 INFO 140149161203520] Epoch[257] Batch[5] avg_epoch_loss=3.208489\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:03 INFO 140149161203520] #quality_metric: host=algo-1, epoch=257, batch=5 train loss <loss>=3.20848910014\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:03 INFO 140149161203520] Epoch[257] Batch [5]#011Speed: 322.75 samples/sec#011loss=3.208489\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:04 INFO 140149161203520] Epoch[257] Batch[10] avg_epoch_loss=3.259110\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:04 INFO 140149161203520] #quality_metric: host=algo-1, epoch=257, batch=10 train loss <loss>=3.31985464096\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:04 INFO 140149161203520] Epoch[257] Batch [10]#011Speed: 312.41 samples/sec#011loss=3.319855\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:04 INFO 140149161203520] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2465.5208587646484, \"sum\": 2465.5208587646484, \"min\": 2465.5208587646484}}, \"EndTime\": 1588627444.433767, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627441.967691}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:04 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=265.244116658 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:04 INFO 140149161203520] #progress_metric: host=algo-1, completed 64 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:04 INFO 140149161203520] #quality_metric: host=algo-1, epoch=257, train loss <loss>=3.25910980051\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:04 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:04 INFO 140149161203520] Epoch[258] Batch[0] avg_epoch_loss=3.132018\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:04 INFO 140149161203520] #quality_metric: host=algo-1, epoch=258, batch=0 train loss <loss>=3.13201808929\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:05 INFO 140149161203520] Epoch[258] Batch[5] avg_epoch_loss=3.273894\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:05 INFO 140149161203520] #quality_metric: host=algo-1, epoch=258, batch=5 train loss <loss>=3.27389383316\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:05 INFO 140149161203520] Epoch[258] Batch [5]#011Speed: 326.09 samples/sec#011loss=3.273894\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:06 INFO 140149161203520] Epoch[258] Batch[10] avg_epoch_loss=3.228916\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:06 INFO 140149161203520] #quality_metric: host=algo-1, epoch=258, batch=10 train loss <loss>=3.17494311333\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:06 INFO 140149161203520] Epoch[258] Batch [10]#011Speed: 321.87 samples/sec#011loss=3.174943\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:06 INFO 140149161203520] processed a total of 667 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2440.316915512085, \"sum\": 2440.316915512085, \"min\": 2440.316915512085}}, \"EndTime\": 1588627446.874606, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627444.433859}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:06 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=273.310602526 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:06 INFO 140149161203520] #progress_metric: host=algo-1, completed 64 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:06 INFO 140149161203520] #quality_metric: host=algo-1, epoch=258, train loss <loss>=3.22891623324\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:06 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:07 INFO 140149161203520] Epoch[259] Batch[0] avg_epoch_loss=3.396469\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:07 INFO 140149161203520] #quality_metric: host=algo-1, epoch=259, batch=0 train loss <loss>=3.39646935463\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:08 INFO 140149161203520] Epoch[259] Batch[5] avg_epoch_loss=3.308154\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:08 INFO 140149161203520] #quality_metric: host=algo-1, epoch=259, batch=5 train loss <loss>=3.3081540664\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:08 INFO 140149161203520] Epoch[259] Batch [5]#011Speed: 312.36 samples/sec#011loss=3.308154\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:09 INFO 140149161203520] Epoch[259] Batch[10] avg_epoch_loss=3.306278\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:09 INFO 140149161203520] #quality_metric: host=algo-1, epoch=259, batch=10 train loss <loss>=3.30402750969\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:09 INFO 140149161203520] Epoch[259] Batch [10]#011Speed: 320.48 samples/sec#011loss=3.304028\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:09 INFO 140149161203520] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2495.474100112915, \"sum\": 2495.474100112915, \"min\": 2495.474100112915}}, \"EndTime\": 1588627449.370687, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627446.874697}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:09 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=264.465258658 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:09 INFO 140149161203520] #progress_metric: host=algo-1, completed 65 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:09 INFO 140149161203520] #quality_metric: host=algo-1, epoch=259, train loss <loss>=3.30627835881\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:09 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:09 INFO 140149161203520] Epoch[260] Batch[0] avg_epoch_loss=3.151110\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:09 INFO 140149161203520] #quality_metric: host=algo-1, epoch=260, batch=0 train loss <loss>=3.15110969543\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:10 INFO 140149161203520] Epoch[260] Batch[5] avg_epoch_loss=3.201114\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:10 INFO 140149161203520] #quality_metric: host=algo-1, epoch=260, batch=5 train loss <loss>=3.2011141777\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:10 INFO 140149161203520] Epoch[260] Batch [5]#011Speed: 322.69 samples/sec#011loss=3.201114\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:11 INFO 140149161203520] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2237.321138381958, \"sum\": 2237.321138381958, \"min\": 2237.321138381958}}, \"EndTime\": 1588627451.608566, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627449.370771}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:11 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=282.910460197 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:11 INFO 140149161203520] #progress_metric: host=algo-1, completed 65 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:11 INFO 140149161203520] #quality_metric: host=algo-1, epoch=260, train loss <loss>=3.18199453354\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:11 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:12 INFO 140149161203520] Epoch[261] Batch[0] avg_epoch_loss=2.914828\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:12 INFO 140149161203520] #quality_metric: host=algo-1, epoch=261, batch=0 train loss <loss>=2.91482782364\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:13 INFO 140149161203520] Epoch[261] Batch[5] avg_epoch_loss=3.176818\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:13 INFO 140149161203520] #quality_metric: host=algo-1, epoch=261, batch=5 train loss <loss>=3.17681845029\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:13 INFO 140149161203520] Epoch[261] Batch [5]#011Speed: 323.68 samples/sec#011loss=3.176818\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:14 INFO 140149161203520] Epoch[261] Batch[10] avg_epoch_loss=3.131003\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:14 INFO 140149161203520] #quality_metric: host=algo-1, epoch=261, batch=10 train loss <loss>=3.07602376938\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:14 INFO 140149161203520] Epoch[261] Batch [10]#011Speed: 321.64 samples/sec#011loss=3.076024\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:14 INFO 140149161203520] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2460.2320194244385, \"sum\": 2460.2320194244385, \"min\": 2460.2320194244385}}, \"EndTime\": 1588627454.069349, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627451.608661}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:14 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=260.530670933 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:14 INFO 140149161203520] #progress_metric: host=algo-1, completed 65 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:14 INFO 140149161203520] #quality_metric: host=algo-1, epoch=261, train loss <loss>=3.13100268624\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:14 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:14 INFO 140149161203520] Epoch[262] Batch[0] avg_epoch_loss=2.985012\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:14 INFO 140149161203520] #quality_metric: host=algo-1, epoch=262, batch=0 train loss <loss>=2.98501229286\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:15 INFO 140149161203520] Epoch[262] Batch[5] avg_epoch_loss=3.102053\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:15 INFO 140149161203520] #quality_metric: host=algo-1, epoch=262, batch=5 train loss <loss>=3.10205344359\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:15 INFO 140149161203520] Epoch[262] Batch [5]#011Speed: 320.07 samples/sec#011loss=3.102053\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:16 INFO 140149161203520] Epoch[262] Batch[10] avg_epoch_loss=3.165202\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:16 INFO 140149161203520] #quality_metric: host=algo-1, epoch=262, batch=10 train loss <loss>=3.24097919464\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:16 INFO 140149161203520] Epoch[262] Batch [10]#011Speed: 315.36 samples/sec#011loss=3.240979\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:16 INFO 140149161203520] processed a total of 681 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2480.5049896240234, \"sum\": 2480.5049896240234, \"min\": 2480.5049896240234}}, \"EndTime\": 1588627456.550376, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627454.069438}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:16 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=274.526490126 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:16 INFO 140149161203520] #progress_metric: host=algo-1, completed 65 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:16 INFO 140149161203520] #quality_metric: host=algo-1, epoch=262, train loss <loss>=3.16520151225\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:16 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:17 INFO 140149161203520] Epoch[263] Batch[0] avg_epoch_loss=3.168319\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:17 INFO 140149161203520] #quality_metric: host=algo-1, epoch=263, batch=0 train loss <loss>=3.16831851006\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:18 INFO 140149161203520] Epoch[263] Batch[5] avg_epoch_loss=3.129924\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:18 INFO 140149161203520] #quality_metric: host=algo-1, epoch=263, batch=5 train loss <loss>=3.12992433707\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:18 INFO 140149161203520] Epoch[263] Batch [5]#011Speed: 324.85 samples/sec#011loss=3.129924\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:19 INFO 140149161203520] Epoch[263] Batch[10] avg_epoch_loss=3.198615\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:19 INFO 140149161203520] #quality_metric: host=algo-1, epoch=263, batch=10 train loss <loss>=3.28104400635\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:19 INFO 140149161203520] Epoch[263] Batch [10]#011Speed: 323.07 samples/sec#011loss=3.281044\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:19 INFO 140149161203520] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2500.663995742798, \"sum\": 2500.663995742798, \"min\": 2500.663995742798}}, \"EndTime\": 1588627459.051576, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627456.550465}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:19 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=260.317330102 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:19 INFO 140149161203520] #progress_metric: host=algo-1, completed 66 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:19 INFO 140149161203520] #quality_metric: host=algo-1, epoch=263, train loss <loss>=3.19861509583\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:19 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:19 INFO 140149161203520] Epoch[264] Batch[0] avg_epoch_loss=3.189282\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:19 INFO 140149161203520] #quality_metric: host=algo-1, epoch=264, batch=0 train loss <loss>=3.18928170204\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:20 INFO 140149161203520] Epoch[264] Batch[5] avg_epoch_loss=3.181651\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:20 INFO 140149161203520] #quality_metric: host=algo-1, epoch=264, batch=5 train loss <loss>=3.18165055911\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:20 INFO 140149161203520] Epoch[264] Batch [5]#011Speed: 325.09 samples/sec#011loss=3.181651\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:21 INFO 140149161203520] Epoch[264] Batch[10] avg_epoch_loss=3.140001\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:21 INFO 140149161203520] #quality_metric: host=algo-1, epoch=264, batch=10 train loss <loss>=3.09002075195\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:21 INFO 140149161203520] Epoch[264] Batch [10]#011Speed: 315.87 samples/sec#011loss=3.090021\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:21 INFO 140149161203520] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2470.4339504241943, \"sum\": 2470.4339504241943, \"min\": 2470.4339504241943}}, \"EndTime\": 1588627461.522548, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627459.051667}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:21 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=263.907133403 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:21 INFO 140149161203520] #progress_metric: host=algo-1, completed 66 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:21 INFO 140149161203520] #quality_metric: host=algo-1, epoch=264, train loss <loss>=3.14000064676\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:21 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:22 INFO 140149161203520] Epoch[265] Batch[0] avg_epoch_loss=3.059269\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:22 INFO 140149161203520] #quality_metric: host=algo-1, epoch=265, batch=0 train loss <loss>=3.05926895142\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:23 INFO 140149161203520] Epoch[265] Batch[5] avg_epoch_loss=3.119616\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:23 INFO 140149161203520] #quality_metric: host=algo-1, epoch=265, batch=5 train loss <loss>=3.11961583296\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:23 INFO 140149161203520] Epoch[265] Batch [5]#011Speed: 317.26 samples/sec#011loss=3.119616\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:24 INFO 140149161203520] Epoch[265] Batch[10] avg_epoch_loss=3.231663\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:24 INFO 140149161203520] #quality_metric: host=algo-1, epoch=265, batch=10 train loss <loss>=3.36611971855\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:24 INFO 140149161203520] Epoch[265] Batch [10]#011Speed: 321.03 samples/sec#011loss=3.366120\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:24 INFO 140149161203520] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2517.5018310546875, \"sum\": 2517.5018310546875, \"min\": 2517.5018310546875}}, \"EndTime\": 1588627464.040602, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627461.522639}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:24 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=258.180556088 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:24 INFO 140149161203520] #progress_metric: host=algo-1, completed 66 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:24 INFO 140149161203520] #quality_metric: host=algo-1, epoch=265, train loss <loss>=3.23166305369\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:24 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:24 INFO 140149161203520] Epoch[266] Batch[0] avg_epoch_loss=3.024924\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:24 INFO 140149161203520] #quality_metric: host=algo-1, epoch=266, batch=0 train loss <loss>=3.02492403984\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:25 INFO 140149161203520] Epoch[266] Batch[5] avg_epoch_loss=3.198639\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:25 INFO 140149161203520] #quality_metric: host=algo-1, epoch=266, batch=5 train loss <loss>=3.1986391147\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:25 INFO 140149161203520] Epoch[266] Batch [5]#011Speed: 323.64 samples/sec#011loss=3.198639\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:26 INFO 140149161203520] Epoch[266] Batch[10] avg_epoch_loss=3.219442\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:26 INFO 140149161203520] #quality_metric: host=algo-1, epoch=266, batch=10 train loss <loss>=3.24440493584\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:26 INFO 140149161203520] Epoch[266] Batch [10]#011Speed: 320.10 samples/sec#011loss=3.244405\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:26 INFO 140149161203520] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2433.424949645996, \"sum\": 2433.424949645996, \"min\": 2433.424949645996}}, \"EndTime\": 1588627466.474571, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627464.040682}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:26 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=266.6874982 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:26 INFO 140149161203520] #progress_metric: host=algo-1, completed 66 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:26 INFO 140149161203520] #quality_metric: host=algo-1, epoch=266, train loss <loss>=3.21944176067\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:26 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:26 INFO 140149161203520] Epoch[267] Batch[0] avg_epoch_loss=3.296828\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:26 INFO 140149161203520] #quality_metric: host=algo-1, epoch=267, batch=0 train loss <loss>=3.29682803154\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:27 INFO 140149161203520] Epoch[267] Batch[5] avg_epoch_loss=3.204041\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:27 INFO 140149161203520] #quality_metric: host=algo-1, epoch=267, batch=5 train loss <loss>=3.20404144128\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:27 INFO 140149161203520] Epoch[267] Batch [5]#011Speed: 320.71 samples/sec#011loss=3.204041\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:28 INFO 140149161203520] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2238.255023956299, \"sum\": 2238.255023956299, \"min\": 2238.255023956299}}, \"EndTime\": 1588627468.713459, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627466.474663}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:28 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=285.025657642 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:28 INFO 140149161203520] #progress_metric: host=algo-1, completed 67 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:28 INFO 140149161203520] #quality_metric: host=algo-1, epoch=267, train loss <loss>=3.18058662415\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:28 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:29 INFO 140149161203520] Epoch[268] Batch[0] avg_epoch_loss=3.139065\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:29 INFO 140149161203520] #quality_metric: host=algo-1, epoch=268, batch=0 train loss <loss>=3.13906526566\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:30 INFO 140149161203520] Epoch[268] Batch[5] avg_epoch_loss=3.141655\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:30 INFO 140149161203520] #quality_metric: host=algo-1, epoch=268, batch=5 train loss <loss>=3.1416545709\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:30 INFO 140149161203520] Epoch[268] Batch [5]#011Speed: 326.82 samples/sec#011loss=3.141655\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:31 INFO 140149161203520] Epoch[268] Batch[10] avg_epoch_loss=3.193364\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:31 INFO 140149161203520] #quality_metric: host=algo-1, epoch=268, batch=10 train loss <loss>=3.25541620255\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:31 INFO 140149161203520] Epoch[268] Batch [10]#011Speed: 324.19 samples/sec#011loss=3.255416\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:31 INFO 140149161203520] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2437.4749660491943, \"sum\": 2437.4749660491943, \"min\": 2437.4749660491943}}, \"EndTime\": 1588627471.151589, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627468.713558}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:31 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=267.886456961 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:31 INFO 140149161203520] #progress_metric: host=algo-1, completed 67 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:31 INFO 140149161203520] #quality_metric: host=algo-1, epoch=268, train loss <loss>=3.19336440346\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:31 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:31 INFO 140149161203520] Epoch[269] Batch[0] avg_epoch_loss=3.161703\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:31 INFO 140149161203520] #quality_metric: host=algo-1, epoch=269, batch=0 train loss <loss>=3.16170310974\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:32 INFO 140149161203520] Epoch[269] Batch[5] avg_epoch_loss=3.160815\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:32 INFO 140149161203520] #quality_metric: host=algo-1, epoch=269, batch=5 train loss <loss>=3.16081456343\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:32 INFO 140149161203520] Epoch[269] Batch [5]#011Speed: 323.61 samples/sec#011loss=3.160815\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:33 INFO 140149161203520] Epoch[269] Batch[10] avg_epoch_loss=3.150693\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:33 INFO 140149161203520] #quality_metric: host=algo-1, epoch=269, batch=10 train loss <loss>=3.13854618073\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:33 INFO 140149161203520] Epoch[269] Batch [10]#011Speed: 318.79 samples/sec#011loss=3.138546\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:33 INFO 140149161203520] processed a total of 687 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2497.8959560394287, \"sum\": 2497.8959560394287, \"min\": 2497.8959560394287}}, \"EndTime\": 1588627473.649995, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627471.151676}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:33 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=275.017165196 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:33 INFO 140149161203520] #progress_metric: host=algo-1, completed 67 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:33 INFO 140149161203520] #quality_metric: host=algo-1, epoch=269, train loss <loss>=3.15069257129\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:33 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:34 INFO 140149161203520] Epoch[270] Batch[0] avg_epoch_loss=3.346318\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:34 INFO 140149161203520] #quality_metric: host=algo-1, epoch=270, batch=0 train loss <loss>=3.34631824493\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:35 INFO 140149161203520] Epoch[270] Batch[5] avg_epoch_loss=3.190952\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:35 INFO 140149161203520] #quality_metric: host=algo-1, epoch=270, batch=5 train loss <loss>=3.19095218182\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:35 INFO 140149161203520] Epoch[270] Batch [5]#011Speed: 323.65 samples/sec#011loss=3.190952\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:35 INFO 140149161203520] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2229.8600673675537, \"sum\": 2229.8600673675537, \"min\": 2229.8600673675537}}, \"EndTime\": 1588627475.880402, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627473.650085}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:35 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=276.681730308 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:35 INFO 140149161203520] #progress_metric: host=algo-1, completed 67 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:35 INFO 140149161203520] #quality_metric: host=algo-1, epoch=270, train loss <loss>=3.17822155952\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:35 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:36 INFO 140149161203520] Epoch[271] Batch[0] avg_epoch_loss=3.144975\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:36 INFO 140149161203520] #quality_metric: host=algo-1, epoch=271, batch=0 train loss <loss>=3.14497470856\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:37 INFO 140149161203520] Epoch[271] Batch[5] avg_epoch_loss=3.140207\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:37 INFO 140149161203520] #quality_metric: host=algo-1, epoch=271, batch=5 train loss <loss>=3.14020733039\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:37 INFO 140149161203520] Epoch[271] Batch [5]#011Speed: 317.08 samples/sec#011loss=3.140207\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:38 INFO 140149161203520] Epoch[271] Batch[10] avg_epoch_loss=3.158214\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:38 INFO 140149161203520] #quality_metric: host=algo-1, epoch=271, batch=10 train loss <loss>=3.17982106209\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:38 INFO 140149161203520] Epoch[271] Batch [10]#011Speed: 302.94 samples/sec#011loss=3.179821\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:38 INFO 140149161203520] processed a total of 679 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2588.804006576538, \"sum\": 2588.804006576538, \"min\": 2588.804006576538}}, \"EndTime\": 1588627478.469802, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627475.880501}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:38 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=262.270620895 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:38 INFO 140149161203520] #progress_metric: host=algo-1, completed 68 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:38 INFO 140149161203520] #quality_metric: host=algo-1, epoch=271, train loss <loss>=3.15821357207\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:38 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:38 INFO 140149161203520] Epoch[272] Batch[0] avg_epoch_loss=2.908642\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:38 INFO 140149161203520] #quality_metric: host=algo-1, epoch=272, batch=0 train loss <loss>=2.90864157677\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:39 INFO 140149161203520] Epoch[272] Batch[5] avg_epoch_loss=3.179200\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:39 INFO 140149161203520] #quality_metric: host=algo-1, epoch=272, batch=5 train loss <loss>=3.17919985453\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:39 INFO 140149161203520] Epoch[272] Batch [5]#011Speed: 314.85 samples/sec#011loss=3.179200\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:40 INFO 140149161203520] Epoch[272] Batch[10] avg_epoch_loss=3.097034\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:40 INFO 140149161203520] #quality_metric: host=algo-1, epoch=272, batch=10 train loss <loss>=2.99843564034\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:40 INFO 140149161203520] Epoch[272] Batch [10]#011Speed: 317.50 samples/sec#011loss=2.998436\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:40 INFO 140149161203520] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2482.517957687378, \"sum\": 2482.517957687378, \"min\": 2482.517957687378}}, \"EndTime\": 1588627480.952911, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627478.469886}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:40 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=259.402201306 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:40 INFO 140149161203520] #progress_metric: host=algo-1, completed 68 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:40 INFO 140149161203520] #quality_metric: host=algo-1, epoch=272, train loss <loss>=3.09703430262\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:40 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:41 INFO 140149161203520] Epoch[273] Batch[0] avg_epoch_loss=3.256029\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:41 INFO 140149161203520] #quality_metric: host=algo-1, epoch=273, batch=0 train loss <loss>=3.25602912903\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:42 INFO 140149161203520] Epoch[273] Batch[5] avg_epoch_loss=3.129652\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:42 INFO 140149161203520] #quality_metric: host=algo-1, epoch=273, batch=5 train loss <loss>=3.129652222\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:42 INFO 140149161203520] Epoch[273] Batch [5]#011Speed: 318.82 samples/sec#011loss=3.129652\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:43 INFO 140149161203520] processed a total of 606 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2253.901958465576, \"sum\": 2253.901958465576, \"min\": 2253.901958465576}}, \"EndTime\": 1588627483.207389, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627480.952982}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:43 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=268.850227128 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:43 INFO 140149161203520] #progress_metric: host=algo-1, completed 68 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:43 INFO 140149161203520] #quality_metric: host=algo-1, epoch=273, train loss <loss>=3.22162210941\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:43 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:43 INFO 140149161203520] Epoch[274] Batch[0] avg_epoch_loss=3.203849\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:43 INFO 140149161203520] #quality_metric: host=algo-1, epoch=274, batch=0 train loss <loss>=3.20384883881\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:44 INFO 140149161203520] Epoch[274] Batch[5] avg_epoch_loss=3.074396\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:44 INFO 140149161203520] #quality_metric: host=algo-1, epoch=274, batch=5 train loss <loss>=3.07439617316\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:44 INFO 140149161203520] Epoch[274] Batch [5]#011Speed: 324.79 samples/sec#011loss=3.074396\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:45 INFO 140149161203520] Epoch[274] Batch[10] avg_epoch_loss=3.189553\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:45 INFO 140149161203520] #quality_metric: host=algo-1, epoch=274, batch=10 train loss <loss>=3.32774057388\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:45 INFO 140149161203520] Epoch[274] Batch [10]#011Speed: 323.73 samples/sec#011loss=3.327741\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:45 INFO 140149161203520] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2446.2978839874268, \"sum\": 2446.2978839874268, \"min\": 2446.2978839874268}}, \"EndTime\": 1588627485.654257, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627483.207487}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:45 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=262.831838688 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:45 INFO 140149161203520] #progress_metric: host=algo-1, completed 68 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:45 INFO 140149161203520] #quality_metric: host=algo-1, epoch=274, train loss <loss>=3.18955271894\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:45 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:46 INFO 140149161203520] Epoch[275] Batch[0] avg_epoch_loss=3.278130\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:46 INFO 140149161203520] #quality_metric: host=algo-1, epoch=275, batch=0 train loss <loss>=3.27813005447\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:47 INFO 140149161203520] Epoch[275] Batch[5] avg_epoch_loss=3.240880\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:47 INFO 140149161203520] #quality_metric: host=algo-1, epoch=275, batch=5 train loss <loss>=3.24087953568\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:47 INFO 140149161203520] Epoch[275] Batch [5]#011Speed: 311.61 samples/sec#011loss=3.240880\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:48 INFO 140149161203520] Epoch[275] Batch[10] avg_epoch_loss=3.248289\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:48 INFO 140149161203520] #quality_metric: host=algo-1, epoch=275, batch=10 train loss <loss>=3.2571805954\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:48 INFO 140149161203520] Epoch[275] Batch [10]#011Speed: 318.51 samples/sec#011loss=3.257181\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:48 INFO 140149161203520] processed a total of 667 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2536.0119342803955, \"sum\": 2536.0119342803955, \"min\": 2536.0119342803955}}, \"EndTime\": 1588627488.190841, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627485.654349}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:48 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=262.998523407 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:48 INFO 140149161203520] #progress_metric: host=algo-1, completed 69 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:48 INFO 140149161203520] #quality_metric: host=algo-1, epoch=275, train loss <loss>=3.24828910828\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:48 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:48 INFO 140149161203520] Epoch[276] Batch[0] avg_epoch_loss=3.128836\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:48 INFO 140149161203520] #quality_metric: host=algo-1, epoch=276, batch=0 train loss <loss>=3.12883639336\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:49 INFO 140149161203520] Epoch[276] Batch[5] avg_epoch_loss=3.173942\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:49 INFO 140149161203520] #quality_metric: host=algo-1, epoch=276, batch=5 train loss <loss>=3.17394157251\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:49 INFO 140149161203520] Epoch[276] Batch [5]#011Speed: 318.82 samples/sec#011loss=3.173942\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:50 INFO 140149161203520] Epoch[276] Batch[10] avg_epoch_loss=3.094526\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:50 INFO 140149161203520] #quality_metric: host=algo-1, epoch=276, batch=10 train loss <loss>=2.99922661781\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:50 INFO 140149161203520] Epoch[276] Batch [10]#011Speed: 322.36 samples/sec#011loss=2.999227\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:50 INFO 140149161203520] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2453.3069133758545, \"sum\": 2453.3069133758545, \"min\": 2453.3069133758545}}, \"EndTime\": 1588627490.644797, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627488.190921}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:50 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=266.565253744 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:50 INFO 140149161203520] #progress_metric: host=algo-1, completed 69 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:50 INFO 140149161203520] #quality_metric: host=algo-1, epoch=276, train loss <loss>=3.09452568401\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:50 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:51 INFO 140149161203520] Epoch[277] Batch[0] avg_epoch_loss=3.114530\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:51 INFO 140149161203520] #quality_metric: host=algo-1, epoch=277, batch=0 train loss <loss>=3.11453032494\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:52 INFO 140149161203520] Epoch[277] Batch[5] avg_epoch_loss=3.190841\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:52 INFO 140149161203520] #quality_metric: host=algo-1, epoch=277, batch=5 train loss <loss>=3.19084056218\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:52 INFO 140149161203520] Epoch[277] Batch [5]#011Speed: 314.79 samples/sec#011loss=3.190841\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:53 INFO 140149161203520] Epoch[277] Batch[10] avg_epoch_loss=3.100594\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:53 INFO 140149161203520] #quality_metric: host=algo-1, epoch=277, batch=10 train loss <loss>=2.99229717255\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:53 INFO 140149161203520] Epoch[277] Batch [10]#011Speed: 319.38 samples/sec#011loss=2.992297\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:53 INFO 140149161203520] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2484.981060028076, \"sum\": 2484.981060028076, \"min\": 2484.981060028076}}, \"EndTime\": 1588627493.130284, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627490.644882}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:53 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=261.155762852 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:53 INFO 140149161203520] #progress_metric: host=algo-1, completed 69 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:53 INFO 140149161203520] #quality_metric: host=algo-1, epoch=277, train loss <loss>=3.10059356689\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:53 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:53 INFO 140149161203520] Epoch[278] Batch[0] avg_epoch_loss=3.265493\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:53 INFO 140149161203520] #quality_metric: host=algo-1, epoch=278, batch=0 train loss <loss>=3.26549291611\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:54 INFO 140149161203520] Epoch[278] Batch[5] avg_epoch_loss=3.076237\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:54 INFO 140149161203520] #quality_metric: host=algo-1, epoch=278, batch=5 train loss <loss>=3.07623712222\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:54 INFO 140149161203520] Epoch[278] Batch [5]#011Speed: 319.85 samples/sec#011loss=3.076237\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:55 INFO 140149161203520] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2255.948066711426, \"sum\": 2255.948066711426, \"min\": 2255.948066711426}}, \"EndTime\": 1588627495.386806, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627493.130367}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:55 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=282.350780446 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:55 INFO 140149161203520] #progress_metric: host=algo-1, completed 69 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:55 INFO 140149161203520] #quality_metric: host=algo-1, epoch=278, train loss <loss>=3.07264626026\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:55 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:55 INFO 140149161203520] Epoch[279] Batch[0] avg_epoch_loss=3.160931\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:55 INFO 140149161203520] #quality_metric: host=algo-1, epoch=279, batch=0 train loss <loss>=3.1609313488\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:56 INFO 140149161203520] Epoch[279] Batch[5] avg_epoch_loss=3.131382\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:56 INFO 140149161203520] #quality_metric: host=algo-1, epoch=279, batch=5 train loss <loss>=3.13138175011\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:56 INFO 140149161203520] Epoch[279] Batch [5]#011Speed: 316.13 samples/sec#011loss=3.131382\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:57 INFO 140149161203520] Epoch[279] Batch[10] avg_epoch_loss=3.230959\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:57 INFO 140149161203520] #quality_metric: host=algo-1, epoch=279, batch=10 train loss <loss>=3.35045070648\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:57 INFO 140149161203520] Epoch[279] Batch [10]#011Speed: 316.12 samples/sec#011loss=3.350451\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:57 INFO 140149161203520] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2479.004144668579, \"sum\": 2479.004144668579, \"min\": 2479.004144668579}}, \"EndTime\": 1588627497.866404, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627495.386877}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:57 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=266.625378658 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:57 INFO 140149161203520] #progress_metric: host=algo-1, completed 70 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:57 INFO 140149161203520] #quality_metric: host=algo-1, epoch=279, train loss <loss>=3.23095854846\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:57 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:58 INFO 140149161203520] Epoch[280] Batch[0] avg_epoch_loss=3.239429\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:58 INFO 140149161203520] #quality_metric: host=algo-1, epoch=280, batch=0 train loss <loss>=3.23942923546\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:59 INFO 140149161203520] Epoch[280] Batch[5] avg_epoch_loss=3.186120\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:59 INFO 140149161203520] #quality_metric: host=algo-1, epoch=280, batch=5 train loss <loss>=3.18611991405\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:24:59 INFO 140149161203520] Epoch[280] Batch [5]#011Speed: 321.42 samples/sec#011loss=3.186120\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:00 INFO 140149161203520] processed a total of 609 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2245.8720207214355, \"sum\": 2245.8720207214355, \"min\": 2245.8720207214355}}, \"EndTime\": 1588627500.112812, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627497.866492}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:00 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=271.147004405 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:00 INFO 140149161203520] #progress_metric: host=algo-1, completed 70 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:00 INFO 140149161203520] #quality_metric: host=algo-1, epoch=280, train loss <loss>=3.23439185619\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:00 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:00 INFO 140149161203520] Epoch[281] Batch[0] avg_epoch_loss=3.123645\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:00 INFO 140149161203520] #quality_metric: host=algo-1, epoch=281, batch=0 train loss <loss>=3.12364506721\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:01 INFO 140149161203520] Epoch[281] Batch[5] avg_epoch_loss=3.150243\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:01 INFO 140149161203520] #quality_metric: host=algo-1, epoch=281, batch=5 train loss <loss>=3.15024288495\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:01 INFO 140149161203520] Epoch[281] Batch [5]#011Speed: 308.19 samples/sec#011loss=3.150243\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:02 INFO 140149161203520] Epoch[281] Batch[10] avg_epoch_loss=3.096090\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:02 INFO 140149161203520] #quality_metric: host=algo-1, epoch=281, batch=10 train loss <loss>=3.03110547066\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:02 INFO 140149161203520] Epoch[281] Batch [10]#011Speed: 315.50 samples/sec#011loss=3.031105\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:02 INFO 140149161203520] processed a total of 673 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2564.1419887542725, \"sum\": 2564.1419887542725, \"min\": 2564.1419887542725}}, \"EndTime\": 1588627502.677577, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627500.112912}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:02 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=262.453494286 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:02 INFO 140149161203520] #progress_metric: host=algo-1, completed 70 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:02 INFO 140149161203520] #quality_metric: host=algo-1, epoch=281, train loss <loss>=3.09608951482\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:02 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:03 INFO 140149161203520] Epoch[282] Batch[0] avg_epoch_loss=3.038839\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:03 INFO 140149161203520] #quality_metric: host=algo-1, epoch=282, batch=0 train loss <loss>=3.03883862495\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:04 INFO 140149161203520] Epoch[282] Batch[5] avg_epoch_loss=3.128069\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:04 INFO 140149161203520] #quality_metric: host=algo-1, epoch=282, batch=5 train loss <loss>=3.12806916237\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:04 INFO 140149161203520] Epoch[282] Batch [5]#011Speed: 318.30 samples/sec#011loss=3.128069\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:05 INFO 140149161203520] Epoch[282] Batch[10] avg_epoch_loss=3.160308\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:05 INFO 140149161203520] #quality_metric: host=algo-1, epoch=282, batch=10 train loss <loss>=3.19899511337\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:05 INFO 140149161203520] Epoch[282] Batch [10]#011Speed: 319.46 samples/sec#011loss=3.198995\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:05 INFO 140149161203520] processed a total of 667 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2462.157964706421, \"sum\": 2462.157964706421, \"min\": 2462.157964706421}}, \"EndTime\": 1588627505.140273, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627502.677661}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:05 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=270.88834796 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:05 INFO 140149161203520] #progress_metric: host=algo-1, completed 70 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:05 INFO 140149161203520] #quality_metric: host=algo-1, epoch=282, train loss <loss>=3.16030823101\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:05 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:05 INFO 140149161203520] Epoch[283] Batch[0] avg_epoch_loss=3.172586\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:05 INFO 140149161203520] #quality_metric: host=algo-1, epoch=283, batch=0 train loss <loss>=3.17258644104\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:06 INFO 140149161203520] Epoch[283] Batch[5] avg_epoch_loss=3.107327\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:06 INFO 140149161203520] #quality_metric: host=algo-1, epoch=283, batch=5 train loss <loss>=3.10732702414\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:06 INFO 140149161203520] Epoch[283] Batch [5]#011Speed: 321.04 samples/sec#011loss=3.107327\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:07 INFO 140149161203520] processed a total of 627 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2247.8151321411133, \"sum\": 2247.8151321411133, \"min\": 2247.8151321411133}}, \"EndTime\": 1588627507.388667, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627505.140341}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:07 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=278.920311622 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:07 INFO 140149161203520] #progress_metric: host=algo-1, completed 71 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:07 INFO 140149161203520] #quality_metric: host=algo-1, epoch=283, train loss <loss>=3.15349895954\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:07 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:07 INFO 140149161203520] Epoch[284] Batch[0] avg_epoch_loss=3.089449\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:07 INFO 140149161203520] #quality_metric: host=algo-1, epoch=284, batch=0 train loss <loss>=3.08944869041\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:08 INFO 140149161203520] Epoch[284] Batch[5] avg_epoch_loss=3.059877\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:08 INFO 140149161203520] #quality_metric: host=algo-1, epoch=284, batch=5 train loss <loss>=3.05987731616\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:08 INFO 140149161203520] Epoch[284] Batch [5]#011Speed: 322.12 samples/sec#011loss=3.059877\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:09 INFO 140149161203520] Epoch[284] Batch[10] avg_epoch_loss=3.026686\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:09 INFO 140149161203520] #quality_metric: host=algo-1, epoch=284, batch=10 train loss <loss>=2.98685598373\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:09 INFO 140149161203520] Epoch[284] Batch [10]#011Speed: 321.23 samples/sec#011loss=2.986856\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:09 INFO 140149161203520] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2521.674156188965, \"sum\": 2521.674156188965, \"min\": 2521.674156188965}}, \"EndTime\": 1588627509.910924, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627507.388763}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:09 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=258.939059121 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:09 INFO 140149161203520] #progress_metric: host=algo-1, completed 71 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:09 INFO 140149161203520] #quality_metric: host=algo-1, epoch=284, train loss <loss>=3.02668580142\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:09 INFO 140149161203520] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:10 INFO 140149161203520] Saved checkpoint to \"/opt/ml/model/state_08f1d5c6-92ed-4c03-8989-ca9f3ff6b364-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 95.15905380249023, \"sum\": 95.15905380249023, \"min\": 95.15905380249023}}, \"EndTime\": 1588627510.006701, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627509.911035}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:10 INFO 140149161203520] Epoch[285] Batch[0] avg_epoch_loss=3.080421\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:10 INFO 140149161203520] #quality_metric: host=algo-1, epoch=285, batch=0 train loss <loss>=3.08042120934\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:11 INFO 140149161203520] Epoch[285] Batch[5] avg_epoch_loss=3.128092\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:11 INFO 140149161203520] #quality_metric: host=algo-1, epoch=285, batch=5 train loss <loss>=3.12809189161\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:11 INFO 140149161203520] Epoch[285] Batch [5]#011Speed: 318.20 samples/sec#011loss=3.128092\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:12 INFO 140149161203520] Epoch[285] Batch[10] avg_epoch_loss=3.135290\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:12 INFO 140149161203520] #quality_metric: host=algo-1, epoch=285, batch=10 train loss <loss>=3.14392709732\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:12 INFO 140149161203520] Epoch[285] Batch [10]#011Speed: 315.84 samples/sec#011loss=3.143927\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:12 INFO 140149161203520] processed a total of 675 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2504.5530796051025, \"sum\": 2504.5530796051025, \"min\": 2504.5530796051025}}, \"EndTime\": 1588627512.511412, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627510.006788}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:12 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=269.49528233 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:12 INFO 140149161203520] #progress_metric: host=algo-1, completed 71 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:12 INFO 140149161203520] #quality_metric: host=algo-1, epoch=285, train loss <loss>=3.13528971239\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:12 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:12 INFO 140149161203520] Epoch[286] Batch[0] avg_epoch_loss=3.142121\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:12 INFO 140149161203520] #quality_metric: host=algo-1, epoch=286, batch=0 train loss <loss>=3.14212059975\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:13 INFO 140149161203520] Epoch[286] Batch[5] avg_epoch_loss=3.150503\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:13 INFO 140149161203520] #quality_metric: host=algo-1, epoch=286, batch=5 train loss <loss>=3.15050272147\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:13 INFO 140149161203520] Epoch[286] Batch [5]#011Speed: 320.72 samples/sec#011loss=3.150503\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:14 INFO 140149161203520] Epoch[286] Batch[10] avg_epoch_loss=3.224008\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:14 INFO 140149161203520] #quality_metric: host=algo-1, epoch=286, batch=10 train loss <loss>=3.31221327782\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:14 INFO 140149161203520] Epoch[286] Batch [10]#011Speed: 323.04 samples/sec#011loss=3.312213\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:14 INFO 140149161203520] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2475.3918647766113, \"sum\": 2475.3918647766113, \"min\": 2475.3918647766113}}, \"EndTime\": 1588627514.987339, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627512.5115}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:14 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=261.762868416 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:14 INFO 140149161203520] #progress_metric: host=algo-1, completed 71 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:14 INFO 140149161203520] #quality_metric: host=algo-1, epoch=286, train loss <loss>=3.22400751981\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:14 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:15 INFO 140149161203520] Epoch[287] Batch[0] avg_epoch_loss=3.029161\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:15 INFO 140149161203520] #quality_metric: host=algo-1, epoch=287, batch=0 train loss <loss>=3.02916121483\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:16 INFO 140149161203520] Epoch[287] Batch[5] avg_epoch_loss=3.169506\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:16 INFO 140149161203520] #quality_metric: host=algo-1, epoch=287, batch=5 train loss <loss>=3.16950551669\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:16 INFO 140149161203520] Epoch[287] Batch [5]#011Speed: 324.34 samples/sec#011loss=3.169506\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:17 INFO 140149161203520] Epoch[287] Batch[10] avg_epoch_loss=3.096182\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:17 INFO 140149161203520] #quality_metric: host=algo-1, epoch=287, batch=10 train loss <loss>=3.00819282532\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:17 INFO 140149161203520] Epoch[287] Batch [10]#011Speed: 319.90 samples/sec#011loss=3.008193\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:17 INFO 140149161203520] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2446.4900493621826, \"sum\": 2446.4900493621826, \"min\": 2446.4900493621826}}, \"EndTime\": 1588627517.434363, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627514.987429}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:17 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=262.811323114 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:17 INFO 140149161203520] #progress_metric: host=algo-1, completed 72 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:17 INFO 140149161203520] #quality_metric: host=algo-1, epoch=287, train loss <loss>=3.09618156607\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:17 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:17 INFO 140149161203520] Epoch[288] Batch[0] avg_epoch_loss=3.004768\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:17 INFO 140149161203520] #quality_metric: host=algo-1, epoch=288, batch=0 train loss <loss>=3.00476813316\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:18 INFO 140149161203520] Epoch[288] Batch[5] avg_epoch_loss=3.144833\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:18 INFO 140149161203520] #quality_metric: host=algo-1, epoch=288, batch=5 train loss <loss>=3.14483336608\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:18 INFO 140149161203520] Epoch[288] Batch [5]#011Speed: 323.20 samples/sec#011loss=3.144833\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:19 INFO 140149161203520] processed a total of 611 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2296.8268394470215, \"sum\": 2296.8268394470215, \"min\": 2296.8268394470215}}, \"EndTime\": 1588627519.731734, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627517.434454}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:19 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=266.003168306 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:19 INFO 140149161203520] #progress_metric: host=algo-1, completed 72 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:19 INFO 140149161203520] #quality_metric: host=algo-1, epoch=288, train loss <loss>=3.16252276897\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:19 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:20 INFO 140149161203520] Epoch[289] Batch[0] avg_epoch_loss=3.223334\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:20 INFO 140149161203520] #quality_metric: host=algo-1, epoch=289, batch=0 train loss <loss>=3.22333407402\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:21 INFO 140149161203520] Epoch[289] Batch[5] avg_epoch_loss=3.188582\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:21 INFO 140149161203520] #quality_metric: host=algo-1, epoch=289, batch=5 train loss <loss>=3.18858194351\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:21 INFO 140149161203520] Epoch[289] Batch [5]#011Speed: 322.33 samples/sec#011loss=3.188582\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:22 INFO 140149161203520] Epoch[289] Batch[10] avg_epoch_loss=3.122678\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:22 INFO 140149161203520] #quality_metric: host=algo-1, epoch=289, batch=10 train loss <loss>=3.04359221458\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:22 INFO 140149161203520] Epoch[289] Batch [10]#011Speed: 301.08 samples/sec#011loss=3.043592\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:22 INFO 140149161203520] processed a total of 681 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2539.36505317688, \"sum\": 2539.36505317688, \"min\": 2539.36505317688}}, \"EndTime\": 1588627522.271684, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627519.73183}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:22 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=268.163553121 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:22 INFO 140149161203520] #progress_metric: host=algo-1, completed 72 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:22 INFO 140149161203520] #quality_metric: host=algo-1, epoch=289, train loss <loss>=3.12267752127\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:22 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:22 INFO 140149161203520] Epoch[290] Batch[0] avg_epoch_loss=3.054082\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:22 INFO 140149161203520] #quality_metric: host=algo-1, epoch=290, batch=0 train loss <loss>=3.05408215523\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:23 INFO 140149161203520] Epoch[290] Batch[5] avg_epoch_loss=3.171076\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:23 INFO 140149161203520] #quality_metric: host=algo-1, epoch=290, batch=5 train loss <loss>=3.17107562224\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:23 INFO 140149161203520] Epoch[290] Batch [5]#011Speed: 325.00 samples/sec#011loss=3.171076\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:24 INFO 140149161203520] processed a total of 609 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2239.1510009765625, \"sum\": 2239.1510009765625, \"min\": 2239.1510009765625}}, \"EndTime\": 1588627524.511386, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627522.271773}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:24 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=271.960941506 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:24 INFO 140149161203520] #progress_metric: host=algo-1, completed 72 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:24 INFO 140149161203520] #quality_metric: host=algo-1, epoch=290, train loss <loss>=3.09283440113\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:24 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:24 INFO 140149161203520] Epoch[291] Batch[0] avg_epoch_loss=3.153094\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:24 INFO 140149161203520] #quality_metric: host=algo-1, epoch=291, batch=0 train loss <loss>=3.15309381485\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:25 INFO 140149161203520] Epoch[291] Batch[5] avg_epoch_loss=3.110474\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:25 INFO 140149161203520] #quality_metric: host=algo-1, epoch=291, batch=5 train loss <loss>=3.1104742686\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:25 INFO 140149161203520] Epoch[291] Batch [5]#011Speed: 321.44 samples/sec#011loss=3.110474\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:27 INFO 140149161203520] Epoch[291] Batch[10] avg_epoch_loss=3.162769\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:27 INFO 140149161203520] #quality_metric: host=algo-1, epoch=291, batch=10 train loss <loss>=3.22552371025\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:27 INFO 140149161203520] Epoch[291] Batch [10]#011Speed: 314.56 samples/sec#011loss=3.225524\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:27 INFO 140149161203520] processed a total of 667 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2491.564989089966, \"sum\": 2491.564989089966, \"min\": 2491.564989089966}}, \"EndTime\": 1588627527.003579, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627524.511484}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:27 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=267.685070935 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:27 INFO 140149161203520] #progress_metric: host=algo-1, completed 73 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:27 INFO 140149161203520] #quality_metric: host=algo-1, epoch=291, train loss <loss>=3.16276946935\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:27 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:27 INFO 140149161203520] Epoch[292] Batch[0] avg_epoch_loss=3.176686\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:27 INFO 140149161203520] #quality_metric: host=algo-1, epoch=292, batch=0 train loss <loss>=3.17668628693\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:28 INFO 140149161203520] Epoch[292] Batch[5] avg_epoch_loss=3.103325\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:28 INFO 140149161203520] #quality_metric: host=algo-1, epoch=292, batch=5 train loss <loss>=3.10332504908\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:28 INFO 140149161203520] Epoch[292] Batch [5]#011Speed: 321.33 samples/sec#011loss=3.103325\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:29 INFO 140149161203520] processed a total of 611 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2258.837938308716, \"sum\": 2258.837938308716, \"min\": 2258.837938308716}}, \"EndTime\": 1588627529.262955, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627527.003668}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:29 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=270.474024477 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:29 INFO 140149161203520] #progress_metric: host=algo-1, completed 73 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:29 INFO 140149161203520] #quality_metric: host=algo-1, epoch=292, train loss <loss>=3.11045796871\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:29 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:29 INFO 140149161203520] Epoch[293] Batch[0] avg_epoch_loss=3.111421\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:29 INFO 140149161203520] #quality_metric: host=algo-1, epoch=293, batch=0 train loss <loss>=3.11142063141\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:30 INFO 140149161203520] Epoch[293] Batch[5] avg_epoch_loss=3.138737\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:30 INFO 140149161203520] #quality_metric: host=algo-1, epoch=293, batch=5 train loss <loss>=3.13873656591\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:30 INFO 140149161203520] Epoch[293] Batch [5]#011Speed: 320.48 samples/sec#011loss=3.138737\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:31 INFO 140149161203520] Epoch[293] Batch[10] avg_epoch_loss=3.169420\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:31 INFO 140149161203520] #quality_metric: host=algo-1, epoch=293, batch=10 train loss <loss>=3.20623927116\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:31 INFO 140149161203520] Epoch[293] Batch [10]#011Speed: 325.10 samples/sec#011loss=3.206239\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:31 INFO 140149161203520] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2471.118927001953, \"sum\": 2471.118927001953, \"min\": 2471.118927001953}}, \"EndTime\": 1588627531.734701, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627529.263072}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:31 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=259.383197677 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:31 INFO 140149161203520] #progress_metric: host=algo-1, completed 73 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:31 INFO 140149161203520] #quality_metric: host=algo-1, epoch=293, train loss <loss>=3.16941961375\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:31 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:32 INFO 140149161203520] Epoch[294] Batch[0] avg_epoch_loss=2.990462\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:32 INFO 140149161203520] #quality_metric: host=algo-1, epoch=294, batch=0 train loss <loss>=2.99046158791\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:33 INFO 140149161203520] Epoch[294] Batch[5] avg_epoch_loss=3.051414\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:33 INFO 140149161203520] #quality_metric: host=algo-1, epoch=294, batch=5 train loss <loss>=3.05141373475\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:33 INFO 140149161203520] Epoch[294] Batch [5]#011Speed: 321.69 samples/sec#011loss=3.051414\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:34 INFO 140149161203520] Epoch[294] Batch[10] avg_epoch_loss=3.002237\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:34 INFO 140149161203520] #quality_metric: host=algo-1, epoch=294, batch=10 train loss <loss>=2.9432246685\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:34 INFO 140149161203520] Epoch[294] Batch [10]#011Speed: 318.23 samples/sec#011loss=2.943225\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:34 INFO 140149161203520] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2484.173059463501, \"sum\": 2484.173059463501, \"min\": 2484.173059463501}}, \"EndTime\": 1588627534.219435, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627531.734789}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:34 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=258.020254678 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:34 INFO 140149161203520] #progress_metric: host=algo-1, completed 73 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:34 INFO 140149161203520] #quality_metric: host=algo-1, epoch=294, train loss <loss>=3.00223688646\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:34 INFO 140149161203520] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:34 INFO 140149161203520] Saved checkpoint to \"/opt/ml/model/state_5046be9a-4e17-40df-8dc3-e24191463349-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 71.34079933166504, \"sum\": 71.34079933166504, \"min\": 71.34079933166504}}, \"EndTime\": 1588627534.291374, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627534.219523}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:34 INFO 140149161203520] Epoch[295] Batch[0] avg_epoch_loss=3.024959\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:34 INFO 140149161203520] #quality_metric: host=algo-1, epoch=295, batch=0 train loss <loss>=3.02495884895\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:35 INFO 140149161203520] Epoch[295] Batch[5] avg_epoch_loss=3.077030\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:35 INFO 140149161203520] #quality_metric: host=algo-1, epoch=295, batch=5 train loss <loss>=3.07703034083\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:35 INFO 140149161203520] Epoch[295] Batch [5]#011Speed: 318.73 samples/sec#011loss=3.077030\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:36 INFO 140149161203520] processed a total of 609 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2262.421131134033, \"sum\": 2262.421131134033, \"min\": 2262.421131134033}}, \"EndTime\": 1588627536.553946, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627534.291455}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:36 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=269.1649682 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:36 INFO 140149161203520] #progress_metric: host=algo-1, completed 74 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:36 INFO 140149161203520] #quality_metric: host=algo-1, epoch=295, train loss <loss>=3.04858796597\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:36 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:37 INFO 140149161203520] Epoch[296] Batch[0] avg_epoch_loss=3.175153\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:37 INFO 140149161203520] #quality_metric: host=algo-1, epoch=296, batch=0 train loss <loss>=3.17515254021\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:38 INFO 140149161203520] Epoch[296] Batch[5] avg_epoch_loss=3.102003\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:38 INFO 140149161203520] #quality_metric: host=algo-1, epoch=296, batch=5 train loss <loss>=3.10200337569\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:38 INFO 140149161203520] Epoch[296] Batch [5]#011Speed: 305.83 samples/sec#011loss=3.102003\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:39 INFO 140149161203520] Epoch[296] Batch[10] avg_epoch_loss=3.074930\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:39 INFO 140149161203520] #quality_metric: host=algo-1, epoch=296, batch=10 train loss <loss>=3.04244170189\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:39 INFO 140149161203520] Epoch[296] Batch [10]#011Speed: 320.41 samples/sec#011loss=3.042442\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:39 INFO 140149161203520] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2514.069080352783, \"sum\": 2514.069080352783, \"min\": 2514.069080352783}}, \"EndTime\": 1588627539.068645, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627536.554037}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:39 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=256.144904373 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:39 INFO 140149161203520] #progress_metric: host=algo-1, completed 74 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:39 INFO 140149161203520] #quality_metric: host=algo-1, epoch=296, train loss <loss>=3.0749298876\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:39 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:39 INFO 140149161203520] Epoch[297] Batch[0] avg_epoch_loss=2.983079\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:39 INFO 140149161203520] #quality_metric: host=algo-1, epoch=297, batch=0 train loss <loss>=2.9830789566\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:40 INFO 140149161203520] Epoch[297] Batch[5] avg_epoch_loss=3.061011\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:40 INFO 140149161203520] #quality_metric: host=algo-1, epoch=297, batch=5 train loss <loss>=3.06101091703\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:40 INFO 140149161203520] Epoch[297] Batch [5]#011Speed: 324.00 samples/sec#011loss=3.061011\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:41 INFO 140149161203520] processed a total of 618 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2253.006935119629, \"sum\": 2253.006935119629, \"min\": 2253.006935119629}}, \"EndTime\": 1588627541.322201, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627539.068736}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:41 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=274.283049904 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:41 INFO 140149161203520] #progress_metric: host=algo-1, completed 74 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:41 INFO 140149161203520] #quality_metric: host=algo-1, epoch=297, train loss <loss>=2.9870783329\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:41 INFO 140149161203520] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:41 INFO 140149161203520] Saved checkpoint to \"/opt/ml/model/state_d62cca55-67d8-499a-9487-c06f3d6ed17c-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 70.16706466674805, \"sum\": 70.16706466674805, \"min\": 70.16706466674805}}, \"EndTime\": 1588627541.392999, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627541.322298}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:41 INFO 140149161203520] Epoch[298] Batch[0] avg_epoch_loss=2.967819\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:41 INFO 140149161203520] #quality_metric: host=algo-1, epoch=298, batch=0 train loss <loss>=2.96781873703\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:42 INFO 140149161203520] Epoch[298] Batch[5] avg_epoch_loss=3.111054\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:42 INFO 140149161203520] #quality_metric: host=algo-1, epoch=298, batch=5 train loss <loss>=3.11105422179\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:42 INFO 140149161203520] Epoch[298] Batch [5]#011Speed: 315.25 samples/sec#011loss=3.111054\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:43 INFO 140149161203520] processed a total of 602 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2269.0608501434326, \"sum\": 2269.0608501434326, \"min\": 2269.0608501434326}}, \"EndTime\": 1588627543.662226, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627541.393088}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:43 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=265.291611152 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:43 INFO 140149161203520] #progress_metric: host=algo-1, completed 74 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:43 INFO 140149161203520] #quality_metric: host=algo-1, epoch=298, train loss <loss>=3.05385148525\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:43 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:44 INFO 140149161203520] Epoch[299] Batch[0] avg_epoch_loss=3.083302\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:44 INFO 140149161203520] #quality_metric: host=algo-1, epoch=299, batch=0 train loss <loss>=3.08330178261\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:45 INFO 140149161203520] Epoch[299] Batch[5] avg_epoch_loss=3.180265\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:45 INFO 140149161203520] #quality_metric: host=algo-1, epoch=299, batch=5 train loss <loss>=3.18026467164\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:45 INFO 140149161203520] Epoch[299] Batch [5]#011Speed: 319.23 samples/sec#011loss=3.180265\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:45 INFO 140149161203520] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2256.289005279541, \"sum\": 2256.289005279541, \"min\": 2256.289005279541}}, \"EndTime\": 1588627545.91921, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627543.662323}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:45 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=281.41800328 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:45 INFO 140149161203520] #progress_metric: host=algo-1, completed 75 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:45 INFO 140149161203520] #quality_metric: host=algo-1, epoch=299, train loss <loss>=3.16313626766\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:45 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:46 INFO 140149161203520] Epoch[300] Batch[0] avg_epoch_loss=3.056514\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:46 INFO 140149161203520] #quality_metric: host=algo-1, epoch=300, batch=0 train loss <loss>=3.0565135479\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:47 INFO 140149161203520] Epoch[300] Batch[5] avg_epoch_loss=3.061421\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:47 INFO 140149161203520] #quality_metric: host=algo-1, epoch=300, batch=5 train loss <loss>=3.06142143408\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:47 INFO 140149161203520] Epoch[300] Batch [5]#011Speed: 319.94 samples/sec#011loss=3.061421\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:48 INFO 140149161203520] processed a total of 611 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2248.699903488159, \"sum\": 2248.699903488159, \"min\": 2248.699903488159}}, \"EndTime\": 1588627548.168499, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627545.919307}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:48 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=271.695762461 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:48 INFO 140149161203520] #progress_metric: host=algo-1, completed 75 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:48 INFO 140149161203520] #quality_metric: host=algo-1, epoch=300, train loss <loss>=3.13625600338\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:48 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:48 INFO 140149161203520] Epoch[301] Batch[0] avg_epoch_loss=3.157844\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:48 INFO 140149161203520] #quality_metric: host=algo-1, epoch=301, batch=0 train loss <loss>=3.15784358978\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:49 INFO 140149161203520] Epoch[301] Batch[5] avg_epoch_loss=3.132405\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:49 INFO 140149161203520] #quality_metric: host=algo-1, epoch=301, batch=5 train loss <loss>=3.13240484397\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:49 INFO 140149161203520] Epoch[301] Batch [5]#011Speed: 326.55 samples/sec#011loss=3.132405\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:50 INFO 140149161203520] Epoch[301] Batch[10] avg_epoch_loss=3.161720\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:50 INFO 140149161203520] #quality_metric: host=algo-1, epoch=301, batch=10 train loss <loss>=3.19689836502\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:50 INFO 140149161203520] Epoch[301] Batch [10]#011Speed: 324.96 samples/sec#011loss=3.196898\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:50 INFO 140149161203520] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2459.798812866211, \"sum\": 2459.798812866211, \"min\": 2459.798812866211}}, \"EndTime\": 1588627550.628894, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627548.168596}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:50 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=268.300739759 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:50 INFO 140149161203520] #progress_metric: host=algo-1, completed 75 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:50 INFO 140149161203520] #quality_metric: host=algo-1, epoch=301, train loss <loss>=3.16172008081\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:50 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:51 INFO 140149161203520] Epoch[302] Batch[0] avg_epoch_loss=3.055707\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:51 INFO 140149161203520] #quality_metric: host=algo-1, epoch=302, batch=0 train loss <loss>=3.05570745468\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:52 INFO 140149161203520] Epoch[302] Batch[5] avg_epoch_loss=3.100687\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:52 INFO 140149161203520] #quality_metric: host=algo-1, epoch=302, batch=5 train loss <loss>=3.10068706671\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:52 INFO 140149161203520] Epoch[302] Batch [5]#011Speed: 319.87 samples/sec#011loss=3.100687\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:52 INFO 140149161203520] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2250.1838207244873, \"sum\": 2250.1838207244873, \"min\": 2250.1838207244873}}, \"EndTime\": 1588627552.879594, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627550.628981}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:52 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=279.072520188 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:52 INFO 140149161203520] #progress_metric: host=algo-1, completed 75 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:52 INFO 140149161203520] #quality_metric: host=algo-1, epoch=302, train loss <loss>=3.11711573601\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:52 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:53 INFO 140149161203520] Epoch[303] Batch[0] avg_epoch_loss=3.041963\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:53 INFO 140149161203520] #quality_metric: host=algo-1, epoch=303, batch=0 train loss <loss>=3.04196333885\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:54 INFO 140149161203520] Epoch[303] Batch[5] avg_epoch_loss=3.119203\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:54 INFO 140149161203520] #quality_metric: host=algo-1, epoch=303, batch=5 train loss <loss>=3.1192031304\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:54 INFO 140149161203520] Epoch[303] Batch [5]#011Speed: 323.15 samples/sec#011loss=3.119203\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:55 INFO 140149161203520] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2232.4111461639404, \"sum\": 2232.4111461639404, \"min\": 2232.4111461639404}}, \"EndTime\": 1588627555.112687, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627552.879685}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:55 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=284.431686195 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:55 INFO 140149161203520] #progress_metric: host=algo-1, completed 76 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:55 INFO 140149161203520] #quality_metric: host=algo-1, epoch=303, train loss <loss>=3.105346632\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:55 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:55 INFO 140149161203520] Epoch[304] Batch[0] avg_epoch_loss=3.092463\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:55 INFO 140149161203520] #quality_metric: host=algo-1, epoch=304, batch=0 train loss <loss>=3.09246325493\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:56 INFO 140149161203520] Epoch[304] Batch[5] avg_epoch_loss=3.134963\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:56 INFO 140149161203520] #quality_metric: host=algo-1, epoch=304, batch=5 train loss <loss>=3.13496255875\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:56 INFO 140149161203520] Epoch[304] Batch [5]#011Speed: 320.29 samples/sec#011loss=3.134963\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:57 INFO 140149161203520] Epoch[304] Batch[10] avg_epoch_loss=3.172282\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:57 INFO 140149161203520] #quality_metric: host=algo-1, epoch=304, batch=10 train loss <loss>=3.21706614494\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:57 INFO 140149161203520] Epoch[304] Batch [10]#011Speed: 315.20 samples/sec#011loss=3.217066\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:57 INFO 140149161203520] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2462.4321460723877, \"sum\": 2462.4321460723877, \"min\": 2462.4321460723877}}, \"EndTime\": 1588627557.575787, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627555.11276}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:57 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=265.983373553 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:57 INFO 140149161203520] #progress_metric: host=algo-1, completed 76 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:57 INFO 140149161203520] #quality_metric: host=algo-1, epoch=304, train loss <loss>=3.17228237065\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:57 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:58 INFO 140149161203520] Epoch[305] Batch[0] avg_epoch_loss=3.181403\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:58 INFO 140149161203520] #quality_metric: host=algo-1, epoch=305, batch=0 train loss <loss>=3.1814031601\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:59 INFO 140149161203520] Epoch[305] Batch[5] avg_epoch_loss=3.166934\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:59 INFO 140149161203520] #quality_metric: host=algo-1, epoch=305, batch=5 train loss <loss>=3.16693425179\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:59 INFO 140149161203520] Epoch[305] Batch [5]#011Speed: 325.61 samples/sec#011loss=3.166934\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:59 INFO 140149161203520] processed a total of 579 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2265.937089920044, \"sum\": 2265.937089920044, \"min\": 2265.937089920044}}, \"EndTime\": 1588627559.84226, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627557.575875}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:59 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=255.50774887 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:59 INFO 140149161203520] #progress_metric: host=algo-1, completed 76 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:59 INFO 140149161203520] #quality_metric: host=algo-1, epoch=305, train loss <loss>=3.02847106457\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:25:59 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:00 INFO 140149161203520] Epoch[306] Batch[0] avg_epoch_loss=3.017835\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:00 INFO 140149161203520] #quality_metric: host=algo-1, epoch=306, batch=0 train loss <loss>=3.01783514023\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:01 INFO 140149161203520] Epoch[306] Batch[5] avg_epoch_loss=3.094571\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:01 INFO 140149161203520] #quality_metric: host=algo-1, epoch=306, batch=5 train loss <loss>=3.0945712328\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:01 INFO 140149161203520] Epoch[306] Batch [5]#011Speed: 322.52 samples/sec#011loss=3.094571\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:02 INFO 140149161203520] processed a total of 601 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2300.4260063171387, \"sum\": 2300.4260063171387, \"min\": 2300.4260063171387}}, \"EndTime\": 1588627562.14329, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627559.842357}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:02 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=261.242447315 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:02 INFO 140149161203520] #progress_metric: host=algo-1, completed 76 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:02 INFO 140149161203520] #quality_metric: host=algo-1, epoch=306, train loss <loss>=3.16079502106\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:02 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:02 INFO 140149161203520] Epoch[307] Batch[0] avg_epoch_loss=3.155139\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:02 INFO 140149161203520] #quality_metric: host=algo-1, epoch=307, batch=0 train loss <loss>=3.15513944626\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:03 INFO 140149161203520] Epoch[307] Batch[5] avg_epoch_loss=3.119327\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:03 INFO 140149161203520] #quality_metric: host=algo-1, epoch=307, batch=5 train loss <loss>=3.11932698886\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:03 INFO 140149161203520] Epoch[307] Batch [5]#011Speed: 318.97 samples/sec#011loss=3.119327\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:04 INFO 140149161203520] Epoch[307] Batch[10] avg_epoch_loss=3.084581\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:04 INFO 140149161203520] #quality_metric: host=algo-1, epoch=307, batch=10 train loss <loss>=3.04288477898\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:04 INFO 140149161203520] Epoch[307] Batch [10]#011Speed: 321.04 samples/sec#011loss=3.042885\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:04 INFO 140149161203520] processed a total of 699 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2463.0050659179688, \"sum\": 2463.0050659179688, \"min\": 2463.0050659179688}}, \"EndTime\": 1588627564.606908, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627562.143367}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:04 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=283.785842256 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:04 INFO 140149161203520] #progress_metric: host=algo-1, completed 77 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:04 INFO 140149161203520] #quality_metric: host=algo-1, epoch=307, train loss <loss>=3.08458052982\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:04 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:05 INFO 140149161203520] Epoch[308] Batch[0] avg_epoch_loss=3.122353\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:05 INFO 140149161203520] #quality_metric: host=algo-1, epoch=308, batch=0 train loss <loss>=3.12235283852\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:06 INFO 140149161203520] Epoch[308] Batch[5] avg_epoch_loss=3.127421\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:06 INFO 140149161203520] #quality_metric: host=algo-1, epoch=308, batch=5 train loss <loss>=3.12742082278\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:06 INFO 140149161203520] Epoch[308] Batch [5]#011Speed: 321.55 samples/sec#011loss=3.127421\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:07 INFO 140149161203520] Epoch[308] Batch[10] avg_epoch_loss=3.180604\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:07 INFO 140149161203520] #quality_metric: host=algo-1, epoch=308, batch=10 train loss <loss>=3.24442334175\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:07 INFO 140149161203520] Epoch[308] Batch [10]#011Speed: 318.70 samples/sec#011loss=3.244423\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:07 INFO 140149161203520] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2444.8888301849365, \"sum\": 2444.8888301849365, \"min\": 2444.8888301849365}}, \"EndTime\": 1588627567.052376, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627564.606988}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:07 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=262.574645937 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:07 INFO 140149161203520] #progress_metric: host=algo-1, completed 77 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:07 INFO 140149161203520] #quality_metric: host=algo-1, epoch=308, train loss <loss>=3.18060378595\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:07 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:07 INFO 140149161203520] Epoch[309] Batch[0] avg_epoch_loss=2.901765\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:07 INFO 140149161203520] #quality_metric: host=algo-1, epoch=309, batch=0 train loss <loss>=2.90176463127\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:08 INFO 140149161203520] Epoch[309] Batch[5] avg_epoch_loss=3.099308\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:08 INFO 140149161203520] #quality_metric: host=algo-1, epoch=309, batch=5 train loss <loss>=3.09930801392\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:08 INFO 140149161203520] Epoch[309] Batch [5]#011Speed: 307.89 samples/sec#011loss=3.099308\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:09 INFO 140149161203520] processed a total of 615 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2346.532106399536, \"sum\": 2346.532106399536, \"min\": 2346.532106399536}}, \"EndTime\": 1588627569.399441, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627567.052465}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:09 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=262.073394658 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:09 INFO 140149161203520] #progress_metric: host=algo-1, completed 77 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:09 INFO 140149161203520] #quality_metric: host=algo-1, epoch=309, train loss <loss>=3.06566982269\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:09 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:09 INFO 140149161203520] Epoch[310] Batch[0] avg_epoch_loss=3.218893\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:09 INFO 140149161203520] #quality_metric: host=algo-1, epoch=310, batch=0 train loss <loss>=3.21889257431\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:10 INFO 140149161203520] Epoch[310] Batch[5] avg_epoch_loss=3.162258\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:10 INFO 140149161203520] #quality_metric: host=algo-1, epoch=310, batch=5 train loss <loss>=3.16225751241\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:10 INFO 140149161203520] Epoch[310] Batch [5]#011Speed: 322.28 samples/sec#011loss=3.162258\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:11 INFO 140149161203520] Epoch[310] Batch[10] avg_epoch_loss=3.095596\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:11 INFO 140149161203520] #quality_metric: host=algo-1, epoch=310, batch=10 train loss <loss>=3.01560230255\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:11 INFO 140149161203520] Epoch[310] Batch [10]#011Speed: 325.70 samples/sec#011loss=3.015602\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:11 INFO 140149161203520] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2459.444046020508, \"sum\": 2459.444046020508, \"min\": 2459.444046020508}}, \"EndTime\": 1588627571.859489, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627569.399539}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:11 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=262.647314179 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:11 INFO 140149161203520] #progress_metric: host=algo-1, completed 77 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:11 INFO 140149161203520] #quality_metric: host=algo-1, epoch=310, train loss <loss>=3.09559605338\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:11 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:12 INFO 140149161203520] Epoch[311] Batch[0] avg_epoch_loss=2.876070\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:12 INFO 140149161203520] #quality_metric: host=algo-1, epoch=311, batch=0 train loss <loss>=2.87607049942\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:13 INFO 140149161203520] Epoch[311] Batch[5] avg_epoch_loss=3.094782\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:13 INFO 140149161203520] #quality_metric: host=algo-1, epoch=311, batch=5 train loss <loss>=3.09478203456\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:13 INFO 140149161203520] Epoch[311] Batch [5]#011Speed: 319.35 samples/sec#011loss=3.094782\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:14 INFO 140149161203520] processed a total of 631 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2264.2080783843994, \"sum\": 2264.2080783843994, \"min\": 2264.2080783843994}}, \"EndTime\": 1588627574.124246, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627571.859578}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:14 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=278.667292243 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:14 INFO 140149161203520] #progress_metric: host=algo-1, completed 78 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:14 INFO 140149161203520] #quality_metric: host=algo-1, epoch=311, train loss <loss>=3.12670514584\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:14 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:14 INFO 140149161203520] Epoch[312] Batch[0] avg_epoch_loss=3.104219\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:14 INFO 140149161203520] #quality_metric: host=algo-1, epoch=312, batch=0 train loss <loss>=3.10421872139\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:15 INFO 140149161203520] Epoch[312] Batch[5] avg_epoch_loss=3.017675\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:15 INFO 140149161203520] #quality_metric: host=algo-1, epoch=312, batch=5 train loss <loss>=3.01767536004\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:15 INFO 140149161203520] Epoch[312] Batch [5]#011Speed: 321.80 samples/sec#011loss=3.017675\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:16 INFO 140149161203520] Epoch[312] Batch[10] avg_epoch_loss=3.134083\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:16 INFO 140149161203520] #quality_metric: host=algo-1, epoch=312, batch=10 train loss <loss>=3.27377324104\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:16 INFO 140149161203520] Epoch[312] Batch [10]#011Speed: 323.08 samples/sec#011loss=3.273773\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:16 INFO 140149161203520] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2484.6270084381104, \"sum\": 2484.6270084381104, \"min\": 2484.6270084381104}}, \"EndTime\": 1588627576.609473, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627574.124343}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:16 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=261.595103044 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:16 INFO 140149161203520] #progress_metric: host=algo-1, completed 78 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:16 INFO 140149161203520] #quality_metric: host=algo-1, epoch=312, train loss <loss>=3.13408348777\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:16 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:17 INFO 140149161203520] Epoch[313] Batch[0] avg_epoch_loss=3.110802\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:17 INFO 140149161203520] #quality_metric: host=algo-1, epoch=313, batch=0 train loss <loss>=3.1108019352\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:18 INFO 140149161203520] Epoch[313] Batch[5] avg_epoch_loss=3.045805\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:18 INFO 140149161203520] #quality_metric: host=algo-1, epoch=313, batch=5 train loss <loss>=3.04580493768\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:18 INFO 140149161203520] Epoch[313] Batch [5]#011Speed: 321.72 samples/sec#011loss=3.045805\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:18 INFO 140149161203520] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2240.29278755188, \"sum\": 2240.29278755188, \"min\": 2240.29278755188}}, \"EndTime\": 1588627578.850308, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627576.609563}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:18 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=284.766748696 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:18 INFO 140149161203520] #progress_metric: host=algo-1, completed 78 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:18 INFO 140149161203520] #quality_metric: host=algo-1, epoch=313, train loss <loss>=3.02954497337\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:18 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:19 INFO 140149161203520] Epoch[314] Batch[0] avg_epoch_loss=3.144632\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:19 INFO 140149161203520] #quality_metric: host=algo-1, epoch=314, batch=0 train loss <loss>=3.14463162422\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:20 INFO 140149161203520] Epoch[314] Batch[5] avg_epoch_loss=3.136888\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:20 INFO 140149161203520] #quality_metric: host=algo-1, epoch=314, batch=5 train loss <loss>=3.13688822587\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:20 INFO 140149161203520] Epoch[314] Batch [5]#011Speed: 326.91 samples/sec#011loss=3.136888\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:21 INFO 140149161203520] Epoch[314] Batch[10] avg_epoch_loss=3.181759\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:21 INFO 140149161203520] #quality_metric: host=algo-1, epoch=314, batch=10 train loss <loss>=3.23560385704\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:21 INFO 140149161203520] Epoch[314] Batch [10]#011Speed: 324.55 samples/sec#011loss=3.235604\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:21 INFO 140149161203520] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2420.0470447540283, \"sum\": 2420.0470447540283, \"min\": 2420.0470447540283}}, \"EndTime\": 1588627581.270943, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627578.850402}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:21 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=266.093148746 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:21 INFO 140149161203520] #progress_metric: host=algo-1, completed 78 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:21 INFO 140149161203520] #quality_metric: host=algo-1, epoch=314, train loss <loss>=3.18175896731\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:21 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:21 INFO 140149161203520] Epoch[315] Batch[0] avg_epoch_loss=2.969647\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:21 INFO 140149161203520] #quality_metric: host=algo-1, epoch=315, batch=0 train loss <loss>=2.96964740753\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:22 INFO 140149161203520] Epoch[315] Batch[5] avg_epoch_loss=3.091936\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:22 INFO 140149161203520] #quality_metric: host=algo-1, epoch=315, batch=5 train loss <loss>=3.09193567435\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:22 INFO 140149161203520] Epoch[315] Batch [5]#011Speed: 315.59 samples/sec#011loss=3.091936\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:23 INFO 140149161203520] processed a total of 610 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2270.627021789551, \"sum\": 2270.627021789551, \"min\": 2270.627021789551}}, \"EndTime\": 1588627583.542159, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627581.27106}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:23 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=268.63170335 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:23 INFO 140149161203520] #progress_metric: host=algo-1, completed 79 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:23 INFO 140149161203520] #quality_metric: host=algo-1, epoch=315, train loss <loss>=3.08977303505\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:23 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:24 INFO 140149161203520] Epoch[316] Batch[0] avg_epoch_loss=3.000362\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:24 INFO 140149161203520] #quality_metric: host=algo-1, epoch=316, batch=0 train loss <loss>=3.0003619194\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:24 INFO 140149161203520] Epoch[316] Batch[5] avg_epoch_loss=3.051926\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:25 INFO 140149161203520] #quality_metric: host=algo-1, epoch=316, batch=5 train loss <loss>=3.0519263347\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:25 INFO 140149161203520] Epoch[316] Batch [5]#011Speed: 327.47 samples/sec#011loss=3.051926\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:25 INFO 140149161203520] Epoch[316] Batch[10] avg_epoch_loss=3.014642\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:25 INFO 140149161203520] #quality_metric: host=algo-1, epoch=316, batch=10 train loss <loss>=2.96990041733\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:25 INFO 140149161203520] Epoch[316] Batch [10]#011Speed: 321.31 samples/sec#011loss=2.969900\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:25 INFO 140149161203520] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2453.87601852417, \"sum\": 2453.87601852417, \"min\": 2453.87601852417}}, \"EndTime\": 1588627585.996655, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627583.542255}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:25 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=268.540948991 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:25 INFO 140149161203520] #progress_metric: host=algo-1, completed 79 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:25 INFO 140149161203520] #quality_metric: host=algo-1, epoch=316, train loss <loss>=3.0146418268\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:25 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:26 INFO 140149161203520] Epoch[317] Batch[0] avg_epoch_loss=3.119278\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:26 INFO 140149161203520] #quality_metric: host=algo-1, epoch=317, batch=0 train loss <loss>=3.1192779541\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:27 INFO 140149161203520] Epoch[317] Batch[5] avg_epoch_loss=3.104207\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:27 INFO 140149161203520] #quality_metric: host=algo-1, epoch=317, batch=5 train loss <loss>=3.10420731703\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:27 INFO 140149161203520] Epoch[317] Batch [5]#011Speed: 319.21 samples/sec#011loss=3.104207\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:28 INFO 140149161203520] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2272.2268104553223, \"sum\": 2272.2268104553223, \"min\": 2272.2268104553223}}, \"EndTime\": 1588627588.269393, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627585.99674}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:28 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=276.366099848 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:28 INFO 140149161203520] #progress_metric: host=algo-1, completed 79 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:28 INFO 140149161203520] #quality_metric: host=algo-1, epoch=317, train loss <loss>=3.07923288345\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:28 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:28 INFO 140149161203520] Epoch[318] Batch[0] avg_epoch_loss=3.314978\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:28 INFO 140149161203520] #quality_metric: host=algo-1, epoch=318, batch=0 train loss <loss>=3.31497788429\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:29 INFO 140149161203520] Epoch[318] Batch[5] avg_epoch_loss=3.125534\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:29 INFO 140149161203520] #quality_metric: host=algo-1, epoch=318, batch=5 train loss <loss>=3.12553385894\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:29 INFO 140149161203520] Epoch[318] Batch [5]#011Speed: 322.12 samples/sec#011loss=3.125534\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:30 INFO 140149161203520] Epoch[318] Batch[10] avg_epoch_loss=3.101686\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:30 INFO 140149161203520] #quality_metric: host=algo-1, epoch=318, batch=10 train loss <loss>=3.07306890488\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:30 INFO 140149161203520] Epoch[318] Batch [10]#011Speed: 322.43 samples/sec#011loss=3.073069\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:30 INFO 140149161203520] processed a total of 676 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2443.9878463745117, \"sum\": 2443.9878463745117, \"min\": 2443.9878463745117}}, \"EndTime\": 1588627590.714009, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627588.269471}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:30 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=276.582037533 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:30 INFO 140149161203520] #progress_metric: host=algo-1, completed 79 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:30 INFO 140149161203520] #quality_metric: host=algo-1, epoch=318, train loss <loss>=3.10168615254\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:30 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:31 INFO 140149161203520] Epoch[319] Batch[0] avg_epoch_loss=3.114752\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:31 INFO 140149161203520] #quality_metric: host=algo-1, epoch=319, batch=0 train loss <loss>=3.11475205421\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:32 INFO 140149161203520] Epoch[319] Batch[5] avg_epoch_loss=3.075014\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:32 INFO 140149161203520] #quality_metric: host=algo-1, epoch=319, batch=5 train loss <loss>=3.07501367728\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:32 INFO 140149161203520] Epoch[319] Batch [5]#011Speed: 322.94 samples/sec#011loss=3.075014\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:33 INFO 140149161203520] Epoch[319] Batch[10] avg_epoch_loss=3.146390\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:33 INFO 140149161203520] #quality_metric: host=algo-1, epoch=319, batch=10 train loss <loss>=3.23204236031\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:33 INFO 140149161203520] Epoch[319] Batch [10]#011Speed: 302.32 samples/sec#011loss=3.232042\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:33 INFO 140149161203520] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2499.6440410614014, \"sum\": 2499.6440410614014, \"min\": 2499.6440410614014}}, \"EndTime\": 1588627593.214232, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627590.7141}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:33 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=262.024014004 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:33 INFO 140149161203520] #progress_metric: host=algo-1, completed 80 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:33 INFO 140149161203520] #quality_metric: host=algo-1, epoch=319, train loss <loss>=3.14639035138\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:33 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:33 INFO 140149161203520] Epoch[320] Batch[0] avg_epoch_loss=3.146342\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:33 INFO 140149161203520] #quality_metric: host=algo-1, epoch=320, batch=0 train loss <loss>=3.14634227753\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:34 INFO 140149161203520] Epoch[320] Batch[5] avg_epoch_loss=3.125892\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:34 INFO 140149161203520] #quality_metric: host=algo-1, epoch=320, batch=5 train loss <loss>=3.12589212259\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:34 INFO 140149161203520] Epoch[320] Batch [5]#011Speed: 325.77 samples/sec#011loss=3.125892\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:35 INFO 140149161203520] Epoch[320] Batch[10] avg_epoch_loss=3.100217\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:35 INFO 140149161203520] #quality_metric: host=algo-1, epoch=320, batch=10 train loss <loss>=3.06940736771\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:35 INFO 140149161203520] Epoch[320] Batch [10]#011Speed: 325.25 samples/sec#011loss=3.069407\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:35 INFO 140149161203520] processed a total of 672 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2418.498992919922, \"sum\": 2418.498992919922, \"min\": 2418.498992919922}}, \"EndTime\": 1588627595.633311, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627593.214318}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:35 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=277.843583742 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:35 INFO 140149161203520] #progress_metric: host=algo-1, completed 80 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:35 INFO 140149161203520] #quality_metric: host=algo-1, epoch=320, train loss <loss>=3.100217234\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:35 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:36 INFO 140149161203520] Epoch[321] Batch[0] avg_epoch_loss=3.062699\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:36 INFO 140149161203520] #quality_metric: host=algo-1, epoch=321, batch=0 train loss <loss>=3.06269931793\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:37 INFO 140149161203520] Epoch[321] Batch[5] avg_epoch_loss=3.082492\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:37 INFO 140149161203520] #quality_metric: host=algo-1, epoch=321, batch=5 train loss <loss>=3.08249215285\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:37 INFO 140149161203520] Epoch[321] Batch [5]#011Speed: 325.66 samples/sec#011loss=3.082492\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:38 INFO 140149161203520] Epoch[321] Batch[10] avg_epoch_loss=3.023438\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:38 INFO 140149161203520] #quality_metric: host=algo-1, epoch=321, batch=10 train loss <loss>=2.9525739193\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:38 INFO 140149161203520] Epoch[321] Batch [10]#011Speed: 319.91 samples/sec#011loss=2.952574\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:38 INFO 140149161203520] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2443.833827972412, \"sum\": 2443.833827972412, \"min\": 2443.833827972412}}, \"EndTime\": 1588627598.077675, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627595.633399}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:38 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=265.143021257 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:38 INFO 140149161203520] #progress_metric: host=algo-1, completed 80 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:38 INFO 140149161203520] #quality_metric: host=algo-1, epoch=321, train loss <loss>=3.02343841033\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:38 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:38 INFO 140149161203520] Epoch[322] Batch[0] avg_epoch_loss=3.175256\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:38 INFO 140149161203520] #quality_metric: host=algo-1, epoch=322, batch=0 train loss <loss>=3.17525577545\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:39 INFO 140149161203520] Epoch[322] Batch[5] avg_epoch_loss=3.099645\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:39 INFO 140149161203520] #quality_metric: host=algo-1, epoch=322, batch=5 train loss <loss>=3.09964497884\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:39 INFO 140149161203520] Epoch[322] Batch [5]#011Speed: 323.76 samples/sec#011loss=3.099645\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:40 INFO 140149161203520] Epoch[322] Batch[10] avg_epoch_loss=3.035522\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:40 INFO 140149161203520] #quality_metric: host=algo-1, epoch=322, batch=10 train loss <loss>=2.95857357979\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:40 INFO 140149161203520] Epoch[322] Batch [10]#011Speed: 313.56 samples/sec#011loss=2.958574\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:40 INFO 140149161203520] processed a total of 666 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2529.7698974609375, \"sum\": 2529.7698974609375, \"min\": 2529.7698974609375}}, \"EndTime\": 1588627600.607998, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627598.077766}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:40 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=263.252025205 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:40 INFO 140149161203520] #progress_metric: host=algo-1, completed 80 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:40 INFO 140149161203520] #quality_metric: host=algo-1, epoch=322, train loss <loss>=3.03552161564\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:40 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:41 INFO 140149161203520] Epoch[323] Batch[0] avg_epoch_loss=2.929825\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:41 INFO 140149161203520] #quality_metric: host=algo-1, epoch=323, batch=0 train loss <loss>=2.92982530594\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:42 INFO 140149161203520] Epoch[323] Batch[5] avg_epoch_loss=2.958277\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:42 INFO 140149161203520] #quality_metric: host=algo-1, epoch=323, batch=5 train loss <loss>=2.95827722549\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:42 INFO 140149161203520] Epoch[323] Batch [5]#011Speed: 323.52 samples/sec#011loss=2.958277\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:43 INFO 140149161203520] Epoch[323] Batch[10] avg_epoch_loss=3.084921\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:43 INFO 140149161203520] #quality_metric: host=algo-1, epoch=323, batch=10 train loss <loss>=3.23689322472\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:43 INFO 140149161203520] Epoch[323] Batch [10]#011Speed: 316.48 samples/sec#011loss=3.236893\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:43 INFO 140149161203520] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2449.263095855713, \"sum\": 2449.263095855713, \"min\": 2449.263095855713}}, \"EndTime\": 1588627603.057864, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627600.608081}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:43 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=268.228620298 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:43 INFO 140149161203520] #progress_metric: host=algo-1, completed 81 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:43 INFO 140149161203520] #quality_metric: host=algo-1, epoch=323, train loss <loss>=3.0849208615\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:43 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:43 INFO 140149161203520] Epoch[324] Batch[0] avg_epoch_loss=3.163285\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:43 INFO 140149161203520] #quality_metric: host=algo-1, epoch=324, batch=0 train loss <loss>=3.16328454018\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:44 INFO 140149161203520] Epoch[324] Batch[5] avg_epoch_loss=2.984365\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:44 INFO 140149161203520] #quality_metric: host=algo-1, epoch=324, batch=5 train loss <loss>=2.98436526457\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:44 INFO 140149161203520] Epoch[324] Batch [5]#011Speed: 321.71 samples/sec#011loss=2.984365\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:45 INFO 140149161203520] processed a total of 606 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2244.0011501312256, \"sum\": 2244.0011501312256, \"min\": 2244.0011501312256}}, \"EndTime\": 1588627605.30248, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627603.057958}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:45 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=270.038791271 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:45 INFO 140149161203520] #progress_metric: host=algo-1, completed 81 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:45 INFO 140149161203520] #quality_metric: host=algo-1, epoch=324, train loss <loss>=2.97494180202\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:45 INFO 140149161203520] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:45 INFO 140149161203520] Saved checkpoint to \"/opt/ml/model/state_6bd47558-a567-43fe-ac71-7187badfb6a2-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 59.635162353515625, \"sum\": 59.635162353515625, \"min\": 59.635162353515625}}, \"EndTime\": 1588627605.362766, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627605.302566}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:45 INFO 140149161203520] Epoch[325] Batch[0] avg_epoch_loss=2.969885\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:45 INFO 140149161203520] #quality_metric: host=algo-1, epoch=325, batch=0 train loss <loss>=2.96988463402\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:46 INFO 140149161203520] Epoch[325] Batch[5] avg_epoch_loss=3.064906\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:46 INFO 140149161203520] #quality_metric: host=algo-1, epoch=325, batch=5 train loss <loss>=3.06490643819\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:46 INFO 140149161203520] Epoch[325] Batch [5]#011Speed: 323.09 samples/sec#011loss=3.064906\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:47 INFO 140149161203520] Epoch[325] Batch[10] avg_epoch_loss=3.012329\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:47 INFO 140149161203520] #quality_metric: host=algo-1, epoch=325, batch=10 train loss <loss>=2.94923524857\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:47 INFO 140149161203520] Epoch[325] Batch [10]#011Speed: 317.01 samples/sec#011loss=2.949235\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:47 INFO 140149161203520] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2447.8249549865723, \"sum\": 2447.8249549865723, \"min\": 2447.8249549865723}}, \"EndTime\": 1588627607.810746, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627605.362853}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:47 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=265.936583506 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:47 INFO 140149161203520] #progress_metric: host=algo-1, completed 81 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:47 INFO 140149161203520] #quality_metric: host=algo-1, epoch=325, train loss <loss>=3.01232862473\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:47 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:48 INFO 140149161203520] Epoch[326] Batch[0] avg_epoch_loss=2.973806\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:48 INFO 140149161203520] #quality_metric: host=algo-1, epoch=326, batch=0 train loss <loss>=2.97380566597\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:49 INFO 140149161203520] Epoch[326] Batch[5] avg_epoch_loss=3.119607\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:49 INFO 140149161203520] #quality_metric: host=algo-1, epoch=326, batch=5 train loss <loss>=3.11960732937\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:49 INFO 140149161203520] Epoch[326] Batch [5]#011Speed: 321.93 samples/sec#011loss=3.119607\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:50 INFO 140149161203520] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2258.4311962127686, \"sum\": 2258.4311962127686, \"min\": 2258.4311962127686}}, \"EndTime\": 1588627610.069726, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627607.810836}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:50 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=283.362361824 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:50 INFO 140149161203520] #progress_metric: host=algo-1, completed 81 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:50 INFO 140149161203520] #quality_metric: host=algo-1, epoch=326, train loss <loss>=3.11291387081\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:50 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:50 INFO 140149161203520] Epoch[327] Batch[0] avg_epoch_loss=3.002709\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:50 INFO 140149161203520] #quality_metric: host=algo-1, epoch=327, batch=0 train loss <loss>=3.0027089119\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:51 INFO 140149161203520] Epoch[327] Batch[5] avg_epoch_loss=3.012082\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:51 INFO 140149161203520] #quality_metric: host=algo-1, epoch=327, batch=5 train loss <loss>=3.01208158334\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:51 INFO 140149161203520] Epoch[327] Batch [5]#011Speed: 321.75 samples/sec#011loss=3.012082\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:52 INFO 140149161203520] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2265.8231258392334, \"sum\": 2265.8231258392334, \"min\": 2265.8231258392334}}, \"EndTime\": 1588627612.336165, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627610.069847}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:52 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=282.44061271 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:52 INFO 140149161203520] #progress_metric: host=algo-1, completed 82 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:52 INFO 140149161203520] #quality_metric: host=algo-1, epoch=327, train loss <loss>=3.01834356785\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:52 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:52 INFO 140149161203520] Epoch[328] Batch[0] avg_epoch_loss=3.152881\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:52 INFO 140149161203520] #quality_metric: host=algo-1, epoch=328, batch=0 train loss <loss>=3.15288066864\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:53 INFO 140149161203520] Epoch[328] Batch[5] avg_epoch_loss=3.103351\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:53 INFO 140149161203520] #quality_metric: host=algo-1, epoch=328, batch=5 train loss <loss>=3.10335083803\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:53 INFO 140149161203520] Epoch[328] Batch [5]#011Speed: 319.46 samples/sec#011loss=3.103351\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:54 INFO 140149161203520] processed a total of 631 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2282.533884048462, \"sum\": 2282.533884048462, \"min\": 2282.533884048462}}, \"EndTime\": 1588627614.619313, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627612.336263}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:54 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=276.43202556 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:54 INFO 140149161203520] #progress_metric: host=algo-1, completed 82 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:54 INFO 140149161203520] #quality_metric: host=algo-1, epoch=328, train loss <loss>=3.09952609539\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:54 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:55 INFO 140149161203520] Epoch[329] Batch[0] avg_epoch_loss=2.959227\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:55 INFO 140149161203520] #quality_metric: host=algo-1, epoch=329, batch=0 train loss <loss>=2.95922684669\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:56 INFO 140149161203520] Epoch[329] Batch[5] avg_epoch_loss=3.079443\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:56 INFO 140149161203520] #quality_metric: host=algo-1, epoch=329, batch=5 train loss <loss>=3.07944309711\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:56 INFO 140149161203520] Epoch[329] Batch [5]#011Speed: 321.40 samples/sec#011loss=3.079443\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:56 INFO 140149161203520] processed a total of 612 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2235.963821411133, \"sum\": 2235.963821411133, \"min\": 2235.963821411133}}, \"EndTime\": 1588627616.855892, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627614.619394}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:56 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=273.692381853 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:56 INFO 140149161203520] #progress_metric: host=algo-1, completed 82 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:56 INFO 140149161203520] #quality_metric: host=algo-1, epoch=329, train loss <loss>=3.10276403427\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:56 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:57 INFO 140149161203520] Epoch[330] Batch[0] avg_epoch_loss=3.122522\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:57 INFO 140149161203520] #quality_metric: host=algo-1, epoch=330, batch=0 train loss <loss>=3.12252235413\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:58 INFO 140149161203520] Epoch[330] Batch[5] avg_epoch_loss=3.089904\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:58 INFO 140149161203520] #quality_metric: host=algo-1, epoch=330, batch=5 train loss <loss>=3.08990414937\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:58 INFO 140149161203520] Epoch[330] Batch [5]#011Speed: 318.86 samples/sec#011loss=3.089904\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:59 INFO 140149161203520] Epoch[330] Batch[10] avg_epoch_loss=2.996228\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:59 INFO 140149161203520] #quality_metric: host=algo-1, epoch=330, batch=10 train loss <loss>=2.88381586075\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:59 INFO 140149161203520] Epoch[330] Batch [10]#011Speed: 319.43 samples/sec#011loss=2.883816\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:59 INFO 140149161203520] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2454.986095428467, \"sum\": 2454.986095428467, \"min\": 2454.986095428467}}, \"EndTime\": 1588627619.311495, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627616.855971}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:59 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=261.902412335 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:59 INFO 140149161203520] #progress_metric: host=algo-1, completed 82 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:59 INFO 140149161203520] #quality_metric: host=algo-1, epoch=330, train loss <loss>=2.99622765454\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:59 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:59 INFO 140149161203520] Epoch[331] Batch[0] avg_epoch_loss=3.131022\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:26:59 INFO 140149161203520] #quality_metric: host=algo-1, epoch=331, batch=0 train loss <loss>=3.13102221489\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:00 INFO 140149161203520] Epoch[331] Batch[5] avg_epoch_loss=3.031365\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:00 INFO 140149161203520] #quality_metric: host=algo-1, epoch=331, batch=5 train loss <loss>=3.03136471907\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:00 INFO 140149161203520] Epoch[331] Batch [5]#011Speed: 320.94 samples/sec#011loss=3.031365\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:01 INFO 140149161203520] Epoch[331] Batch[10] avg_epoch_loss=3.070881\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:01 INFO 140149161203520] #quality_metric: host=algo-1, epoch=331, batch=10 train loss <loss>=3.1183011055\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:01 INFO 140149161203520] Epoch[331] Batch [10]#011Speed: 320.51 samples/sec#011loss=3.118301\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:01 INFO 140149161203520] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2441.48588180542, \"sum\": 2441.48588180542, \"min\": 2441.48588180542}}, \"EndTime\": 1588627621.753597, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627619.311577}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:01 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=268.674956788 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:01 INFO 140149161203520] #progress_metric: host=algo-1, completed 83 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:01 INFO 140149161203520] #quality_metric: host=algo-1, epoch=331, train loss <loss>=3.07088125836\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:01 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:02 INFO 140149161203520] Epoch[332] Batch[0] avg_epoch_loss=3.221308\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:02 INFO 140149161203520] #quality_metric: host=algo-1, epoch=332, batch=0 train loss <loss>=3.22130799294\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:03 INFO 140149161203520] Epoch[332] Batch[5] avg_epoch_loss=3.109603\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:03 INFO 140149161203520] #quality_metric: host=algo-1, epoch=332, batch=5 train loss <loss>=3.10960336526\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:03 INFO 140149161203520] Epoch[332] Batch [5]#011Speed: 316.28 samples/sec#011loss=3.109603\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:04 INFO 140149161203520] Epoch[332] Batch[10] avg_epoch_loss=3.142457\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:04 INFO 140149161203520] #quality_metric: host=algo-1, epoch=332, batch=10 train loss <loss>=3.1818813324\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:04 INFO 140149161203520] Epoch[332] Batch [10]#011Speed: 320.18 samples/sec#011loss=3.181881\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:04 INFO 140149161203520] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2473.0899333953857, \"sum\": 2473.0899333953857, \"min\": 2473.0899333953857}}, \"EndTime\": 1588627624.227232, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627621.753679}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:04 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=262.006704241 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:04 INFO 140149161203520] #progress_metric: host=algo-1, completed 83 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:04 INFO 140149161203520] #quality_metric: host=algo-1, epoch=332, train loss <loss>=3.14245698669\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:04 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:04 INFO 140149161203520] Epoch[333] Batch[0] avg_epoch_loss=3.106938\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:04 INFO 140149161203520] #quality_metric: host=algo-1, epoch=333, batch=0 train loss <loss>=3.1069381237\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:05 INFO 140149161203520] Epoch[333] Batch[5] avg_epoch_loss=3.123910\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:05 INFO 140149161203520] #quality_metric: host=algo-1, epoch=333, batch=5 train loss <loss>=3.12390998999\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:05 INFO 140149161203520] Epoch[333] Batch [5]#011Speed: 321.05 samples/sec#011loss=3.123910\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:06 INFO 140149161203520] Epoch[333] Batch[10] avg_epoch_loss=3.111410\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:06 INFO 140149161203520] #quality_metric: host=algo-1, epoch=333, batch=10 train loss <loss>=3.09641108513\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:06 INFO 140149161203520] Epoch[333] Batch [10]#011Speed: 321.89 samples/sec#011loss=3.096411\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:06 INFO 140149161203520] processed a total of 665 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2458.6780071258545, \"sum\": 2458.6780071258545, \"min\": 2458.6780071258545}}, \"EndTime\": 1588627626.686465, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627624.227321}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:06 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=270.456495645 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:06 INFO 140149161203520] #progress_metric: host=algo-1, completed 83 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:06 INFO 140149161203520] #quality_metric: host=algo-1, epoch=333, train loss <loss>=3.11141048778\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:06 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:07 INFO 140149161203520] Epoch[334] Batch[0] avg_epoch_loss=3.017341\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:07 INFO 140149161203520] #quality_metric: host=algo-1, epoch=334, batch=0 train loss <loss>=3.0173406601\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:08 INFO 140149161203520] Epoch[334] Batch[5] avg_epoch_loss=3.026700\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:08 INFO 140149161203520] #quality_metric: host=algo-1, epoch=334, batch=5 train loss <loss>=3.02670017878\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:08 INFO 140149161203520] Epoch[334] Batch [5]#011Speed: 321.50 samples/sec#011loss=3.026700\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:09 INFO 140149161203520] Epoch[334] Batch[10] avg_epoch_loss=3.011634\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:09 INFO 140149161203520] #quality_metric: host=algo-1, epoch=334, batch=10 train loss <loss>=2.99355511665\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:09 INFO 140149161203520] Epoch[334] Batch [10]#011Speed: 296.76 samples/sec#011loss=2.993555\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:09 INFO 140149161203520] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2555.0479888916016, \"sum\": 2555.0479888916016, \"min\": 2555.0479888916016}}, \"EndTime\": 1588627629.24208, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627626.686553}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:09 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=257.516001092 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:09 INFO 140149161203520] #progress_metric: host=algo-1, completed 83 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:09 INFO 140149161203520] #quality_metric: host=algo-1, epoch=334, train loss <loss>=3.01163424145\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:09 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:09 INFO 140149161203520] Epoch[335] Batch[0] avg_epoch_loss=3.134535\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:09 INFO 140149161203520] #quality_metric: host=algo-1, epoch=335, batch=0 train loss <loss>=3.13453507423\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:10 INFO 140149161203520] Epoch[335] Batch[5] avg_epoch_loss=3.050173\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:10 INFO 140149161203520] #quality_metric: host=algo-1, epoch=335, batch=5 train loss <loss>=3.05017276605\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:10 INFO 140149161203520] Epoch[335] Batch [5]#011Speed: 323.27 samples/sec#011loss=3.050173\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:11 INFO 140149161203520] Epoch[335] Batch[10] avg_epoch_loss=2.978381\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:11 INFO 140149161203520] #quality_metric: host=algo-1, epoch=335, batch=10 train loss <loss>=2.89222979546\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:11 INFO 140149161203520] Epoch[335] Batch [10]#011Speed: 320.38 samples/sec#011loss=2.892230\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:11 INFO 140149161203520] processed a total of 671 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2438.5221004486084, \"sum\": 2438.5221004486084, \"min\": 2438.5221004486084}}, \"EndTime\": 1588627631.681165, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627629.242171}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:11 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=275.153270193 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:11 INFO 140149161203520] #progress_metric: host=algo-1, completed 84 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:11 INFO 140149161203520] #quality_metric: host=algo-1, epoch=335, train loss <loss>=2.97838050669\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:11 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:12 INFO 140149161203520] Epoch[336] Batch[0] avg_epoch_loss=3.075919\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:12 INFO 140149161203520] #quality_metric: host=algo-1, epoch=336, batch=0 train loss <loss>=3.07591938972\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:13 INFO 140149161203520] Epoch[336] Batch[5] avg_epoch_loss=2.973366\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:13 INFO 140149161203520] #quality_metric: host=algo-1, epoch=336, batch=5 train loss <loss>=2.97336566448\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:13 INFO 140149161203520] Epoch[336] Batch [5]#011Speed: 317.68 samples/sec#011loss=2.973366\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:13 INFO 140149161203520] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2260.477066040039, \"sum\": 2260.477066040039, \"min\": 2260.477066040039}}, \"EndTime\": 1588627633.942202, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627631.681247}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:13 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=278.246434114 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:13 INFO 140149161203520] #progress_metric: host=algo-1, completed 84 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:13 INFO 140149161203520] #quality_metric: host=algo-1, epoch=336, train loss <loss>=2.98651456833\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:13 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:14 INFO 140149161203520] Epoch[337] Batch[0] avg_epoch_loss=3.148406\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:14 INFO 140149161203520] #quality_metric: host=algo-1, epoch=337, batch=0 train loss <loss>=3.14840579033\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:15 INFO 140149161203520] Epoch[337] Batch[5] avg_epoch_loss=3.021770\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:15 INFO 140149161203520] #quality_metric: host=algo-1, epoch=337, batch=5 train loss <loss>=3.0217701594\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:15 INFO 140149161203520] Epoch[337] Batch [5]#011Speed: 322.43 samples/sec#011loss=3.021770\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:16 INFO 140149161203520] processed a total of 594 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2246.043920516968, \"sum\": 2246.043920516968, \"min\": 2246.043920516968}}, \"EndTime\": 1588627636.188824, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627633.942275}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:16 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=264.450147753 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:16 INFO 140149161203520] #progress_metric: host=algo-1, completed 84 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:16 INFO 140149161203520] #quality_metric: host=algo-1, epoch=337, train loss <loss>=2.98033447266\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:16 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:16 INFO 140149161203520] Epoch[338] Batch[0] avg_epoch_loss=2.994455\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:16 INFO 140149161203520] #quality_metric: host=algo-1, epoch=338, batch=0 train loss <loss>=2.99445533752\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:17 INFO 140149161203520] Epoch[338] Batch[5] avg_epoch_loss=3.022304\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:17 INFO 140149161203520] #quality_metric: host=algo-1, epoch=338, batch=5 train loss <loss>=3.02230429649\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:17 INFO 140149161203520] Epoch[338] Batch [5]#011Speed: 318.44 samples/sec#011loss=3.022304\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:18 INFO 140149161203520] Epoch[338] Batch[10] avg_epoch_loss=2.971203\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:18 INFO 140149161203520] #quality_metric: host=algo-1, epoch=338, batch=10 train loss <loss>=2.90988130569\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:18 INFO 140149161203520] Epoch[338] Batch [10]#011Speed: 322.16 samples/sec#011loss=2.909881\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:18 INFO 140149161203520] processed a total of 669 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2458.737850189209, \"sum\": 2458.737850189209, \"min\": 2458.737850189209}}, \"EndTime\": 1588627638.648169, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627636.18891}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:18 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=272.076969955 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:18 INFO 140149161203520] #progress_metric: host=algo-1, completed 84 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:18 INFO 140149161203520] #quality_metric: host=algo-1, epoch=338, train loss <loss>=2.97120293704\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:18 INFO 140149161203520] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:18 INFO 140149161203520] Saved checkpoint to \"/opt/ml/model/state_c7e4dce6-8ac6-4818-ba7e-a8deb643070e-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 57.76786804199219, \"sum\": 57.76786804199219, \"min\": 57.76786804199219}}, \"EndTime\": 1588627638.706531, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627638.648249}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:19 INFO 140149161203520] Epoch[339] Batch[0] avg_epoch_loss=3.095335\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:19 INFO 140149161203520] #quality_metric: host=algo-1, epoch=339, batch=0 train loss <loss>=3.0953347683\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:20 INFO 140149161203520] Epoch[339] Batch[5] avg_epoch_loss=2.991337\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:20 INFO 140149161203520] #quality_metric: host=algo-1, epoch=339, batch=5 train loss <loss>=2.99133741856\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:20 INFO 140149161203520] Epoch[339] Batch [5]#011Speed: 325.62 samples/sec#011loss=2.991337\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:21 INFO 140149161203520] Epoch[339] Batch[10] avg_epoch_loss=3.057059\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:21 INFO 140149161203520] #quality_metric: host=algo-1, epoch=339, batch=10 train loss <loss>=3.13592596054\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:21 INFO 140149161203520] Epoch[339] Batch [10]#011Speed: 323.66 samples/sec#011loss=3.135926\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:21 INFO 140149161203520] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2440.7119750976562, \"sum\": 2440.7119750976562, \"min\": 2440.7119750976562}}, \"EndTime\": 1588627641.147383, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627638.706612}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:21 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=268.350239435 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:21 INFO 140149161203520] #progress_metric: host=algo-1, completed 85 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:21 INFO 140149161203520] #quality_metric: host=algo-1, epoch=339, train loss <loss>=3.05705948309\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:21 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:21 INFO 140149161203520] Epoch[340] Batch[0] avg_epoch_loss=3.100513\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:21 INFO 140149161203520] #quality_metric: host=algo-1, epoch=340, batch=0 train loss <loss>=3.10051345825\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:22 INFO 140149161203520] Epoch[340] Batch[5] avg_epoch_loss=3.118531\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:22 INFO 140149161203520] #quality_metric: host=algo-1, epoch=340, batch=5 train loss <loss>=3.11853134632\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:22 INFO 140149161203520] Epoch[340] Batch [5]#011Speed: 326.67 samples/sec#011loss=3.118531\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:23 INFO 140149161203520] Epoch[340] Batch[10] avg_epoch_loss=3.182500\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:23 INFO 140149161203520] #quality_metric: host=algo-1, epoch=340, batch=10 train loss <loss>=3.2592625618\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:23 INFO 140149161203520] Epoch[340] Batch [10]#011Speed: 314.24 samples/sec#011loss=3.259263\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:23 INFO 140149161203520] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2471.4620113372803, \"sum\": 2471.4620113372803, \"min\": 2471.4620113372803}}, \"EndTime\": 1588627643.619447, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627641.147472}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:23 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=266.225483466 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:23 INFO 140149161203520] #progress_metric: host=algo-1, completed 85 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:23 INFO 140149161203520] #quality_metric: host=algo-1, epoch=340, train loss <loss>=3.18250008063\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:23 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:24 INFO 140149161203520] Epoch[341] Batch[0] avg_epoch_loss=3.117222\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:24 INFO 140149161203520] #quality_metric: host=algo-1, epoch=341, batch=0 train loss <loss>=3.11722207069\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:25 INFO 140149161203520] Epoch[341] Batch[5] avg_epoch_loss=3.066814\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:25 INFO 140149161203520] #quality_metric: host=algo-1, epoch=341, batch=5 train loss <loss>=3.06681402524\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:25 INFO 140149161203520] Epoch[341] Batch [5]#011Speed: 325.89 samples/sec#011loss=3.066814\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:25 INFO 140149161203520] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2253.0078887939453, \"sum\": 2253.0078887939453, \"min\": 2253.0078887939453}}, \"EndTime\": 1588627645.872992, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627643.619534}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:25 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=276.50167104 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:25 INFO 140149161203520] #progress_metric: host=algo-1, completed 85 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:25 INFO 140149161203520] #quality_metric: host=algo-1, epoch=341, train loss <loss>=3.0561549902\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:25 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:26 INFO 140149161203520] Epoch[342] Batch[0] avg_epoch_loss=2.812204\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:26 INFO 140149161203520] #quality_metric: host=algo-1, epoch=342, batch=0 train loss <loss>=2.81220388412\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:27 INFO 140149161203520] Epoch[342] Batch[5] avg_epoch_loss=2.916141\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:27 INFO 140149161203520] #quality_metric: host=algo-1, epoch=342, batch=5 train loss <loss>=2.91614107291\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:27 INFO 140149161203520] Epoch[342] Batch [5]#011Speed: 323.47 samples/sec#011loss=2.916141\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:28 INFO 140149161203520] Epoch[342] Batch[10] avg_epoch_loss=3.065647\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:28 INFO 140149161203520] #quality_metric: host=algo-1, epoch=342, batch=10 train loss <loss>=3.24505338669\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:28 INFO 140149161203520] Epoch[342] Batch [10]#011Speed: 319.35 samples/sec#011loss=3.245053\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:28 INFO 140149161203520] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2459.0210914611816, \"sum\": 2459.0210914611816, \"min\": 2459.0210914611816}}, \"EndTime\": 1588627648.332603, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627645.873091}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:28 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=265.945335812 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:28 INFO 140149161203520] #progress_metric: host=algo-1, completed 85 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:28 INFO 140149161203520] #quality_metric: host=algo-1, epoch=342, train loss <loss>=3.06564667008\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:28 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:28 INFO 140149161203520] Epoch[343] Batch[0] avg_epoch_loss=3.027757\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:28 INFO 140149161203520] #quality_metric: host=algo-1, epoch=343, batch=0 train loss <loss>=3.02775669098\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:29 INFO 140149161203520] Epoch[343] Batch[5] avg_epoch_loss=3.055450\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:29 INFO 140149161203520] #quality_metric: host=algo-1, epoch=343, batch=5 train loss <loss>=3.05544956525\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:29 INFO 140149161203520] Epoch[343] Batch [5]#011Speed: 326.07 samples/sec#011loss=3.055450\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:30 INFO 140149161203520] processed a total of 611 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2270.709991455078, \"sum\": 2270.709991455078, \"min\": 2270.709991455078}}, \"EndTime\": 1588627650.603841, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627648.332694}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:30 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=269.062704135 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:30 INFO 140149161203520] #progress_metric: host=algo-1, completed 86 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:30 INFO 140149161203520] #quality_metric: host=algo-1, epoch=343, train loss <loss>=2.97615261078\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:30 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:31 INFO 140149161203520] Epoch[344] Batch[0] avg_epoch_loss=3.221651\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:31 INFO 140149161203520] #quality_metric: host=algo-1, epoch=344, batch=0 train loss <loss>=3.22165107727\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:32 INFO 140149161203520] Epoch[344] Batch[5] avg_epoch_loss=3.057542\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:32 INFO 140149161203520] #quality_metric: host=algo-1, epoch=344, batch=5 train loss <loss>=3.05754232407\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:32 INFO 140149161203520] Epoch[344] Batch [5]#011Speed: 316.97 samples/sec#011loss=3.057542\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:32 INFO 140149161203520] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2291.9628620147705, \"sum\": 2291.9628620147705, \"min\": 2291.9628620147705}}, \"EndTime\": 1588627652.896389, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627650.603936}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:32 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=278.347256976 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:32 INFO 140149161203520] #progress_metric: host=algo-1, completed 86 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:32 INFO 140149161203520] #quality_metric: host=algo-1, epoch=344, train loss <loss>=3.02085843086\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:32 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:33 INFO 140149161203520] Epoch[345] Batch[0] avg_epoch_loss=3.124100\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:33 INFO 140149161203520] #quality_metric: host=algo-1, epoch=345, batch=0 train loss <loss>=3.12409973145\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:34 INFO 140149161203520] Epoch[345] Batch[5] avg_epoch_loss=3.095479\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:34 INFO 140149161203520] #quality_metric: host=algo-1, epoch=345, batch=5 train loss <loss>=3.09547857443\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:34 INFO 140149161203520] Epoch[345] Batch [5]#011Speed: 323.87 samples/sec#011loss=3.095479\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:35 INFO 140149161203520] processed a total of 602 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2266.6139602661133, \"sum\": 2266.6139602661133, \"min\": 2266.6139602661133}}, \"EndTime\": 1588627655.163584, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627652.896485}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:35 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=265.578013043 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:35 INFO 140149161203520] #progress_metric: host=algo-1, completed 86 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:35 INFO 140149161203520] #quality_metric: host=algo-1, epoch=345, train loss <loss>=3.11824636459\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:35 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:35 INFO 140149161203520] Epoch[346] Batch[0] avg_epoch_loss=2.987770\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:35 INFO 140149161203520] #quality_metric: host=algo-1, epoch=346, batch=0 train loss <loss>=2.98777008057\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:36 INFO 140149161203520] Epoch[346] Batch[5] avg_epoch_loss=3.057918\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:36 INFO 140149161203520] #quality_metric: host=algo-1, epoch=346, batch=5 train loss <loss>=3.05791751544\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:36 INFO 140149161203520] Epoch[346] Batch [5]#011Speed: 326.12 samples/sec#011loss=3.057918\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:37 INFO 140149161203520] Epoch[346] Batch[10] avg_epoch_loss=3.104449\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:37 INFO 140149161203520] #quality_metric: host=algo-1, epoch=346, batch=10 train loss <loss>=3.16028671265\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:37 INFO 140149161203520] Epoch[346] Batch [10]#011Speed: 314.43 samples/sec#011loss=3.160287\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:37 INFO 140149161203520] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2458.0469131469727, \"sum\": 2458.0469131469727, \"min\": 2458.0469131469727}}, \"EndTime\": 1588627657.622207, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627655.16368}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:37 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=261.983209839 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:37 INFO 140149161203520] #progress_metric: host=algo-1, completed 86 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:37 INFO 140149161203520] #quality_metric: host=algo-1, epoch=346, train loss <loss>=3.10444896871\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:37 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:38 INFO 140149161203520] Epoch[347] Batch[0] avg_epoch_loss=2.824560\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:38 INFO 140149161203520] #quality_metric: host=algo-1, epoch=347, batch=0 train loss <loss>=2.82456040382\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:39 INFO 140149161203520] Epoch[347] Batch[5] avg_epoch_loss=3.061696\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:39 INFO 140149161203520] #quality_metric: host=algo-1, epoch=347, batch=5 train loss <loss>=3.06169609229\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:39 INFO 140149161203520] Epoch[347] Batch [5]#011Speed: 315.22 samples/sec#011loss=3.061696\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:39 INFO 140149161203520] processed a total of 602 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2267.657995223999, \"sum\": 2267.657995223999, \"min\": 2267.657995223999}}, \"EndTime\": 1588627659.890356, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627657.622293}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:39 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=265.456082757 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:39 INFO 140149161203520] #progress_metric: host=algo-1, completed 87 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:39 INFO 140149161203520] #quality_metric: host=algo-1, epoch=347, train loss <loss>=3.04271194935\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:39 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:40 INFO 140149161203520] Epoch[348] Batch[0] avg_epoch_loss=3.173119\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:40 INFO 140149161203520] #quality_metric: host=algo-1, epoch=348, batch=0 train loss <loss>=3.17311882973\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:41 INFO 140149161203520] Epoch[348] Batch[5] avg_epoch_loss=3.009372\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:41 INFO 140149161203520] #quality_metric: host=algo-1, epoch=348, batch=5 train loss <loss>=3.00937179724\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:41 INFO 140149161203520] Epoch[348] Batch [5]#011Speed: 321.60 samples/sec#011loss=3.009372\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:42 INFO 140149161203520] processed a total of 608 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2249.6979236602783, \"sum\": 2249.6979236602783, \"min\": 2249.6979236602783}}, \"EndTime\": 1588627662.140644, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627659.890451}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:42 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=270.242038249 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:42 INFO 140149161203520] #progress_metric: host=algo-1, completed 87 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:42 INFO 140149161203520] #quality_metric: host=algo-1, epoch=348, train loss <loss>=2.97842125893\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:42 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:42 INFO 140149161203520] Epoch[349] Batch[0] avg_epoch_loss=2.864505\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:42 INFO 140149161203520] #quality_metric: host=algo-1, epoch=349, batch=0 train loss <loss>=2.86450481415\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:43 INFO 140149161203520] Epoch[349] Batch[5] avg_epoch_loss=3.017991\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:43 INFO 140149161203520] #quality_metric: host=algo-1, epoch=349, batch=5 train loss <loss>=3.01799082756\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:43 INFO 140149161203520] Epoch[349] Batch [5]#011Speed: 319.31 samples/sec#011loss=3.017991\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:44 INFO 140149161203520] Epoch[349] Batch[10] avg_epoch_loss=2.965461\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:44 INFO 140149161203520] #quality_metric: host=algo-1, epoch=349, batch=10 train loss <loss>=2.90242590904\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:44 INFO 140149161203520] Epoch[349] Batch [10]#011Speed: 322.20 samples/sec#011loss=2.902426\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:44 INFO 140149161203520] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2484.930992126465, \"sum\": 2484.930992126465, \"min\": 2484.930992126465}}, \"EndTime\": 1588627664.626151, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627662.14074}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:44 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=264.782443369 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:44 INFO 140149161203520] #progress_metric: host=algo-1, completed 87 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:44 INFO 140149161203520] #quality_metric: host=algo-1, epoch=349, train loss <loss>=2.96546131914\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:44 INFO 140149161203520] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:44 INFO 140149161203520] Saved checkpoint to \"/opt/ml/model/state_b93da9a1-d95d-4ea7-8d86-f36a34e653a6-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 61.46502494812012, \"sum\": 61.46502494812012, \"min\": 61.46502494812012}}, \"EndTime\": 1588627664.688226, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627664.626239}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:45 INFO 140149161203520] Epoch[350] Batch[0] avg_epoch_loss=2.905478\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:45 INFO 140149161203520] #quality_metric: host=algo-1, epoch=350, batch=0 train loss <loss>=2.90547776222\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:46 INFO 140149161203520] Epoch[350] Batch[5] avg_epoch_loss=3.051146\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:46 INFO 140149161203520] #quality_metric: host=algo-1, epoch=350, batch=5 train loss <loss>=3.0511456728\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:46 INFO 140149161203520] Epoch[350] Batch [5]#011Speed: 324.05 samples/sec#011loss=3.051146\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:47 INFO 140149161203520] Epoch[350] Batch[10] avg_epoch_loss=3.047508\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:47 INFO 140149161203520] #quality_metric: host=algo-1, epoch=350, batch=10 train loss <loss>=3.04314255714\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:47 INFO 140149161203520] Epoch[350] Batch [10]#011Speed: 319.98 samples/sec#011loss=3.043143\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:47 INFO 140149161203520] processed a total of 692 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2454.67209815979, \"sum\": 2454.67209815979, \"min\": 2454.67209815979}}, \"EndTime\": 1588627667.143063, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627664.688314}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:47 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=281.896239329 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:47 INFO 140149161203520] #progress_metric: host=algo-1, completed 87 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:47 INFO 140149161203520] #quality_metric: host=algo-1, epoch=350, train loss <loss>=3.04750789296\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:47 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:47 INFO 140149161203520] Epoch[351] Batch[0] avg_epoch_loss=3.010492\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:47 INFO 140149161203520] #quality_metric: host=algo-1, epoch=351, batch=0 train loss <loss>=3.01049208641\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:48 INFO 140149161203520] Epoch[351] Batch[5] avg_epoch_loss=3.091883\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:48 INFO 140149161203520] #quality_metric: host=algo-1, epoch=351, batch=5 train loss <loss>=3.09188298384\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:48 INFO 140149161203520] Epoch[351] Batch [5]#011Speed: 316.51 samples/sec#011loss=3.091883\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:49 INFO 140149161203520] Epoch[351] Batch[10] avg_epoch_loss=2.986985\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:49 INFO 140149161203520] #quality_metric: host=algo-1, epoch=351, batch=10 train loss <loss>=2.86110634804\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:49 INFO 140149161203520] Epoch[351] Batch [10]#011Speed: 315.69 samples/sec#011loss=2.861106\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:49 INFO 140149161203520] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2496.441125869751, \"sum\": 2496.441125869751, \"min\": 2496.441125869751}}, \"EndTime\": 1588627669.64004, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627667.143151}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:49 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=259.556099515 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:49 INFO 140149161203520] #progress_metric: host=algo-1, completed 88 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:49 INFO 140149161203520] #quality_metric: host=algo-1, epoch=351, train loss <loss>=2.98698451302\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:49 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:50 INFO 140149161203520] Epoch[352] Batch[0] avg_epoch_loss=2.929960\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:50 INFO 140149161203520] #quality_metric: host=algo-1, epoch=352, batch=0 train loss <loss>=2.9299595356\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:51 INFO 140149161203520] Epoch[352] Batch[5] avg_epoch_loss=2.940078\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:51 INFO 140149161203520] #quality_metric: host=algo-1, epoch=352, batch=5 train loss <loss>=2.94007805983\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:51 INFO 140149161203520] Epoch[352] Batch [5]#011Speed: 321.16 samples/sec#011loss=2.940078\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:52 INFO 140149161203520] Epoch[352] Batch[10] avg_epoch_loss=3.080865\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:52 INFO 140149161203520] #quality_metric: host=algo-1, epoch=352, batch=10 train loss <loss>=3.24980950356\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:52 INFO 140149161203520] Epoch[352] Batch [10]#011Speed: 319.26 samples/sec#011loss=3.249810\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:52 INFO 140149161203520] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2467.895984649658, \"sum\": 2467.895984649658, \"min\": 2467.895984649658}}, \"EndTime\": 1588627672.108476, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627669.640129}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:52 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=262.55875572 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:52 INFO 140149161203520] #progress_metric: host=algo-1, completed 88 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:52 INFO 140149161203520] #quality_metric: host=algo-1, epoch=352, train loss <loss>=3.08086507971\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:52 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:52 INFO 140149161203520] Epoch[353] Batch[0] avg_epoch_loss=3.058874\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:52 INFO 140149161203520] #quality_metric: host=algo-1, epoch=353, batch=0 train loss <loss>=3.05887389183\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:53 INFO 140149161203520] Epoch[353] Batch[5] avg_epoch_loss=3.015932\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:53 INFO 140149161203520] #quality_metric: host=algo-1, epoch=353, batch=5 train loss <loss>=3.01593152682\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:53 INFO 140149161203520] Epoch[353] Batch [5]#011Speed: 318.06 samples/sec#011loss=3.015932\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:54 INFO 140149161203520] Epoch[353] Batch[10] avg_epoch_loss=2.938708\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:54 INFO 140149161203520] #quality_metric: host=algo-1, epoch=353, batch=10 train loss <loss>=2.84603900909\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:54 INFO 140149161203520] Epoch[353] Batch [10]#011Speed: 320.06 samples/sec#011loss=2.846039\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:54 INFO 140149161203520] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2455.108165740967, \"sum\": 2455.108165740967, \"min\": 2455.108165740967}}, \"EndTime\": 1588627674.564152, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627672.10856}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:54 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=261.889060367 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:54 INFO 140149161203520] #progress_metric: host=algo-1, completed 88 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:54 INFO 140149161203520] #quality_metric: host=algo-1, epoch=353, train loss <loss>=2.93870765513\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:54 INFO 140149161203520] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:54 INFO 140149161203520] Saved checkpoint to \"/opt/ml/model/state_69a39925-148a-4403-b522-ab1630cae3e8-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 94.72799301147461, \"sum\": 94.72799301147461, \"min\": 94.72799301147461}}, \"EndTime\": 1588627674.659474, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627674.564242}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:55 INFO 140149161203520] Epoch[354] Batch[0] avg_epoch_loss=3.097543\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:55 INFO 140149161203520] #quality_metric: host=algo-1, epoch=354, batch=0 train loss <loss>=3.09754252434\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:56 INFO 140149161203520] Epoch[354] Batch[5] avg_epoch_loss=3.064278\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:56 INFO 140149161203520] #quality_metric: host=algo-1, epoch=354, batch=5 train loss <loss>=3.06427824497\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:56 INFO 140149161203520] Epoch[354] Batch [5]#011Speed: 327.32 samples/sec#011loss=3.064278\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:56 INFO 140149161203520] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2215.843915939331, \"sum\": 2215.843915939331, \"min\": 2215.843915939331}}, \"EndTime\": 1588627676.875483, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627674.659564}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:56 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=280.687933771 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:56 INFO 140149161203520] #progress_metric: host=algo-1, completed 88 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:56 INFO 140149161203520] #quality_metric: host=algo-1, epoch=354, train loss <loss>=3.05893023014\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:56 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:57 INFO 140149161203520] Epoch[355] Batch[0] avg_epoch_loss=3.008235\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:57 INFO 140149161203520] #quality_metric: host=algo-1, epoch=355, batch=0 train loss <loss>=3.00823521614\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:58 INFO 140149161203520] Epoch[355] Batch[5] avg_epoch_loss=2.982101\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:58 INFO 140149161203520] #quality_metric: host=algo-1, epoch=355, batch=5 train loss <loss>=2.98210116227\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:58 INFO 140149161203520] Epoch[355] Batch [5]#011Speed: 319.57 samples/sec#011loss=2.982101\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:59 INFO 140149161203520] Epoch[355] Batch[10] avg_epoch_loss=2.956302\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:59 INFO 140149161203520] #quality_metric: host=algo-1, epoch=355, batch=10 train loss <loss>=2.92534236908\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:59 INFO 140149161203520] Epoch[355] Batch [10]#011Speed: 324.54 samples/sec#011loss=2.925342\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:59 INFO 140149161203520] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2451.9691467285156, \"sum\": 2451.9691467285156, \"min\": 2451.9691467285156}}, \"EndTime\": 1588627679.328036, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627676.875579}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:59 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=269.157489726 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:59 INFO 140149161203520] #progress_metric: host=algo-1, completed 89 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:59 INFO 140149161203520] #quality_metric: host=algo-1, epoch=355, train loss <loss>=2.95630171082\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:59 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:59 INFO 140149161203520] Epoch[356] Batch[0] avg_epoch_loss=2.995679\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:27:59 INFO 140149161203520] #quality_metric: host=algo-1, epoch=356, batch=0 train loss <loss>=2.99567937851\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:00 INFO 140149161203520] Epoch[356] Batch[5] avg_epoch_loss=3.074254\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:00 INFO 140149161203520] #quality_metric: host=algo-1, epoch=356, batch=5 train loss <loss>=3.07425363859\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:00 INFO 140149161203520] Epoch[356] Batch [5]#011Speed: 322.86 samples/sec#011loss=3.074254\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:01 INFO 140149161203520] processed a total of 612 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2229.2640209198, \"sum\": 2229.2640209198, \"min\": 2229.2640209198}}, \"EndTime\": 1588627681.55781, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627679.328124}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:01 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=274.513331881 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:01 INFO 140149161203520] #progress_metric: host=algo-1, completed 89 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:01 INFO 140149161203520] #quality_metric: host=algo-1, epoch=356, train loss <loss>=3.063788867\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:01 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:02 INFO 140149161203520] Epoch[357] Batch[0] avg_epoch_loss=2.965010\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:02 INFO 140149161203520] #quality_metric: host=algo-1, epoch=357, batch=0 train loss <loss>=2.96500992775\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:03 INFO 140149161203520] Epoch[357] Batch[5] avg_epoch_loss=3.027153\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:03 INFO 140149161203520] #quality_metric: host=algo-1, epoch=357, batch=5 train loss <loss>=3.02715313435\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:03 INFO 140149161203520] Epoch[357] Batch [5]#011Speed: 320.21 samples/sec#011loss=3.027153\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:03 INFO 140149161203520] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2349.2870330810547, \"sum\": 2349.2870330810547, \"min\": 2349.2870330810547}}, \"EndTime\": 1588627683.907683, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627681.557904}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:03 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=265.170826019 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:03 INFO 140149161203520] #progress_metric: host=algo-1, completed 89 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:03 INFO 140149161203520] #quality_metric: host=algo-1, epoch=357, train loss <loss>=3.00251393318\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:03 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:04 INFO 140149161203520] Epoch[358] Batch[0] avg_epoch_loss=3.057638\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:04 INFO 140149161203520] #quality_metric: host=algo-1, epoch=358, batch=0 train loss <loss>=3.05763840675\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:05 INFO 140149161203520] Epoch[358] Batch[5] avg_epoch_loss=2.986600\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:05 INFO 140149161203520] #quality_metric: host=algo-1, epoch=358, batch=5 train loss <loss>=2.98660000165\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:05 INFO 140149161203520] Epoch[358] Batch [5]#011Speed: 326.19 samples/sec#011loss=2.986600\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:06 INFO 140149161203520] Epoch[358] Batch[10] avg_epoch_loss=3.090543\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:06 INFO 140149161203520] #quality_metric: host=algo-1, epoch=358, batch=10 train loss <loss>=3.21527547836\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:06 INFO 140149161203520] Epoch[358] Batch [10]#011Speed: 319.05 samples/sec#011loss=3.215275\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:06 INFO 140149161203520] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2452.932119369507, \"sum\": 2452.932119369507, \"min\": 2452.932119369507}}, \"EndTime\": 1588627686.361245, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627683.907782}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:06 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=263.75263117 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:06 INFO 140149161203520] #progress_metric: host=algo-1, completed 89 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:06 INFO 140149161203520] #quality_metric: host=algo-1, epoch=358, train loss <loss>=3.09054340016\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:06 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:06 INFO 140149161203520] Epoch[359] Batch[0] avg_epoch_loss=3.047821\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:06 INFO 140149161203520] #quality_metric: host=algo-1, epoch=359, batch=0 train loss <loss>=3.0478208065\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:07 INFO 140149161203520] Epoch[359] Batch[5] avg_epoch_loss=3.037268\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:07 INFO 140149161203520] #quality_metric: host=algo-1, epoch=359, batch=5 train loss <loss>=3.03726843993\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:07 INFO 140149161203520] Epoch[359] Batch [5]#011Speed: 324.80 samples/sec#011loss=3.037268\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:08 INFO 140149161203520] processed a total of 619 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2260.834217071533, \"sum\": 2260.834217071533, \"min\": 2260.834217071533}}, \"EndTime\": 1588627688.622644, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627686.361329}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:08 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=273.775823634 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:08 INFO 140149161203520] #progress_metric: host=algo-1, completed 90 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:08 INFO 140149161203520] #quality_metric: host=algo-1, epoch=359, train loss <loss>=2.98978877068\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:08 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:09 INFO 140149161203520] Epoch[360] Batch[0] avg_epoch_loss=2.924138\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:09 INFO 140149161203520] #quality_metric: host=algo-1, epoch=360, batch=0 train loss <loss>=2.92413759232\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:10 INFO 140149161203520] Epoch[360] Batch[5] avg_epoch_loss=3.060151\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:10 INFO 140149161203520] #quality_metric: host=algo-1, epoch=360, batch=5 train loss <loss>=3.06015090148\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:10 INFO 140149161203520] Epoch[360] Batch [5]#011Speed: 324.43 samples/sec#011loss=3.060151\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:11 INFO 140149161203520] Epoch[360] Batch[10] avg_epoch_loss=3.034231\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:11 INFO 140149161203520] #quality_metric: host=algo-1, epoch=360, batch=10 train loss <loss>=3.00312728882\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:11 INFO 140149161203520] Epoch[360] Batch [10]#011Speed: 317.24 samples/sec#011loss=3.003127\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:11 INFO 140149161203520] processed a total of 680 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2484.062910079956, \"sum\": 2484.062910079956, \"min\": 2484.062910079956}}, \"EndTime\": 1588627691.107324, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627688.622742}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:11 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=273.730971648 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:11 INFO 140149161203520] #progress_metric: host=algo-1, completed 90 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:11 INFO 140149161203520] #quality_metric: host=algo-1, epoch=360, train loss <loss>=3.03423107754\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:11 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:11 INFO 140149161203520] Epoch[361] Batch[0] avg_epoch_loss=2.942492\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:11 INFO 140149161203520] #quality_metric: host=algo-1, epoch=361, batch=0 train loss <loss>=2.94249224663\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:12 INFO 140149161203520] Epoch[361] Batch[5] avg_epoch_loss=3.012306\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:12 INFO 140149161203520] #quality_metric: host=algo-1, epoch=361, batch=5 train loss <loss>=3.01230621338\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:12 INFO 140149161203520] Epoch[361] Batch [5]#011Speed: 316.88 samples/sec#011loss=3.012306\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:13 INFO 140149161203520] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2281.0869216918945, \"sum\": 2281.0869216918945, \"min\": 2281.0869216918945}}, \"EndTime\": 1588627693.388949, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627691.107413}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:13 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=275.728678564 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:13 INFO 140149161203520] #progress_metric: host=algo-1, completed 90 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:13 INFO 140149161203520] #quality_metric: host=algo-1, epoch=361, train loss <loss>=2.99464826584\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:13 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:13 INFO 140149161203520] Epoch[362] Batch[0] avg_epoch_loss=2.885153\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:13 INFO 140149161203520] #quality_metric: host=algo-1, epoch=362, batch=0 train loss <loss>=2.88515305519\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:14 INFO 140149161203520] Epoch[362] Batch[5] avg_epoch_loss=3.023800\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:14 INFO 140149161203520] #quality_metric: host=algo-1, epoch=362, batch=5 train loss <loss>=3.02380005519\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:14 INFO 140149161203520] Epoch[362] Batch [5]#011Speed: 326.57 samples/sec#011loss=3.023800\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:15 INFO 140149161203520] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2230.3431034088135, \"sum\": 2230.3431034088135, \"min\": 2230.3431034088135}}, \"EndTime\": 1588627695.619896, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627693.389049}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:15 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=282.44990662 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:15 INFO 140149161203520] #progress_metric: host=algo-1, completed 90 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:15 INFO 140149161203520] #quality_metric: host=algo-1, epoch=362, train loss <loss>=3.00002901554\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:15 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:16 INFO 140149161203520] Epoch[363] Batch[0] avg_epoch_loss=3.091361\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:16 INFO 140149161203520] #quality_metric: host=algo-1, epoch=363, batch=0 train loss <loss>=3.09136080742\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:17 INFO 140149161203520] Epoch[363] Batch[5] avg_epoch_loss=3.029432\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:17 INFO 140149161203520] #quality_metric: host=algo-1, epoch=363, batch=5 train loss <loss>=3.02943241596\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:17 INFO 140149161203520] Epoch[363] Batch [5]#011Speed: 321.91 samples/sec#011loss=3.029432\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:17 INFO 140149161203520] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2255.6209564208984, \"sum\": 2255.6209564208984, \"min\": 2255.6209564208984}}, \"EndTime\": 1588627697.87611, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627695.619994}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:17 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=276.627985296 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:17 INFO 140149161203520] #progress_metric: host=algo-1, completed 91 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:17 INFO 140149161203520] #quality_metric: host=algo-1, epoch=363, train loss <loss>=3.03630228043\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:17 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:18 INFO 140149161203520] Epoch[364] Batch[0] avg_epoch_loss=2.998865\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:18 INFO 140149161203520] #quality_metric: host=algo-1, epoch=364, batch=0 train loss <loss>=2.99886512756\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:19 INFO 140149161203520] Epoch[364] Batch[5] avg_epoch_loss=2.996635\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:19 INFO 140149161203520] #quality_metric: host=algo-1, epoch=364, batch=5 train loss <loss>=2.99663507938\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:19 INFO 140149161203520] Epoch[364] Batch [5]#011Speed: 324.44 samples/sec#011loss=2.996635\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:20 INFO 140149161203520] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2233.5150241851807, \"sum\": 2233.5150241851807, \"min\": 2233.5150241851807}}, \"EndTime\": 1588627700.110243, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627697.876185}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:20 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=286.527428175 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:20 INFO 140149161203520] #progress_metric: host=algo-1, completed 91 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:20 INFO 140149161203520] #quality_metric: host=algo-1, epoch=364, train loss <loss>=2.99509344101\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:20 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:20 INFO 140149161203520] Epoch[365] Batch[0] avg_epoch_loss=3.182264\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:20 INFO 140149161203520] #quality_metric: host=algo-1, epoch=365, batch=0 train loss <loss>=3.182264328\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:21 INFO 140149161203520] Epoch[365] Batch[5] avg_epoch_loss=3.013109\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:21 INFO 140149161203520] #quality_metric: host=algo-1, epoch=365, batch=5 train loss <loss>=3.01310877005\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:21 INFO 140149161203520] Epoch[365] Batch [5]#011Speed: 322.14 samples/sec#011loss=3.013109\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:22 INFO 140149161203520] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2230.280876159668, \"sum\": 2230.280876159668, \"min\": 2230.280876159668}}, \"EndTime\": 1588627702.341132, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627700.110334}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:22 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=277.976677255 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:22 INFO 140149161203520] #progress_metric: host=algo-1, completed 91 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:22 INFO 140149161203520] #quality_metric: host=algo-1, epoch=365, train loss <loss>=3.01695494652\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:22 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:22 INFO 140149161203520] Epoch[366] Batch[0] avg_epoch_loss=3.111684\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:22 INFO 140149161203520] #quality_metric: host=algo-1, epoch=366, batch=0 train loss <loss>=3.11168384552\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:23 INFO 140149161203520] Epoch[366] Batch[5] avg_epoch_loss=2.979946\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:23 INFO 140149161203520] #quality_metric: host=algo-1, epoch=366, batch=5 train loss <loss>=2.97994645437\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:23 INFO 140149161203520] Epoch[366] Batch [5]#011Speed: 320.16 samples/sec#011loss=2.979946\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:24 INFO 140149161203520] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2244.7121143341064, \"sum\": 2244.7121143341064, \"min\": 2244.7121143341064}}, \"EndTime\": 1588627704.586456, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627702.341218}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:24 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=283.311723391 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:24 INFO 140149161203520] #progress_metric: host=algo-1, completed 91 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:24 INFO 140149161203520] #quality_metric: host=algo-1, epoch=366, train loss <loss>=3.03129858971\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:24 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:25 INFO 140149161203520] Epoch[367] Batch[0] avg_epoch_loss=3.252181\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:25 INFO 140149161203520] #quality_metric: host=algo-1, epoch=367, batch=0 train loss <loss>=3.25218081474\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:26 INFO 140149161203520] Epoch[367] Batch[5] avg_epoch_loss=3.050270\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:26 INFO 140149161203520] #quality_metric: host=algo-1, epoch=367, batch=5 train loss <loss>=3.05027004083\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:26 INFO 140149161203520] Epoch[367] Batch [5]#011Speed: 326.11 samples/sec#011loss=3.050270\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:27 INFO 140149161203520] Epoch[367] Batch[10] avg_epoch_loss=3.029553\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:27 INFO 140149161203520] #quality_metric: host=algo-1, epoch=367, batch=10 train loss <loss>=3.00469346046\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:27 INFO 140149161203520] Epoch[367] Batch [10]#011Speed: 320.51 samples/sec#011loss=3.004693\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:27 INFO 140149161203520] processed a total of 698 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2473.0660915374756, \"sum\": 2473.0660915374756, \"min\": 2473.0660915374756}}, \"EndTime\": 1588627707.060168, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627704.586577}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:27 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=282.226835004 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:27 INFO 140149161203520] #progress_metric: host=algo-1, completed 92 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:27 INFO 140149161203520] #quality_metric: host=algo-1, epoch=367, train loss <loss>=3.02955341339\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:27 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:27 INFO 140149161203520] Epoch[368] Batch[0] avg_epoch_loss=2.989204\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:27 INFO 140149161203520] #quality_metric: host=algo-1, epoch=368, batch=0 train loss <loss>=2.9892039299\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:28 INFO 140149161203520] Epoch[368] Batch[5] avg_epoch_loss=3.045520\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:28 INFO 140149161203520] #quality_metric: host=algo-1, epoch=368, batch=5 train loss <loss>=3.04552030563\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:28 INFO 140149161203520] Epoch[368] Batch [5]#011Speed: 323.94 samples/sec#011loss=3.045520\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:29 INFO 140149161203520] Epoch[368] Batch[10] avg_epoch_loss=3.072700\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:29 INFO 140149161203520] #quality_metric: host=algo-1, epoch=368, batch=10 train loss <loss>=3.10531516075\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:29 INFO 140149161203520] Epoch[368] Batch [10]#011Speed: 326.22 samples/sec#011loss=3.105315\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:29 INFO 140149161203520] processed a total of 671 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2444.8721408843994, \"sum\": 2444.8721408843994, \"min\": 2444.8721408843994}}, \"EndTime\": 1588627709.505631, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627707.060246}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:29 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=274.437394455 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:29 INFO 140149161203520] #progress_metric: host=algo-1, completed 92 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:29 INFO 140149161203520] #quality_metric: host=algo-1, epoch=368, train loss <loss>=3.07269978523\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:29 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:29 INFO 140149161203520] Epoch[369] Batch[0] avg_epoch_loss=3.032807\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:29 INFO 140149161203520] #quality_metric: host=algo-1, epoch=369, batch=0 train loss <loss>=3.03280711174\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:30 INFO 140149161203520] Epoch[369] Batch[5] avg_epoch_loss=3.007160\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:30 INFO 140149161203520] #quality_metric: host=algo-1, epoch=369, batch=5 train loss <loss>=3.00716006756\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:30 INFO 140149161203520] Epoch[369] Batch [5]#011Speed: 322.82 samples/sec#011loss=3.007160\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:31 INFO 140149161203520] Epoch[369] Batch[10] avg_epoch_loss=2.973630\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:31 INFO 140149161203520] #quality_metric: host=algo-1, epoch=369, batch=10 train loss <loss>=2.93339314461\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:31 INFO 140149161203520] Epoch[369] Batch [10]#011Speed: 320.43 samples/sec#011loss=2.933393\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:31 INFO 140149161203520] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2454.752206802368, \"sum\": 2454.752206802368, \"min\": 2454.752206802368}}, \"EndTime\": 1588627711.960924, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627709.505721}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:31 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=267.629747886 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:31 INFO 140149161203520] #progress_metric: host=algo-1, completed 92 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:31 INFO 140149161203520] #quality_metric: host=algo-1, epoch=369, train loss <loss>=2.97362964804\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:31 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:32 INFO 140149161203520] Epoch[370] Batch[0] avg_epoch_loss=2.843729\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:32 INFO 140149161203520] #quality_metric: host=algo-1, epoch=370, batch=0 train loss <loss>=2.84372925758\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:33 INFO 140149161203520] Epoch[370] Batch[5] avg_epoch_loss=2.999289\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:33 INFO 140149161203520] #quality_metric: host=algo-1, epoch=370, batch=5 train loss <loss>=2.99928935369\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:33 INFO 140149161203520] Epoch[370] Batch [5]#011Speed: 319.07 samples/sec#011loss=2.999289\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:34 INFO 140149161203520] processed a total of 619 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2239.8149967193604, \"sum\": 2239.8149967193604, \"min\": 2239.8149967193604}}, \"EndTime\": 1588627714.201328, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627711.961015}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:34 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=276.345196614 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:34 INFO 140149161203520] #progress_metric: host=algo-1, completed 92 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:34 INFO 140149161203520] #quality_metric: host=algo-1, epoch=370, train loss <loss>=3.02756240368\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:34 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:34 INFO 140149161203520] Epoch[371] Batch[0] avg_epoch_loss=2.959341\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:34 INFO 140149161203520] #quality_metric: host=algo-1, epoch=371, batch=0 train loss <loss>=2.95934128761\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:35 INFO 140149161203520] Epoch[371] Batch[5] avg_epoch_loss=3.004458\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:35 INFO 140149161203520] #quality_metric: host=algo-1, epoch=371, batch=5 train loss <loss>=3.00445826848\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:35 INFO 140149161203520] Epoch[371] Batch [5]#011Speed: 321.38 samples/sec#011loss=3.004458\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:36 INFO 140149161203520] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2228.813886642456, \"sum\": 2228.813886642456, \"min\": 2228.813886642456}}, \"EndTime\": 1588627716.430763, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627714.201422}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:36 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=281.746254513 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:36 INFO 140149161203520] #progress_metric: host=algo-1, completed 93 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:36 INFO 140149161203520] #quality_metric: host=algo-1, epoch=371, train loss <loss>=3.00243811607\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:36 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:36 INFO 140149161203520] Epoch[372] Batch[0] avg_epoch_loss=3.036740\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:36 INFO 140149161203520] #quality_metric: host=algo-1, epoch=372, batch=0 train loss <loss>=3.03673958778\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:37 INFO 140149161203520] Epoch[372] Batch[5] avg_epoch_loss=3.010831\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:37 INFO 140149161203520] #quality_metric: host=algo-1, epoch=372, batch=5 train loss <loss>=3.0108311971\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:37 INFO 140149161203520] Epoch[372] Batch [5]#011Speed: 321.63 samples/sec#011loss=3.010831\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:38 INFO 140149161203520] processed a total of 597 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2269.7389125823975, \"sum\": 2269.7389125823975, \"min\": 2269.7389125823975}}, \"EndTime\": 1588627718.701138, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627716.430861}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:38 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=263.013110744 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:38 INFO 140149161203520] #progress_metric: host=algo-1, completed 93 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:38 INFO 140149161203520] #quality_metric: host=algo-1, epoch=372, train loss <loss>=2.94711539745\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:38 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:39 INFO 140149161203520] Epoch[373] Batch[0] avg_epoch_loss=3.131386\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:39 INFO 140149161203520] #quality_metric: host=algo-1, epoch=373, batch=0 train loss <loss>=3.13138604164\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:40 INFO 140149161203520] Epoch[373] Batch[5] avg_epoch_loss=3.060098\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:40 INFO 140149161203520] #quality_metric: host=algo-1, epoch=373, batch=5 train loss <loss>=3.06009844939\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:40 INFO 140149161203520] Epoch[373] Batch [5]#011Speed: 322.32 samples/sec#011loss=3.060098\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:40 INFO 140149161203520] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2253.5688877105713, \"sum\": 2253.5688877105713, \"min\": 2253.5688877105713}}, \"EndTime\": 1588627720.95531, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627718.701212}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:40 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=280.430230818 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:40 INFO 140149161203520] #progress_metric: host=algo-1, completed 93 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:40 INFO 140149161203520] #quality_metric: host=algo-1, epoch=373, train loss <loss>=3.04001097679\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:40 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:41 INFO 140149161203520] Epoch[374] Batch[0] avg_epoch_loss=2.848388\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:41 INFO 140149161203520] #quality_metric: host=algo-1, epoch=374, batch=0 train loss <loss>=2.84838843346\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:42 INFO 140149161203520] Epoch[374] Batch[5] avg_epoch_loss=2.962573\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:42 INFO 140149161203520] #quality_metric: host=algo-1, epoch=374, batch=5 train loss <loss>=2.96257317066\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:42 INFO 140149161203520] Epoch[374] Batch [5]#011Speed: 317.57 samples/sec#011loss=2.962573\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:43 INFO 140149161203520] processed a total of 616 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2263.5841369628906, \"sum\": 2263.5841369628906, \"min\": 2263.5841369628906}}, \"EndTime\": 1588627723.2195, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627720.955384}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:43 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=272.120146554 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:43 INFO 140149161203520] #progress_metric: host=algo-1, completed 93 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:43 INFO 140149161203520] #quality_metric: host=algo-1, epoch=374, train loss <loss>=2.97160704136\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:43 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:43 INFO 140149161203520] Epoch[375] Batch[0] avg_epoch_loss=3.024184\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:43 INFO 140149161203520] #quality_metric: host=algo-1, epoch=375, batch=0 train loss <loss>=3.02418398857\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:44 INFO 140149161203520] Epoch[375] Batch[5] avg_epoch_loss=2.979041\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:44 INFO 140149161203520] #quality_metric: host=algo-1, epoch=375, batch=5 train loss <loss>=2.97904098034\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:44 INFO 140149161203520] Epoch[375] Batch [5]#011Speed: 320.77 samples/sec#011loss=2.979041\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:45 INFO 140149161203520] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2261.850118637085, \"sum\": 2261.850118637085, \"min\": 2261.850118637085}}, \"EndTime\": 1588627725.481959, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627723.219577}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:45 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=282.052282365 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:45 INFO 140149161203520] #progress_metric: host=algo-1, completed 94 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:45 INFO 140149161203520] #quality_metric: host=algo-1, epoch=375, train loss <loss>=2.99755306244\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:45 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:45 INFO 140149161203520] Epoch[376] Batch[0] avg_epoch_loss=2.829490\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:45 INFO 140149161203520] #quality_metric: host=algo-1, epoch=376, batch=0 train loss <loss>=2.8294904232\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:46 INFO 140149161203520] Epoch[376] Batch[5] avg_epoch_loss=2.998861\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:46 INFO 140149161203520] #quality_metric: host=algo-1, epoch=376, batch=5 train loss <loss>=2.99886071682\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:46 INFO 140149161203520] Epoch[376] Batch [5]#011Speed: 322.19 samples/sec#011loss=2.998861\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:47 INFO 140149161203520] Epoch[376] Batch[10] avg_epoch_loss=3.127842\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:47 INFO 140149161203520] #quality_metric: host=algo-1, epoch=376, batch=10 train loss <loss>=3.28261852264\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:47 INFO 140149161203520] Epoch[376] Batch [10]#011Speed: 317.27 samples/sec#011loss=3.282619\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:47 INFO 140149161203520] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2454.97989654541, \"sum\": 2454.97989654541, \"min\": 2454.97989654541}}, \"EndTime\": 1588627727.937565, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627725.482056}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:47 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=265.163519024 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:47 INFO 140149161203520] #progress_metric: host=algo-1, completed 94 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:47 INFO 140149161203520] #quality_metric: host=algo-1, epoch=376, train loss <loss>=3.12784153765\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:47 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:48 INFO 140149161203520] Epoch[377] Batch[0] avg_epoch_loss=3.056477\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:48 INFO 140149161203520] #quality_metric: host=algo-1, epoch=377, batch=0 train loss <loss>=3.05647730827\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:49 INFO 140149161203520] Epoch[377] Batch[5] avg_epoch_loss=3.016008\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:49 INFO 140149161203520] #quality_metric: host=algo-1, epoch=377, batch=5 train loss <loss>=3.01600825787\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:49 INFO 140149161203520] Epoch[377] Batch [5]#011Speed: 320.27 samples/sec#011loss=3.016008\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:50 INFO 140149161203520] Epoch[377] Batch[10] avg_epoch_loss=3.021293\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:50 INFO 140149161203520] #quality_metric: host=algo-1, epoch=377, batch=10 train loss <loss>=3.02763462067\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:50 INFO 140149161203520] Epoch[377] Batch [10]#011Speed: 321.70 samples/sec#011loss=3.027635\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:50 INFO 140149161203520] processed a total of 675 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2455.0039768218994, \"sum\": 2455.0039768218994, \"min\": 2455.0039768218994}}, \"EndTime\": 1588627730.393128, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627727.93764}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:50 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=274.934300034 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:50 INFO 140149161203520] #progress_metric: host=algo-1, completed 94 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:50 INFO 140149161203520] #quality_metric: host=algo-1, epoch=377, train loss <loss>=3.02129296823\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:50 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:50 INFO 140149161203520] Epoch[378] Batch[0] avg_epoch_loss=3.067011\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:50 INFO 140149161203520] #quality_metric: host=algo-1, epoch=378, batch=0 train loss <loss>=3.06701135635\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:51 INFO 140149161203520] Epoch[378] Batch[5] avg_epoch_loss=3.007594\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:51 INFO 140149161203520] #quality_metric: host=algo-1, epoch=378, batch=5 train loss <loss>=3.00759351254\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:51 INFO 140149161203520] Epoch[378] Batch [5]#011Speed: 319.57 samples/sec#011loss=3.007594\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:52 INFO 140149161203520] Epoch[378] Batch[10] avg_epoch_loss=2.934504\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:52 INFO 140149161203520] #quality_metric: host=algo-1, epoch=378, batch=10 train loss <loss>=2.84679636955\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:52 INFO 140149161203520] Epoch[378] Batch [10]#011Speed: 320.53 samples/sec#011loss=2.846796\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:52 INFO 140149161203520] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2457.6339721679688, \"sum\": 2457.6339721679688, \"min\": 2457.6339721679688}}, \"EndTime\": 1588627732.851295, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627730.393215}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:52 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=262.840517199 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:52 INFO 140149161203520] #progress_metric: host=algo-1, completed 94 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:52 INFO 140149161203520] #quality_metric: host=algo-1, epoch=378, train loss <loss>=2.93450390209\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:52 INFO 140149161203520] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:52 INFO 140149161203520] Saved checkpoint to \"/opt/ml/model/state_b8ba0a26-df64-4eb9-b23c-1013c57494d7-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 72.55697250366211, \"sum\": 72.55697250366211, \"min\": 72.55697250366211}}, \"EndTime\": 1588627732.924444, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627732.851385}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:53 INFO 140149161203520] Epoch[379] Batch[0] avg_epoch_loss=2.855280\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:53 INFO 140149161203520] #quality_metric: host=algo-1, epoch=379, batch=0 train loss <loss>=2.85528039932\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:54 INFO 140149161203520] Epoch[379] Batch[5] avg_epoch_loss=3.009284\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:54 INFO 140149161203520] #quality_metric: host=algo-1, epoch=379, batch=5 train loss <loss>=3.00928405921\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:54 INFO 140149161203520] Epoch[379] Batch [5]#011Speed: 317.55 samples/sec#011loss=3.009284\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:55 INFO 140149161203520] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2289.159059524536, \"sum\": 2289.159059524536, \"min\": 2289.159059524536}}, \"EndTime\": 1588627735.213759, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627732.924524}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:55 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=269.517285279 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:55 INFO 140149161203520] #progress_metric: host=algo-1, completed 95 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:55 INFO 140149161203520] #quality_metric: host=algo-1, epoch=379, train loss <loss>=3.08695442677\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:55 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:55 INFO 140149161203520] Epoch[380] Batch[0] avg_epoch_loss=3.117787\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:55 INFO 140149161203520] #quality_metric: host=algo-1, epoch=380, batch=0 train loss <loss>=3.11778712273\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:56 INFO 140149161203520] Epoch[380] Batch[5] avg_epoch_loss=2.983801\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:56 INFO 140149161203520] #quality_metric: host=algo-1, epoch=380, batch=5 train loss <loss>=2.98380108674\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:56 INFO 140149161203520] Epoch[380] Batch [5]#011Speed: 318.60 samples/sec#011loss=2.983801\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:57 INFO 140149161203520] Epoch[380] Batch[10] avg_epoch_loss=3.012710\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:57 INFO 140149161203520] #quality_metric: host=algo-1, epoch=380, batch=10 train loss <loss>=3.04739999771\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:57 INFO 140149161203520] Epoch[380] Batch [10]#011Speed: 317.92 samples/sec#011loss=3.047400\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:57 INFO 140149161203520] processed a total of 688 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2472.414016723633, \"sum\": 2472.414016723633, \"min\": 2472.414016723633}}, \"EndTime\": 1588627737.686795, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627735.213835}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:57 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=278.257505924 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:57 INFO 140149161203520] #progress_metric: host=algo-1, completed 95 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:57 INFO 140149161203520] #quality_metric: host=algo-1, epoch=380, train loss <loss>=3.01270968264\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:57 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:58 INFO 140149161203520] Epoch[381] Batch[0] avg_epoch_loss=3.019200\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:58 INFO 140149161203520] #quality_metric: host=algo-1, epoch=381, batch=0 train loss <loss>=3.01919960976\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:59 INFO 140149161203520] Epoch[381] Batch[5] avg_epoch_loss=2.992875\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:59 INFO 140149161203520] #quality_metric: host=algo-1, epoch=381, batch=5 train loss <loss>=2.99287525813\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:28:59 INFO 140149161203520] Epoch[381] Batch [5]#011Speed: 284.80 samples/sec#011loss=2.992875\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:00 INFO 140149161203520] Epoch[381] Batch[10] avg_epoch_loss=2.915428\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:00 INFO 140149161203520] #quality_metric: host=algo-1, epoch=381, batch=10 train loss <loss>=2.8224916935\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:00 INFO 140149161203520] Epoch[381] Batch [10]#011Speed: 316.78 samples/sec#011loss=2.822492\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:00 INFO 140149161203520] processed a total of 673 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2626.328945159912, \"sum\": 2626.328945159912, \"min\": 2626.328945159912}}, \"EndTime\": 1588627740.313757, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627737.686872}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:00 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=256.239218499 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:00 INFO 140149161203520] #progress_metric: host=algo-1, completed 95 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:00 INFO 140149161203520] #quality_metric: host=algo-1, epoch=381, train loss <loss>=2.9154281833\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:00 INFO 140149161203520] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:00 INFO 140149161203520] Saved checkpoint to \"/opt/ml/model/state_62742138-7208-4d24-98ad-7b3204eb49bf-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 61.319828033447266, \"sum\": 61.319828033447266, \"min\": 61.319828033447266}}, \"EndTime\": 1588627740.375716, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627740.313841}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:00 INFO 140149161203520] Epoch[382] Batch[0] avg_epoch_loss=2.926119\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:00 INFO 140149161203520] #quality_metric: host=algo-1, epoch=382, batch=0 train loss <loss>=2.92611885071\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:01 INFO 140149161203520] Epoch[382] Batch[5] avg_epoch_loss=3.044444\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:01 INFO 140149161203520] #quality_metric: host=algo-1, epoch=382, batch=5 train loss <loss>=3.04444356759\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:01 INFO 140149161203520] Epoch[382] Batch [5]#011Speed: 322.40 samples/sec#011loss=3.044444\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:02 INFO 140149161203520] Epoch[382] Batch[10] avg_epoch_loss=3.031215\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:02 INFO 140149161203520] #quality_metric: host=algo-1, epoch=382, batch=10 train loss <loss>=3.01534047127\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:02 INFO 140149161203520] Epoch[382] Batch [10]#011Speed: 317.25 samples/sec#011loss=3.015340\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:02 INFO 140149161203520] processed a total of 688 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2465.794086456299, \"sum\": 2465.794086456299, \"min\": 2465.794086456299}}, \"EndTime\": 1588627742.841662, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627740.375795}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:02 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=279.003161418 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:02 INFO 140149161203520] #progress_metric: host=algo-1, completed 95 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:02 INFO 140149161203520] #quality_metric: host=algo-1, epoch=382, train loss <loss>=3.03121488745\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:02 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:03 INFO 140149161203520] Epoch[383] Batch[0] avg_epoch_loss=2.765973\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:03 INFO 140149161203520] #quality_metric: host=algo-1, epoch=383, batch=0 train loss <loss>=2.76597309113\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:04 INFO 140149161203520] Epoch[383] Batch[5] avg_epoch_loss=3.020884\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:04 INFO 140149161203520] #quality_metric: host=algo-1, epoch=383, batch=5 train loss <loss>=3.0208837986\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:04 INFO 140149161203520] Epoch[383] Batch [5]#011Speed: 317.55 samples/sec#011loss=3.020884\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:05 INFO 140149161203520] Epoch[383] Batch[10] avg_epoch_loss=2.917454\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:05 INFO 140149161203520] #quality_metric: host=algo-1, epoch=383, batch=10 train loss <loss>=2.79333724976\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:05 INFO 140149161203520] Epoch[383] Batch [10]#011Speed: 317.03 samples/sec#011loss=2.793337\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:05 INFO 140149161203520] processed a total of 676 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2560.793161392212, \"sum\": 2560.793161392212, \"min\": 2560.793161392212}}, \"EndTime\": 1588627745.402986, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627742.841749}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:05 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=263.965056266 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:05 INFO 140149161203520] #progress_metric: host=algo-1, completed 96 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:05 INFO 140149161203520] #quality_metric: host=algo-1, epoch=383, train loss <loss>=2.91745354912\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:05 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:05 INFO 140149161203520] Epoch[384] Batch[0] avg_epoch_loss=2.976304\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:05 INFO 140149161203520] #quality_metric: host=algo-1, epoch=384, batch=0 train loss <loss>=2.97630405426\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:06 INFO 140149161203520] Epoch[384] Batch[5] avg_epoch_loss=3.005084\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:06 INFO 140149161203520] #quality_metric: host=algo-1, epoch=384, batch=5 train loss <loss>=3.00508364042\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:06 INFO 140149161203520] Epoch[384] Batch [5]#011Speed: 322.38 samples/sec#011loss=3.005084\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:07 INFO 140149161203520] Epoch[384] Batch[10] avg_epoch_loss=3.061701\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:07 INFO 140149161203520] #quality_metric: host=algo-1, epoch=384, batch=10 train loss <loss>=3.12964172363\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:07 INFO 140149161203520] Epoch[384] Batch [10]#011Speed: 321.20 samples/sec#011loss=3.129642\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:07 INFO 140149161203520] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2469.1410064697266, \"sum\": 2469.1410064697266, \"min\": 2469.1410064697266}}, \"EndTime\": 1588627747.87269, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627745.403098}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:07 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=263.640672099 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:07 INFO 140149161203520] #progress_metric: host=algo-1, completed 96 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:07 INFO 140149161203520] #quality_metric: host=algo-1, epoch=384, train loss <loss>=3.06170095097\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:07 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:08 INFO 140149161203520] Epoch[385] Batch[0] avg_epoch_loss=2.953510\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:08 INFO 140149161203520] #quality_metric: host=algo-1, epoch=385, batch=0 train loss <loss>=2.95351004601\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:09 INFO 140149161203520] Epoch[385] Batch[5] avg_epoch_loss=3.026685\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:09 INFO 140149161203520] #quality_metric: host=algo-1, epoch=385, batch=5 train loss <loss>=3.02668491999\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:09 INFO 140149161203520] Epoch[385] Batch [5]#011Speed: 303.67 samples/sec#011loss=3.026685\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:10 INFO 140149161203520] Epoch[385] Batch[10] avg_epoch_loss=3.089466\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:10 INFO 140149161203520] #quality_metric: host=algo-1, epoch=385, batch=10 train loss <loss>=3.16480431557\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:10 INFO 140149161203520] Epoch[385] Batch [10]#011Speed: 315.90 samples/sec#011loss=3.164804\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:10 INFO 140149161203520] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2582.911968231201, \"sum\": 2582.911968231201, \"min\": 2582.911968231201}}, \"EndTime\": 1588627750.456126, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627747.872778}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:10 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=249.319875086 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:10 INFO 140149161203520] #progress_metric: host=algo-1, completed 96 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:10 INFO 140149161203520] #quality_metric: host=algo-1, epoch=385, train loss <loss>=3.08946646344\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:10 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:10 INFO 140149161203520] Epoch[386] Batch[0] avg_epoch_loss=2.959983\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:10 INFO 140149161203520] #quality_metric: host=algo-1, epoch=386, batch=0 train loss <loss>=2.95998311043\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:11 INFO 140149161203520] Epoch[386] Batch[5] avg_epoch_loss=3.041079\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:11 INFO 140149161203520] #quality_metric: host=algo-1, epoch=386, batch=5 train loss <loss>=3.04107896487\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:11 INFO 140149161203520] Epoch[386] Batch [5]#011Speed: 319.53 samples/sec#011loss=3.041079\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:12 INFO 140149161203520] processed a total of 606 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2250.108003616333, \"sum\": 2250.108003616333, \"min\": 2250.108003616333}}, \"EndTime\": 1588627752.706796, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627750.456205}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:12 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=269.304967972 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:12 INFO 140149161203520] #progress_metric: host=algo-1, completed 96 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:12 INFO 140149161203520] #quality_metric: host=algo-1, epoch=386, train loss <loss>=3.06440520287\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:12 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:13 INFO 140149161203520] Epoch[387] Batch[0] avg_epoch_loss=2.860661\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:13 INFO 140149161203520] #quality_metric: host=algo-1, epoch=387, batch=0 train loss <loss>=2.86066055298\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:14 INFO 140149161203520] Epoch[387] Batch[5] avg_epoch_loss=3.017953\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:14 INFO 140149161203520] #quality_metric: host=algo-1, epoch=387, batch=5 train loss <loss>=3.01795260111\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:14 INFO 140149161203520] Epoch[387] Batch [5]#011Speed: 306.58 samples/sec#011loss=3.017953\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:14 INFO 140149161203520] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2292.707920074463, \"sum\": 2292.707920074463, \"min\": 2292.707920074463}}, \"EndTime\": 1588627755.000196, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627752.706884}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:15 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=272.150746337 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:15 INFO 140149161203520] #progress_metric: host=algo-1, completed 97 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:15 INFO 140149161203520] #quality_metric: host=algo-1, epoch=387, train loss <loss>=3.0315350771\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:15 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:15 INFO 140149161203520] Epoch[388] Batch[0] avg_epoch_loss=3.088697\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:15 INFO 140149161203520] #quality_metric: host=algo-1, epoch=388, batch=0 train loss <loss>=3.08869671822\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:16 INFO 140149161203520] Epoch[388] Batch[5] avg_epoch_loss=2.972985\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:16 INFO 140149161203520] #quality_metric: host=algo-1, epoch=388, batch=5 train loss <loss>=2.97298467159\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:16 INFO 140149161203520] Epoch[388] Batch [5]#011Speed: 320.93 samples/sec#011loss=2.972985\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:17 INFO 140149161203520] Epoch[388] Batch[10] avg_epoch_loss=3.057070\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:17 INFO 140149161203520] #quality_metric: host=algo-1, epoch=388, batch=10 train loss <loss>=3.15797133446\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:17 INFO 140149161203520] Epoch[388] Batch [10]#011Speed: 318.26 samples/sec#011loss=3.157971\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:17 INFO 140149161203520] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2507.162094116211, \"sum\": 2507.162094116211, \"min\": 2507.162094116211}}, \"EndTime\": 1588627757.507938, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627755.000292}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:17 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=257.249650901 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:17 INFO 140149161203520] #progress_metric: host=algo-1, completed 97 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:17 INFO 140149161203520] #quality_metric: host=algo-1, epoch=388, train loss <loss>=3.05706951835\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:17 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:17 INFO 140149161203520] Epoch[389] Batch[0] avg_epoch_loss=3.067920\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:17 INFO 140149161203520] #quality_metric: host=algo-1, epoch=389, batch=0 train loss <loss>=3.06791973114\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:18 INFO 140149161203520] Epoch[389] Batch[5] avg_epoch_loss=3.022302\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:18 INFO 140149161203520] #quality_metric: host=algo-1, epoch=389, batch=5 train loss <loss>=3.02230246862\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:18 INFO 140149161203520] Epoch[389] Batch [5]#011Speed: 317.21 samples/sec#011loss=3.022302\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:19 INFO 140149161203520] Epoch[389] Batch[10] avg_epoch_loss=2.951440\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:19 INFO 140149161203520] #quality_metric: host=algo-1, epoch=389, batch=10 train loss <loss>=2.86640534401\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:19 INFO 140149161203520] Epoch[389] Batch [10]#011Speed: 318.53 samples/sec#011loss=2.866405\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:19 INFO 140149161203520] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2485.931158065796, \"sum\": 2485.931158065796, \"min\": 2485.931158065796}}, \"EndTime\": 1588627759.994411, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627757.508027}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:19 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=262.665257796 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:19 INFO 140149161203520] #progress_metric: host=algo-1, completed 97 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:19 INFO 140149161203520] #quality_metric: host=algo-1, epoch=389, train loss <loss>=2.95144013925\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:19 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:20 INFO 140149161203520] Epoch[390] Batch[0] avg_epoch_loss=2.997744\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:20 INFO 140149161203520] #quality_metric: host=algo-1, epoch=390, batch=0 train loss <loss>=2.99774360657\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:21 INFO 140149161203520] Epoch[390] Batch[5] avg_epoch_loss=3.017106\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:21 INFO 140149161203520] #quality_metric: host=algo-1, epoch=390, batch=5 train loss <loss>=3.01710617542\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:21 INFO 140149161203520] Epoch[390] Batch [5]#011Speed: 308.00 samples/sec#011loss=3.017106\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:22 INFO 140149161203520] Epoch[390] Batch[10] avg_epoch_loss=2.922985\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:22 INFO 140149161203520] #quality_metric: host=algo-1, epoch=390, batch=10 train loss <loss>=2.81003985405\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:22 INFO 140149161203520] Epoch[390] Batch [10]#011Speed: 312.38 samples/sec#011loss=2.810040\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:22 INFO 140149161203520] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2517.84610748291, \"sum\": 2517.84610748291, \"min\": 2517.84610748291}}, \"EndTime\": 1588627762.512872, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627759.994493}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:22 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=255.364044894 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:22 INFO 140149161203520] #progress_metric: host=algo-1, completed 97 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:22 INFO 140149161203520] #quality_metric: host=algo-1, epoch=390, train loss <loss>=2.92298512025\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:22 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:22 INFO 140149161203520] Epoch[391] Batch[0] avg_epoch_loss=2.945035\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:23 INFO 140149161203520] #quality_metric: host=algo-1, epoch=391, batch=0 train loss <loss>=2.94503545761\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:24 INFO 140149161203520] Epoch[391] Batch[5] avg_epoch_loss=2.978822\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:24 INFO 140149161203520] #quality_metric: host=algo-1, epoch=391, batch=5 train loss <loss>=2.97882155577\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:24 INFO 140149161203520] Epoch[391] Batch [5]#011Speed: 316.71 samples/sec#011loss=2.978822\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:24 INFO 140149161203520] processed a total of 591 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2294.5871353149414, \"sum\": 2294.5871353149414, \"min\": 2294.5871353149414}}, \"EndTime\": 1588627764.807998, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627762.51296}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:24 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=257.54675234 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:24 INFO 140149161203520] #progress_metric: host=algo-1, completed 98 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:24 INFO 140149161203520] #quality_metric: host=algo-1, epoch=391, train loss <loss>=3.03098163605\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:24 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:25 INFO 140149161203520] Epoch[392] Batch[0] avg_epoch_loss=2.965911\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:25 INFO 140149161203520] #quality_metric: host=algo-1, epoch=392, batch=0 train loss <loss>=2.96591114998\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:26 INFO 140149161203520] Epoch[392] Batch[5] avg_epoch_loss=3.004224\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:26 INFO 140149161203520] #quality_metric: host=algo-1, epoch=392, batch=5 train loss <loss>=3.00422374407\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:26 INFO 140149161203520] Epoch[392] Batch [5]#011Speed: 318.33 samples/sec#011loss=3.004224\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:27 INFO 140149161203520] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2269.9878215789795, \"sum\": 2269.9878215789795, \"min\": 2269.9878215789795}}, \"EndTime\": 1588627767.078563, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627764.808097}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:27 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=277.076740367 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:27 INFO 140149161203520] #progress_metric: host=algo-1, completed 98 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:27 INFO 140149161203520] #quality_metric: host=algo-1, epoch=392, train loss <loss>=2.96819913387\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:27 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:27 INFO 140149161203520] Epoch[393] Batch[0] avg_epoch_loss=3.025788\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:27 INFO 140149161203520] #quality_metric: host=algo-1, epoch=393, batch=0 train loss <loss>=3.02578830719\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:28 INFO 140149161203520] Epoch[393] Batch[5] avg_epoch_loss=2.940685\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:28 INFO 140149161203520] #quality_metric: host=algo-1, epoch=393, batch=5 train loss <loss>=2.94068535169\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:28 INFO 140149161203520] Epoch[393] Batch [5]#011Speed: 317.06 samples/sec#011loss=2.940685\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:29 INFO 140149161203520] Epoch[393] Batch[10] avg_epoch_loss=2.990665\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:29 INFO 140149161203520] #quality_metric: host=algo-1, epoch=393, batch=10 train loss <loss>=3.05063948631\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:29 INFO 140149161203520] Epoch[393] Batch [10]#011Speed: 319.54 samples/sec#011loss=3.050639\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:29 INFO 140149161203520] processed a total of 683 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2463.068962097168, \"sum\": 2463.068962097168, \"min\": 2463.068962097168}}, \"EndTime\": 1588627769.542318, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627767.078661}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:29 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=277.281923442 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:29 INFO 140149161203520] #progress_metric: host=algo-1, completed 98 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:29 INFO 140149161203520] #quality_metric: host=algo-1, epoch=393, train loss <loss>=2.99066450379\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:29 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:29 INFO 140149161203520] Epoch[394] Batch[0] avg_epoch_loss=2.880268\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:29 INFO 140149161203520] #quality_metric: host=algo-1, epoch=394, batch=0 train loss <loss>=2.88026833534\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:30 INFO 140149161203520] Epoch[394] Batch[5] avg_epoch_loss=2.880992\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:30 INFO 140149161203520] #quality_metric: host=algo-1, epoch=394, batch=5 train loss <loss>=2.88099241257\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:30 INFO 140149161203520] Epoch[394] Batch [5]#011Speed: 320.52 samples/sec#011loss=2.880992\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:31 INFO 140149161203520] processed a total of 611 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2238.697052001953, \"sum\": 2238.697052001953, \"min\": 2238.697052001953}}, \"EndTime\": 1588627771.7816, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627769.542401}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:31 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=272.912834483 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:31 INFO 140149161203520] #progress_metric: host=algo-1, completed 98 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:31 INFO 140149161203520] #quality_metric: host=algo-1, epoch=394, train loss <loss>=2.90486352444\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:31 INFO 140149161203520] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:31 INFO 140149161203520] Saved checkpoint to \"/opt/ml/model/state_e7cfb504-6012-4d74-8148-d1a2a41e9965-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 59.4019889831543, \"sum\": 59.4019889831543, \"min\": 59.4019889831543}}, \"EndTime\": 1588627771.841599, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627771.781678}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:32 INFO 140149161203520] Epoch[395] Batch[0] avg_epoch_loss=2.854659\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:32 INFO 140149161203520] #quality_metric: host=algo-1, epoch=395, batch=0 train loss <loss>=2.85465860367\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:33 INFO 140149161203520] Epoch[395] Batch[5] avg_epoch_loss=2.938901\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:33 INFO 140149161203520] #quality_metric: host=algo-1, epoch=395, batch=5 train loss <loss>=2.93890078863\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:33 INFO 140149161203520] Epoch[395] Batch [5]#011Speed: 315.42 samples/sec#011loss=2.938901\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:34 INFO 140149161203520] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2279.3519496917725, \"sum\": 2279.3519496917725, \"min\": 2279.3519496917725}}, \"EndTime\": 1588627774.121106, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627771.841683}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:34 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=276.37784276 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:34 INFO 140149161203520] #progress_metric: host=algo-1, completed 99 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:34 INFO 140149161203520] #quality_metric: host=algo-1, epoch=395, train loss <loss>=2.9089830637\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:34 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:34 INFO 140149161203520] Epoch[396] Batch[0] avg_epoch_loss=3.231608\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:34 INFO 140149161203520] #quality_metric: host=algo-1, epoch=396, batch=0 train loss <loss>=3.23160815239\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:35 INFO 140149161203520] Epoch[396] Batch[5] avg_epoch_loss=3.039094\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:35 INFO 140149161203520] #quality_metric: host=algo-1, epoch=396, batch=5 train loss <loss>=3.03909424941\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:35 INFO 140149161203520] Epoch[396] Batch [5]#011Speed: 323.04 samples/sec#011loss=3.039094\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:36 INFO 140149161203520] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2284.492015838623, \"sum\": 2284.492015838623, \"min\": 2284.492015838623}}, \"EndTime\": 1588627776.406191, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627774.1212}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:36 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=272.254029742 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:36 INFO 140149161203520] #progress_metric: host=algo-1, completed 99 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:36 INFO 140149161203520] #quality_metric: host=algo-1, epoch=396, train loss <loss>=3.04975008965\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:36 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:36 INFO 140149161203520] Epoch[397] Batch[0] avg_epoch_loss=3.246209\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:36 INFO 140149161203520] #quality_metric: host=algo-1, epoch=397, batch=0 train loss <loss>=3.24620914459\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:37 INFO 140149161203520] Epoch[397] Batch[5] avg_epoch_loss=2.989617\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:37 INFO 140149161203520] #quality_metric: host=algo-1, epoch=397, batch=5 train loss <loss>=2.98961691062\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:37 INFO 140149161203520] Epoch[397] Batch [5]#011Speed: 324.50 samples/sec#011loss=2.989617\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:38 INFO 140149161203520] Epoch[397] Batch[10] avg_epoch_loss=2.930670\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:38 INFO 140149161203520] #quality_metric: host=algo-1, epoch=397, batch=10 train loss <loss>=2.85993332863\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:38 INFO 140149161203520] Epoch[397] Batch [10]#011Speed: 317.57 samples/sec#011loss=2.859933\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:38 INFO 140149161203520] processed a total of 675 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2482.517957687378, \"sum\": 2482.517957687378, \"min\": 2482.517957687378}}, \"EndTime\": 1588627778.889288, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627776.406288}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:38 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=271.887673811 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:38 INFO 140149161203520] #progress_metric: host=algo-1, completed 99 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:38 INFO 140149161203520] #quality_metric: host=algo-1, epoch=397, train loss <loss>=2.93066982789\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:38 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:39 INFO 140149161203520] Epoch[398] Batch[0] avg_epoch_loss=2.949281\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:39 INFO 140149161203520] #quality_metric: host=algo-1, epoch=398, batch=0 train loss <loss>=2.94928050041\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:40 INFO 140149161203520] Epoch[398] Batch[5] avg_epoch_loss=3.006698\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:40 INFO 140149161203520] #quality_metric: host=algo-1, epoch=398, batch=5 train loss <loss>=3.00669821103\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:40 INFO 140149161203520] Epoch[398] Batch [5]#011Speed: 323.06 samples/sec#011loss=3.006698\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:41 INFO 140149161203520] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2336.4100456237793, \"sum\": 2336.4100456237793, \"min\": 2336.4100456237793}}, \"EndTime\": 1588627781.22624, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627778.889375}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:41 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=273.908923231 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:41 INFO 140149161203520] #progress_metric: host=algo-1, completed 99 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:41 INFO 140149161203520] #quality_metric: host=algo-1, epoch=398, train loss <loss>=3.03480184078\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:41 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:41 INFO 140149161203520] Epoch[399] Batch[0] avg_epoch_loss=2.911858\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:41 INFO 140149161203520] #quality_metric: host=algo-1, epoch=399, batch=0 train loss <loss>=2.91185808182\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:42 INFO 140149161203520] Epoch[399] Batch[5] avg_epoch_loss=2.967634\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:42 INFO 140149161203520] #quality_metric: host=algo-1, epoch=399, batch=5 train loss <loss>=2.96763443947\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:42 INFO 140149161203520] Epoch[399] Batch [5]#011Speed: 322.90 samples/sec#011loss=2.967634\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:43 INFO 140149161203520] Epoch[399] Batch[10] avg_epoch_loss=2.989427\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:43 INFO 140149161203520] #quality_metric: host=algo-1, epoch=399, batch=10 train loss <loss>=3.01557717323\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:43 INFO 140149161203520] Epoch[399] Batch [10]#011Speed: 320.25 samples/sec#011loss=3.015577\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:43 INFO 140149161203520] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2455.1830291748047, \"sum\": 2455.1830291748047, \"min\": 2455.1830291748047}}, \"EndTime\": 1588627783.68197, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627781.226334}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:43 INFO 140149161203520] #throughput_metric: host=algo-1, train throughput=266.361259331 records/second\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:43 INFO 140149161203520] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:43 INFO 140149161203520] #quality_metric: host=algo-1, epoch=399, train loss <loss>=2.98942659118\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:43 INFO 140149161203520] loss did not improve\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:43 INFO 140149161203520] Final loss: 2.90486352444 (occurred at epoch 394)\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:43 INFO 140149161203520] #quality_metric: host=algo-1, train final_loss <loss>=2.90486352444\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:43 INFO 140149161203520] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:43 WARNING 140149161203520] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:43 INFO 140149161203520] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 825.0339031219482, \"sum\": 825.0339031219482, \"min\": 825.0339031219482}}, \"EndTime\": 1588627784.507955, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627783.682058}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:44 INFO 140149161203520] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"finalize.time\": {\"count\": 1, \"max\": 1060.1089000701904, \"sum\": 1060.1089000701904, \"min\": 1060.1089000701904}}, \"EndTime\": 1588627784.742984, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627784.508041}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:44 INFO 140149161203520] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:44 INFO 140149161203520] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.serialize.time\": {\"count\": 1, \"max\": 35.980939865112305, \"sum\": 35.980939865112305, \"min\": 35.980939865112305}}, \"EndTime\": 1588627784.779115, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627784.743083}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:44 INFO 140149161203520] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:44 INFO 140149161203520] Evaluating model accuracy on testset using 100 samples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.bind.time\": {\"count\": 1, \"max\": 0.030040740966796875, \"sum\": 0.030040740966796875, \"min\": 0.030040740966796875}}, \"EndTime\": 1588627784.779936, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627784.779173}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.score.time\": {\"count\": 1, \"max\": 3214.570999145508, \"sum\": 3214.570999145508, \"min\": 3214.570999145508}}, \"EndTime\": 1588627787.994479, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627784.779995}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:47 INFO 140149161203520] #test_score (algo-1, RMSE): 25.8048119638\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:47 INFO 140149161203520] #test_score (algo-1, mean_absolute_QuantileLoss): 20719.508656930924\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:47 INFO 140149161203520] #test_score (algo-1, mean_wQuantileLoss): 0.18874029632046463\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:47 INFO 140149161203520] #test_score (algo-1, wQuantileLoss[0.1]): 0.1182360300392704\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:47 INFO 140149161203520] #test_score (algo-1, wQuantileLoss[0.2]): 0.17303626475216888\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:47 INFO 140149161203520] #test_score (algo-1, wQuantileLoss[0.3]): 0.2071058195981005\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:47 INFO 140149161203520] #test_score (algo-1, wQuantileLoss[0.4]): 0.22762100887763245\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:47 INFO 140149161203520] #test_score (algo-1, wQuantileLoss[0.5]): 0.2345252371830079\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:47 INFO 140149161203520] #test_score (algo-1, wQuantileLoss[0.6]): 0.22847165117470955\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:47 INFO 140149161203520] #test_score (algo-1, wQuantileLoss[0.7]): 0.21101329664204896\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:47 INFO 140149161203520] #test_score (algo-1, wQuantileLoss[0.8]): 0.17793740775509562\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:47 INFO 140149161203520] #test_score (algo-1, wQuantileLoss[0.9]): 0.12071595086214726\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:47 INFO 140149161203520] #quality_metric: host=algo-1, test mean_wQuantileLoss <loss>=0.18874029632\u001b[0m\n",
      "\u001b[34m[05/04/2020 21:29:47 INFO 140149161203520] #quality_metric: host=algo-1, test RMSE <loss>=25.8048119638\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 959540.9989356995, \"sum\": 959540.9989356995, \"min\": 959540.9989356995}, \"setuptime\": {\"count\": 1, \"max\": 8.883953094482422, \"sum\": 8.883953094482422, \"min\": 8.883953094482422}}, \"EndTime\": 1588627788.057695, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1588627787.994551}\n",
      "\u001b[0m\n",
      "\n",
      "2020-05-04 21:30:00 Uploading - Uploading generated training model\n",
      "2020-05-04 21:30:00 Completed - Training job completed\n",
      "Training seconds: 1021\n",
      "Billable seconds: 1021\n",
      "CPU times: user 3.19 s, sys: 142 ms, total: 3.33 s\n",
      "Wall time: 18min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_channels = {\n",
    "    \"train\": \"{}/train/\".format(s3_data_path),\n",
    "    \"test\": \"{}/test/\".format(s3_data_path)\n",
    "}\n",
    "\n",
    "estimator.fit(inputs=data_channels, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since you pass a test set in this example, accuracy metrics for the forecast are computed and logged (see bottom of the log). You can find the definition of these metrics from our documentation. You can use these to optimize the parameters and tune your model or use SageMaker's Automated Model Tuning service to tune the model for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ### Create endpoint and predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a trained model, we can use it to perform predictions by deploying it to an endpoint.\n",
    "\n",
    "**Note: Remember to delete the endpoint after running this experiment. A cell at the very bottom of this notebook will do that: make sure you run it at the end.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To query the endpoint and perform predictions, we can define the following utility class: this allows making requests using `pandas.Series` objects rather than raw JSON strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create endpoint and predictor\n",
    "Now that we have a trained model, we can use it to perform predictions by deploying it to an endpoint.\n",
    "\n",
    "Note: Remember to delete the endpoint after running this experiment. A cell at the very bottom of this notebook will do that: make sure you run it at the end.\n",
    "\n",
    "To query the endpoint and perform predictions, we can define the following utility class: this allows making requests using pandas.Series objects rather than raw JSON strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepARPredictor(sagemaker.predictor.RealTimePredictor):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, content_type=sagemaker.content_types.CONTENT_TYPE_JSON, **kwargs)\n",
    "        \n",
    "    def predict(self, ts, cat=None, dynamic_feat=None, \n",
    "                num_samples=100, return_samples=False, quantiles=[\"0.1\", \"0.5\", \"0.9\"]):\n",
    "        \"\"\"Requests the prediction of for the time series listed in `ts`, each with the (optional)\n",
    "        corresponding category listed in `cat`.\n",
    "        \n",
    "        ts -- `pandas.Series` object, the time series to predict\n",
    "        cat -- integer, the group associated to the time series (default: None)\n",
    "        num_samples -- integer, number of samples to compute at prediction time (default: 100)\n",
    "        return_samples -- boolean indicating whether to include samples in the response (default: False)\n",
    "        quantiles -- list of strings specifying the quantiles to compute (default: [\"0.1\", \"0.5\", \"0.9\"])\n",
    "        \n",
    "        Return value: list of `pandas.DataFrame` objects, each containing the predictions\n",
    "        \"\"\"\n",
    "        prediction_time = ts.index[-1]\n",
    "        quantiles = [str(q) for q in quantiles]\n",
    "        req = self.__encode_request(ts, cat, dynamic_feat, num_samples, return_samples, quantiles)\n",
    "        res = super(DeepARPredictor, self).predict(req)\n",
    "        return self.__decode_response(res, ts.index.freq, prediction_time, return_samples)\n",
    "    \n",
    "    def __encode_request(self, ts, cat, dynamic_feat, num_samples, return_samples, quantiles):\n",
    "        instance = series_to_dict(ts, cat if cat is not None else None, dynamic_feat if dynamic_feat else None)\n",
    "\n",
    "        configuration = {\n",
    "            \"num_samples\": num_samples,\n",
    "            \"output_types\": [\"quantiles\", \"samples\"] if return_samples else [\"quantiles\"],\n",
    "            \"quantiles\": quantiles\n",
    "        }\n",
    "        \n",
    "        http_request_data = {\n",
    "            \"instances\": [instance],\n",
    "            \"configuration\": configuration\n",
    "        }\n",
    "        \n",
    "        return json.dumps(http_request_data).encode('utf-8')\n",
    "    \n",
    "    def __decode_response(self, response, freq, prediction_time, return_samples):\n",
    "        # we only sent one time series so we only receive one in return\n",
    "        # however, if possible one will pass multiple time series as predictions will then be faster\n",
    "        predictions = json.loads(response.decode('utf-8'))['predictions'][0]\n",
    "        prediction_length = len(next(iter(predictions['quantiles'].values())))\n",
    "        #prediction_index = pd.DatetimeIndex(start=prediction_time, freq=freq, periods=prediction_length)\n",
    "        prediction_index = pd.date_range(prediction_time, periods = prediction_length, freq=freq)\n",
    "        if return_samples:\n",
    "            dict_of_samples = {'sample_' + str(i): s for i, s in enumerate(predictions['samples'])}\n",
    "        else:\n",
    "            dict_of_samples = {}\n",
    "        return pd.DataFrame(data={**predictions['quantiles'], **dict_of_samples}, index=prediction_index)\n",
    "\n",
    "    def set_frequency(self, freq):\n",
    "        self.freq = freq\n",
    "        \n",
    "def encode_target(ts):\n",
    "    return [x if np.isfinite(x) else \"NaN\" for x in ts]        \n",
    "\n",
    "def series_to_dict(ts, cat=None, dynamic_feat=None):\n",
    "    \"\"\"Given a pandas.Series object, returns a dictionary encoding the time series.\n",
    "\n",
    "    ts -- a pands.Series object with the target time series\n",
    "    cat -- an integer indicating the time series category\n",
    "\n",
    "    Return value: a dictionary\n",
    "    \"\"\"\n",
    "    obj = {\"start\": str(ts.index[0]), \"target\": encode_target(ts)}\n",
    "    if cat is not None:\n",
    "        obj[\"cat\"] = cat\n",
    "    if dynamic_feat is not None:\n",
    "        obj[\"dynamic_feat\"] = dynamic_feat        \n",
    "    return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can deploy the model and create and endpoint that can be queried using our custom DeepARPredictor class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    predictor_cls=DeepARPredictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions and plot results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the `predictor` object to generate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.9</th>\n",
       "      <th>0.5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-07-01 22:00:00</th>\n",
       "      <td>30.041714</td>\n",
       "      <td>42.949574</td>\n",
       "      <td>36.940876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-02 00:00:00</th>\n",
       "      <td>-0.374691</td>\n",
       "      <td>7.125742</td>\n",
       "      <td>3.226394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-02 02:00:00</th>\n",
       "      <td>2.323544</td>\n",
       "      <td>9.957971</td>\n",
       "      <td>6.066413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-02 04:00:00</th>\n",
       "      <td>30.744331</td>\n",
       "      <td>67.653412</td>\n",
       "      <td>50.211800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-02 06:00:00</th>\n",
       "      <td>74.129120</td>\n",
       "      <td>201.084015</td>\n",
       "      <td>132.145020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           0.1         0.9         0.5\n",
       "2019-07-01 22:00:00  30.041714   42.949574   36.940876\n",
       "2019-07-02 00:00:00  -0.374691    7.125742    3.226394\n",
       "2019-07-02 02:00:00   2.323544    9.957971    6.066413\n",
       "2019-07-02 04:00:00  30.744331   67.653412   50.211800\n",
       "2019-07-02 06:00:00  74.129120  201.084015  132.145020"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict(ts=timeseries[1], quantiles=[0.10, 0.5, 0.90]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define a plotting function that queries the model and displays the forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(\n",
    "    predictor, \n",
    "    target_ts, \n",
    "    cat=None, \n",
    "    dynamic_feat=None, \n",
    "    forecast_date=end_training, \n",
    "    show_samples=False, \n",
    "    plot_history=7 * 12,\n",
    "    confidence=80\n",
    "):\n",
    "    print(\"calling served model to generate predictions starting from {}\".format(str(forecast_date)))\n",
    "    assert(confidence > 50 and confidence < 100)\n",
    "    low_quantile = 0.5 - confidence * 0.005\n",
    "    up_quantile = confidence * 0.005 + 0.5\n",
    "        \n",
    "    # we first construct the argument to call our model\n",
    "    args = {\n",
    "        \"ts\": target_ts[:forecast_date],\n",
    "        \"return_samples\": show_samples,\n",
    "        \"quantiles\": [low_quantile, 0.5, up_quantile],\n",
    "        \"num_samples\": 100\n",
    "    }\n",
    "\n",
    "\n",
    "    if dynamic_feat is not None:\n",
    "        args[\"dynamic_feat\"] = dynamic_feat\n",
    "        fig = plt.figure(figsize=(20, 6))\n",
    "        ax = plt.subplot(2, 1, 1)\n",
    "    else:\n",
    "        fig = plt.figure(figsize=(20, 3))\n",
    "        ax = plt.subplot(1,1,1)\n",
    "    \n",
    "    if cat is not None:\n",
    "        args[\"cat\"] = cat\n",
    "        ax.text(0.9, 0.9, 'cat = {}'.format(cat), transform=ax.transAxes)\n",
    "\n",
    "    # call the end point to get the prediction\n",
    "    prediction = predictor.predict(**args)\n",
    "\n",
    "    # plot the samples\n",
    "    if show_samples: \n",
    "        for key in prediction.keys():\n",
    "            if \"sample\" in key:\n",
    "                prediction[key].plot(color='lightskyblue', alpha=0.2, label='_nolegend_')\n",
    "                \n",
    "                \n",
    "    # plot the target\n",
    "    target_section = target_ts[forecast_date-plot_history:forecast_date+prediction_length]\n",
    "    target_section.plot(color=\"black\", label='target')\n",
    "    \n",
    "    # plot the confidence interval and the median predicted\n",
    "    ax.fill_between(\n",
    "        prediction[str(low_quantile)].index, \n",
    "        prediction[str(low_quantile)].values, \n",
    "        prediction[str(up_quantile)].values, \n",
    "        color=\"b\", alpha=0.3, label='{}% confidence interval'.format(confidence)\n",
    "    )\n",
    "    prediction[\"0.5\"].plot(color=\"b\", label='P50')\n",
    "    ax.legend(loc=2)    \n",
    "    \n",
    "    # fix the scale as the samples may change it\n",
    "    ax.set_ylim(target_section.min() * 0.5, target_section.max() * 1.5)\n",
    "    \n",
    "    if dynamic_feat is not None:\n",
    "        for i, f in enumerate(dynamic_feat, start=1):\n",
    "            ax = plt.subplot(len(dynamic_feat) * 2, 1, len(dynamic_feat) + i, sharex=ax)\n",
    "            feat_ts = pd.Series(\n",
    "                index=pd.DatetimeIndex(start=target_ts.index[0], freq=target_ts.index.freq, periods=len(f)),\n",
    "                data=f\n",
    "            )\n",
    "            feat_ts[forecast_date-plot_history:forecast_date+prediction_length].plot(ax=ax, color='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can interact with the function previously defined, to look at the forecast of count of rides in 15 minutes interval for any vendor at any point in (future) time. \n",
    "\n",
    "For each request, the predictions are obtained by calling our served model on the fly.\n",
    "\n",
    "Here we forecast the ride count after week-end (note the lower week-end consumption). \n",
    "You can select any time series and any forecast date, just click on `Run Interact` to generate the predictions from our served endpoint and see the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "style = {'description_width': 'initial'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b11d4d436eb4bfc833114a11ef273df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='vendor_id', max=1, style=SliderStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact_manual(\n",
    "    vendor_id=IntSlider(min=0, max=1, value=1, style=style), \n",
    "    forecast_day=IntSlider(min=0, max=100, value=51, style=style),\n",
    "    confidence=IntSlider(min=60, max=95, value=80, step=5, style=style),\n",
    "    history_weeks_plot=IntSlider(min=1, max=20, value=1, style=style),\n",
    "    show_samples=Checkbox(value=False),\n",
    "    continuous_update=False\n",
    ")\n",
    "def plot_interact(vendor_id, forecast_day, confidence, history_weeks_plot, show_samples):\n",
    "    plot(\n",
    "        predictor,\n",
    "        target_ts=timeseries[vendor_id],\n",
    "        forecast_date=end_training + datetime.timedelta(days=forecast_day),\n",
    "        show_samples=show_samples,\n",
    "        plot_history=history_weeks_plot * 12 * 7,\n",
    "        confidence=confidence\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
